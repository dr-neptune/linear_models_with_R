
```{r}
library(faraway)
library(tidyverse)
library(magrittr)
library(broom)
```

# Prediction 

Say we have built a model $y = X \beta + \epsilon$. Given a new set of predictors, $x_0$, the predicted response is $\hat{y_0 = x_0^T \hat{\beta}}$.

We need to asses the uncertainty in this prediction in order to give decision makers more than a point estimate. 

## 4.1 | Confidence Intervals for Predictions 

There are two kinds of predictions made from regression models. One is a predicted mean response and the other is a prediction of a future observation. 

Suppose we have built a regression model on houses

- A prediction of a future value would be the following : Suppose a house comes on the market with characteristics $x_0$. We can set the rental price as $y = X \beta + \epsilon$. Since $E(\epsilon) = 0$, the predicted price is $x_0^T\hat{\beta}$, but when assessing the variance of this prediction, we must include the variance of $\epsilon$.

- A predicted mean response would be the following: Given a house with characteristics $x_0$, what would it rent for on average? This selling price is $x_0^T\beta$ and is again predicted by $x_0^T\hat{\beta}$, but now only the variance in $\hat{\beta}$ needs to be taken into account. 

## 4.2 | Predicting Body Fat 

```{r}
# load data
data(fat, package = "faraway")

(fat %<>% as_tibble())

# fit model 
lmod <- lm(brozek ~ age + weight + height +
               neck + chest + abdom + hip +
               thigh + knee + ankle + biceps +
               forearm + wrist, data = fat)

# brozek is Brozek's equation, which estimates body fat from density

# create a set of predictors for an average man
(x <- model.matrix(lmod) %>% as_tibble())
(x0 <- map_dfc(x, ~ median(.x)))
(y0 <- sum(x0 * coef(lmod)))
```

The predicted body fat for the typical man in this case is ~ 17.5%. 

The same result may be obtained more directly using the predict function:

```{r}
predict(lmod, new = x0)
```

Now if we want a 95% CI for the prediction, we must decide whether we are predicting the same body fat for one man or the mean body fat for all men with the same characteristics

```{r}
# one person
predict(lmod, x0, interval = "prediction")
```

The prediction interval is quite large, as it encompasses the uncertainty of the entire model for its output.

```{r}
# all men with the same characteristics
predict(lmod, x0, interval = "confidence")
```

The prediction interval for the mean above is much narrower, indicating we can be more certain about the average body fat of the man with median characteristics. 

There are two types of extrapolation: quantitative and qualitative. Quantitative extrapolation concerns x_0 which are far from the original data. In general, prediction intervals become wider as we move further from the data. 

Let's see what happens with a prediction for values at the 95th percentile of data:

```{r}
# generate sample at the 95% percentile for all of the predictors
(x1 <- map_dfc(x, ~ quantile(.x, 0.95)))

# one person
predict(lmod, x1, interval = "prediction")

# mean for given characteristics 
predict(lmod, x1, interval = "confidence")
```

We see that the interval for the mean is now almost 4% wide, which is a considerable increase in uncertainty over the uncertainty with the median sample. 

The prediction interval is only slightly wider because the interval is now dominated by the new error $\epsilon$ rather than the uncertainty in the estimate of $\beta$. 

In this case, we can account for parametric uncertainty using the methods we have described, but model uncertainty is harder to quantify. 

# 4.3 | Autoregression 

```{r}
data(airpass, package = "faraway")

airpass %<>% as_tibble()

# fit linear model 
(lmod <- lm(log(pass) ~ year, airpass))

# grab fitted values 
pred_vals <- augment(lmod) %>% select(2:3)

# plot linear fit 
airpass %>%
    ggplot(aes(x = year, y = pass)) +
    geom_line() +
    geom_line(color = "mediumpurple", data = pred_vals,
              aes(x = year, y = exp(.fitted)))
```

This captures the general upward trend in numbers, but does not capture the seasonal variation. 

Suppose we wish to predict passenger numbers for the next month. We might expect this to depend on the current month. The seasonal variation also suggests that we use the observed numbers from 12 months ago. Since we are already considering a monthly change, it might also make sense to use the numbers from 13 months ago as well. 

We combine these in a regression model: 

$y_t = \beta_0 + \beta_1 y_{t-1} + \beta_{12} y_{t-12} + \beta_{13} y_{t-13} + \epsilon_t$

This is an example of an autoregressive process. The response depends on past values of the response. The $y_{t-i}$ are called lagged variables. We can construct a matrix of the lagged variables with the embed function. 


```{r}
lag_df <- embed(log(airpass$pass), dimension = 14) %>% as_tibble() %>% set_colnames(c("y", paste0("lag", 1:13)))

(armod <- lm(y ~ lag1 + lag12 + lag13, lag_df))

armod %>% summary() %>% tidy()
```

We see all three lagged variables are strongly significant (at the 1 / 1000 level) and the R^2 is very high. It is also now redundant to include a year term in the model since this linear change in time is taken care of with the lag1 term.

```{r}
# check the fit
pred_vals <- augment(armod)

# plot linear fit 
airpass %>%
    ggplot(aes(x = year, y = pass)) +
    geom_line() +
    geom_line(color = "mediumpurple", data = pred_vals,
              aes(x = airpass$year[14:144], y = exp(.fitted)))
```

Suppose we wish to predict future values. The last observation in our data is 

```{r}
lag_df[nrow(lag_df),]
```

The current response becomes the lag one value and the other two also shift. A 95% prediction interval for the logged number of passengers is : 

```{r}
predict(object = armod, newdata = tibble(lag1 = 6.0684, lag12 = 6.0331, lag13 = 6.0039), interval = "prediction")
```

# 4.4 | What Can Go Wrong with Predictions? 

1. Bad Model. 
2. Quantitative Extrapolation. We try to predict outcomes for cases with predictor values much different from what we saw in the data. 
3. Qualitative Extrapolation. We try to predict outcomes for observations that come from a different population. 
4. Overconfidence due to overfitting. Practicioners searching around for good models for some data often do too good a job in finding a fit, which can lead to an unrealistically small variance 
5. Black swans. Sometimes errors appear to be normally distributed because we haven't seen enough data to be aware of extremes. 

# Exercises 

1. 

```{r}
# get data
data(prostate, package = "faraway")

# fit a model
(lmod <- lm(lpsa ~ ., prostate))

prostate %<>% as_tibble()

# predict the lpsa for a patient with an appropriate 95% confidence interval 
prostate %>%
    add_row("lcavol" = 1.44692,
            "lweight" = 3.62301,
            "age" = 65.00000,
            "lbph" = 0.30010,
            "svi" = 0.00000,
            "lcp" = -0.79851,
            "gleason" = 7.00000, "pgg45" = 15.00000) %>%
    slice(nrow(.)) %>%
    predict(lmod, ., interval = "prediction")


# repeat the last prediction, except for a patient who is 20
prostate %>%
    add_row("lcavol" = 1.44692,
            "lweight" = 3.62301,
            "age" = 20.00000,
            "lbph" = 0.30010,
            "svi" = 0.00000,
            "lcp" = -0.79851,
            "gleason" = 7.00000, "pgg45" = 15.00000) %>%
    slice(nrow(.)) %>%
    predict(lmod, ., interval = "prediction")

```

In the first prediction, the confidence interval is between 0.96 and 3.81, a range of 2.85. In the second prediction, the confidence interval is between 1.54 and 5.0, a difference of 3.46. This is likely because when we set the age to 20, we are setting the value of the point estimate for the age coefficient to 20, and, since 20 is an outlier age for someone getting this test done, this provides a lot of error that is propagated forward through the model's beta estimates. 

```{r}
# for the model in part a, remove all the nonsignificant predictors at the 5% level.
lmod %>% summary()

(lmod2 <- lm(lpsa ~ lcavol + lweight + svi, prostate))

# now recompute all the predictions of the previous question. Are the CIs wider or narrower? Which predictions would you prefer?

# predict the lpsa for a patient with an appropriate 95% confidence interval 
prostate %>%
    add_row("lcavol" = 1.44692,
            "lweight" = 3.62301,
            "age" = 65.00000,
            "lbph" = 0.30010,
            "svi" = 0.00000,
            "lcp" = -0.79851,
            "gleason" = 7.00000, "pgg45" = 15.00000) %>%
    slice(nrow(.)) %>%
    predict(lmod2, ., interval = "prediction")


# repeat the last prediction, except for a patient who is 20
prostate %>%
    add_row("lcavol" = 1.44692,
            "lweight" = 3.62301,
            "age" = 20.00000,
            "lbph" = 0.30010,
            "svi" = 0.00000,
            "lcp" = -0.79851,
            "gleason" = 7.00000, "pgg45" = 15.00000) %>%
    slice(nrow(.)) %>%
    predict(lmod2, ., interval = "prediction")
```

The new confidence intervals are both the same: 0.93 - 3.81, with the same range of 2.85. This is because the only differentiating explanatory variable in the first two models, age, has been removed. Technically the CI is the same for the first group and narrower for the second. In this case, I would prefer the first model since it provides additional factors that affect the outcome and the outliers have a stronger effect on the final prediction.

2. 

```{r}
data(teengamb, package = "faraway")

teengamb %>% as_tibble()

(lmod <- lm(gamble ~ ., teengamb))

lmod %>% summary()

# predict the amount that a male with average (given the data) status, income, and verbal score would gamble along with an appropriate 95% CI
teengamb %>%
    filter(sex == 0) %>% 
    map_dfc(mean) %>%
    predict(lmod, ., interval = "confidence")

# repeat the prediction for a male with maximal values
teengamb %>%
    filter(sex == 0) %>% 
    map_dfc(max) %>%
    predict(lmod, ., interval = "confidence")
```

The CI for the maximal values is much wider, ~100 - ~42 ~= 58, whereas the mean valued male has a CI range of ~38 - ~21 ~= 17. This result is expected, because there is a lot of error when every value is an outlier. The lack of information makes our point estimates extremely uncertain and that error is propagated forward. 


```{r}
# fit a model with sqrt(gamble) as the response, but with the same predictors.
(lmod2 <- lm(sqrt(gamble) ~ ., teengamb))

lmod %>% summary()
lmod2 %>% summary()

# now predict the response and give a 95% CI for the `mean` individual in a). Take care to give the answer in the original units of response.
teengamb %>%
    filter(sex == 0) %>% 
    map_dfc(mean) %>%
    predict(lmod2, ., interval = "confidence") %>%
    tidy() %>%
    map_dfc(., function(x) x*x)
```

In this case, our new CI is (12.9, 26.9), with a range of 14 and a fitted value of 19.3. In the previous case, we say a CI of (~21, ~38) with a range of ~17 and a fitted value of 29.775. 

In this case, our confidence interval is narrower and our model has a higher R^2 value. 

```{r}
# repeat the prediction for the model in c with the given values
teengamb %>% as_tibble() %>% add_row("sex" = 1,
                                     "status" = 20,
                                     "income" = 1,
                                     "verbal" = 10) %>%
    slice(nrow(.)) %>%
    predict(lmod2, ., interval = "prediction") 
```

This does not make sense, as gamble is a percentage of the takehome pay spent gambling. With the exception of one (likely errant) record, all of the gambling values are contained in [0, 100].
 
```{r}
teengamb %>%
    filter(gamble <= 0 | gamble > 100)
```

3. 

```{r}
data(snail, package = "faraway")
(snail %<>% as_tibble())

# produce a table of mean water content for each combination of temperature and humidity
xtabs(water ~ temp + humid, snail) / 4
```

Can you use this table to predict the water content for a temperature of 25c and 60% humidity? 

We would need some sort of interpolation. Since 25 = (30 + 20)/2 and 60 = (45 + 75) / 2, we may infer the midpoint if the data was entirely linear or regular 

```{r}
# grab midpoint of temperatures at 45 humidity
(72.5 + 69.50) / 2

# grab midpoint of temperatures at 75 humidity
(81.5 + 78.25) / 2

# grab midpoint of midpoints

(71 + 79.875) / 2
```

```{r}
# fit a regression model and use the model to predict the water content at 25c, 60 humidity
(lmod <- lm(water ~ temp + humid, snail))

snail %>%
    add_row("temp" = 25, "humid" = 60) %>%
    slice(nrow(.)) %>% 
predict(lmod, ., interval = "prediction")

# predict water content for a temperature of 30c and a humidity of 75%
snail %>%
    add_row("temp" = 30, "humid" = 75) %>%
    slice(nrow(.)) %>% 
predict(lmod, ., interval = "prediction")
```

Compare your predictions to the prediction from a. Discuss the relative merits of the two predictions.

For the first model, my initial prediction based on sheer linear interpolation was within the prediction interval, but still off by around 1. The first prediction has merit because it is easier to calculate and can be done quickly. The second has merit because its likely more accurate, as it fits a linear model to the entire set of points as opposed to just the two closest points. 

```{r}
snail %>%
    add_row("water" = 75.4375, "temp" = 25, "humid" = 60) %>%
    add_row("water" = 76.43681, "temp" = 25, "humid" = 60) %>%
    add_row("water" = 82.62248, "temp" = 30, "humid" = 75) -> snail_out_1

plotly::plot_ly(x = snail_out_1$water, y = snail_out_1$temp, z = snail_out_1$humid, type = "scatter3d", mode = "markers")
```

d. The intercept in our model is 54.6%. Give two values of the predictors for which this represents the predicted response. 

a) beta_1 * temp = - beta_2 * humid
b) beta_2 * humid = - beta_1 * temp 

Is the answer unique? No, it comprises all real numbers. 

Do you think this represents a reasonable prediction? No, since none of the water data points lie near the intercept 

```{r}
lmod %>% summary()

snail %>%
    filter(between(water, 45, 60))

snail$water %>% min()
```

e) For a temperature of 25 C, what value of humidity would give a predicted response of 80% water content? 

```{r}
lmod
```

Our formula for the linear model is 

water = 52.6108 - 0.1833 * temp + 0.4735 * humid

Filling this in, we get 

80 = 51.6108 - 0.1833 * 25 + 0.4735 * humid 

which, after algebraic manipulation gives us 

(80 - 51.6108 + 0.1833 * 25)/0.4735

which is 

```{r}
(80 - 51.6108 + 0.1833 * 25)/0.4735
```

4. 

```{r}
data(UKLungDeaths, package = "datasets")

mdeaths %<>%
    timetk::tk_tbl() %>%
    mutate(year = lubridate::year(index),
           month = lubridate::month(index)) %>%
    select("date" = index, "ndeaths" = value, year, month)

mdeaths %>%
    ggplot(aes(x = date, y = ndeaths)) +
    geom_line() +
    xlab("year") + ylab("number of deaths") +
    ggtitle("UK Lung Deaths") 
```

Deaths are most likely to occur in the first few months of the year

```{r}
mdeaths %>%
    ggplot(aes(x = month, y = ndeaths)) +
    geom_line() +
    facet_wrap(~year) + 
    xlab("month") + ylab("number of deaths") +
    ggtitle("UK Lung Deaths") 
```

Fit an autoregressive model of the same form used for the airline data. Are all the predictors statistically significant? 

```{r}
lag_df <- embed(log(mdeaths$ndeaths), dimension = 14) %>% as_tibble() %>% set_colnames(c("y", paste0("lag", 1:13)))

(armod <- lm(y ~ lag1 + lag12 + lag13, lag_df))

armod %>% summary()

pred_vals <- augment(armod)

mdeaths %>%
    ggplot(aes(x = date, y = ndeaths)) +
    geom_line() +
    geom_line(color = "mediumpurple", data = pred_vals,
              aes(x = mdeaths$year[14:length(mdeaths$year)], y = exp(.fitted)))
```

Only the lag12 variable is statistically significant at any level. 

Use the model to predict the number of deaths in January 1980 along with a 95% prediction interval

```{r}
mdeaths %>%
    slice(nrow(.))

lag_df %>%
    slice(nrow(.))

predict(armod, data.frame(lag1 = 7.2, lag12 = 7.72, lag13 = 7.5), interval = "prediction")

# this equates to
exp(7.55) %>% round(0)
```

d. Use your answer from the previous question to compute a prediction and interval for February 1980

```{r}
# in order to predict February, we take our predicted value for January and treat it as the lag1 variable. We also shift the lag12 and lag13 as well
predict(armod, data.frame(lag1 = 7.55, lag12 = 7.51, lag13 = 7.72), interval = "prediction")

# this equates to
exp(7.51) %>% round(0)
```

e. Plot the fitted values against the observed values

```{r}
pred_vals %<>%
    select("response" = y, everything())

pred_vals %>%
    ggplot(aes(x = response, y = .fitted)) +
    geom_point() +
    geom_segment(aes(x = response, y = .fitted, xend = .fitted, yend = .fitted), alpha = 0.3) +
    geom_abline(slope = 1, intercept = 0, color = "mediumpurple") +
    geom_point(aes(color = abs(.resid))) +
    scale_color_continuous(low = "mediumpurple", high = "red") +
    guides(color = FALSE) +
    xlim(c(6.75, 8)) + ylim(c(6.75, 8)) +
    geom_point(aes(y = .fitted), shape = 1)
```

Do you think the accuracy of the predictions will be the same for all months of the year? 

No, accuracy will be better for months with less spread in the response e^ndeaths. This means that the summer and early fall months will be more accurate than the early months. 

The absolute percentage error is a unitless accuracy metric that can be used as a simple measure of prediction accuracy: 

```{r}
pred_vals %>%
    mutate(abs_p_e = abs(100 * (response - .fitted) / response),
           month = rep_len(month.abb[1:12], nrow(.))) %>%
    group_by(month) %>%
    summarize(avg_ape = mean(abs_p_e)) %>%
    arrange(avg_ape) %>%
    ggplot(aes(x = reorder(month, avg_ape), y = avg_ape)) +
    geom_col(fill = "mediumpurple", color = "gray6") +
    geom_text(aes(label = round(avg_ape, 2)), position = position_dodge(width = 0.9), vjust = -0.25) +
    xlab("month") + ylab("absolute percent error")
```

5. 

```{r}
lmod

lmod2 <- lm(brozek ~ age + weight + height + abdom, fat)

lmod %>% summary()
lmod2 %>% summary()

anova(lmod2, lmod)
```

It is not justifiable to use the smaller value, as the P-value for the F-test is low enough to reject the null hypothesis that the smaller model is a better fit on the data.

b. Compute a 95% prediction interval for the median predictor values and compare to the results to the interval for the full model

```{r}
fat %>% map_dfc(median) %>%
    predict(lmod, ., interval = "prediction")

fat %>% map_dfc(median) %>%
    predict(lmod2, ., interval = "prediction")
```

Do the intervals differ by a practically important amount? 

```{r}
fat %>%
    ggplot(aes(x = brozek)) +
    geom_density(fill = "mediumpurple", color = "gray6") +
    geom_vline(xintercept = c(17.49322, 9.61783, 25.36861), color = "gold") +
    geom_vline(xintercept = c(17.84028, 9.696631, 25.98392), color = "red") +
    ggtitle("Brozek Densities",
            subtitle = "Median Prediction Intervals\nGold = LM_1\nRed = LM_2")
```

No.

c. For the smaller model, examine all the observations from case numbers 25 to 50. 

```{r}
augment(lmod2) %>%
    mutate(index = row_number()) %>% 
    slice(25:50) %>%
    select(index, 1:6, .cooksd) %>% 
    arrange(desc(.cooksd))
```

Which two observations seem particularly anomalous? 

The 29th and 42nd observations have very high cook's distances, indicating a lot of pull on the linear regression model.

d. Recompute the 95% prediction interval for the median predictor variables after these two anomalous cases have been excluded.

```{r}
fat %>% slice(-c(39, 42)) %>%
    map_dfc(median) %>%
    predict(lmod2, ., interval = "prediction")

fat %>%
    ggplot(aes(x = brozek)) +
    geom_density(fill = "mediumpurple", color = "gray6") +
    geom_vline(xintercept = c(17.49322, 9.61783, 25.36861), color = "gold") +
    geom_vline(xintercept = c(17.84028, 9.696631, 25.98392), color = "red") +
    geom_vline(xintercept = c(17.84219, 9.698588, 25.98579), color = "blue") + 
    ggtitle("Brozek Densities",
            subtitle = "Median Prediction Intervals\nGold = LM_1\nRed = LM_2\nBlue = De-leveraged Pred")
```

This did not make much difference to the outcome.

