
```{r}
library(faraway)
library(tidyverse)
library(magrittr)
library(broom)
```

# Prediction 

Say we have built a model $y = X \beta + \epsilon$. Given a new set of predictors, $x_0$, the predicted response is $\hat{y_0 = x_0^T \hat{\beta}}$.

We need to asses the uncertainty in this prediction in order to give decision makers more than a point estimate. 

## 4.1 | Confidence Intervals for Predictions 

There are two kinds of predictions made from regression models. One is a predicted mean response and the other is a prediction of a future observation. 

Suppose we have built a regression model on houses

- A prediction of a future value would be the following : Suppose a house comes on the market with characteristics $x_0$. We can set the rental price as $y = X \beta + \epsilon$. Since $E(\epsilon) = 0$, the predicted price is $x_0^T\hat{\beta}$, but when assessing the variance of this prediction, we must include the variance of $\epsilon$.

- A predicted mean response would be the following: Given a house with characteristics $x_0$, what would it rent for on average? This selling price is $x_0^T\beta$ and is again predicted by $x_0^T\hat{\beta}$, but now only the variance in $\hat{\beta}$ needs to be taken into account. 

## 4.2 | Predicting Body Fat 

```{r}
# load data
data(fat, package = "faraway")

(fat %<>% as_tibble())

# fit model 
lmod <- lm(brozek ~ age + weight + height +
               neck + chest + abdom + hip +
               thigh + knee + ankle + biceps +
               forearm + wrist, data = fat)

# brozek is Brozek's equation, which estimates body fat from density

# create a set of predictors for an average man
(x <- model.matrix(lmod) %>% as_tibble())
(x0 <- map_dfc(x, ~ median(.x)))
(y0 <- sum(x0 * coef(lmod)))
```

The predicted body fat for the typical man in this case is ~ 17.5%. 

The same result may be obtained more directly using the predict function:

```{r}
predict(lmod, new = x0)
```

Now if we want a 95% CI for the prediction, we must decide whether we are predicting the same body fat for one man or the mean body fat for all men with the same characteristics

```{r}
# one person
predict(lmod, x0, interval = "prediction")
```

The prediction interval is quite large, as it encompasses the uncertainty of the entire model for its output.

```{r}
# all men with the same characteristics
predict(lmod, x0, interval = "confidence")
```

The prediction interval for the mean above is much narrower, indicating we can be more certain about the average body fat of the man with median characteristics. 

There are two types of extrapolation: quantitative and qualitative. Quantitative extrapolation concerns x_0 which are far from the original data. In general, prediction intervals become wider as we move further from the data. 

Let's see what happens with a prediction for values at the 95th percentile of data:

```{r}
# generate sample at the 95% percentile for all of the predictors
(x1 <- map_dfc(x, ~ quantile(.x, 0.95)))

# one person
predict(lmod, x1, interval = "prediction")

# mean for given characteristics 
predict(lmod, x1, interval = "confidence")
```

We see that the interval for the mean is now almost 4% wide, which is a considerable increase in uncertainty over the uncertainty with the median sample. 

The prediction interval is only slightly wider because the interval is now dominated by the new error $\epsilon$ rather than the uncertainty in the estimate of $\beta$. 

In this case, we can account for parametric uncertainty using the methods we have described, but model uncertainty is harder to quantify. 

# 4.3 | Autoregression 

```{r}
data(airpass, package = "faraway")

airpass %<>% as_tibble()

# fit linear model 
(lmod <- lm(log(pass) ~ year, airpass))

# grab fitted values 
pred_vals <- augment(lmod) %>% select(2:3)

# plot linear fit 
airpass %>%
    ggplot(aes(x = year, y = pass)) +
    geom_line() +
    geom_line(color = "mediumpurple", data = pred_vals,
              aes(x = year, y = exp(.fitted)))
```

This captures the general upward trend in numbers, but does not capture the seasonal variation. 

Suppose we wish to predict passenger numbers for the next month. We might expect this to depend on the current month. The seasonal variation also suggests that we use the observed numbers from 12 months ago. Since we are already considering a monthly change, it might also make sense to use the numbers from 13 months ago as well. 

We combine these in a regression model: 

$y_t = \beta_0 + \beta_1 y_{t-1} + \beta_{12} y_{t-12} + \beta_{13} y_{t-13} + \epsilon_t$

This is an example of an autoregressive process. The response depends on past values of the response. The $y_{t-i}$ are called lagged variables. We can construct a matrix of the lagged variables with the embed function. 


```{r}
lag_df <- embed(log(airpass$pass), dimension = 14) %>% as_tibble() %>% set_colnames(c("y", paste0("lag", 1:13)))

(armod <- lm(y ~ lag1 + lag12 + lag13, lag_df))

armod %>% summary() %>% tidy()
```

We see all three lagged variables are strongly significant (at the 1 / 1000 level) and the R^2 is very high. It is also now redundant to include a year term in the model since this linear change in time is taken care of with the lag1 term.

```{r}
# check the fit
pred_vals <- augment(armod)

# plot linear fit 
airpass %>%
    ggplot(aes(x = year, y = pass)) +
    geom_line() +
    geom_line(color = "mediumpurple", data = pred_vals,
              aes(x = airpass$year[14:144], y = exp(.fitted)))
```

Suppose we wish to predict future values. The last observation in our data is 

```{r}
lag_df[nrow(lag_df),]
```

The current response becomes the lag one value and the other two also shift. A 95% prediction interval for the logged number of passengers is : 

```{r}
predict(object = armod, newdata = tibble(lag1 = 6.0684, lag12 = 6.0331, lag13 = 6.0039), interval = "prediction")
```

# 4.4 | What Can Go Wrong with Predictions? 

1. Bad Model. 
2. Quantitative Extrapolation. We try to predict outcomes for cases with predictor values much different from what we saw in the data. 
3. Qualitative Extrapolation. We try to predict outcomes for observations that come from a different population. 
4. Overconfidence due to overfitting. Practicioners searching around for good models for some data often do too good a job in finding a fit, which can lead to an unrealistically small variance 
5. Black swans. Sometimes errors appear to be normally distributed because we haven't seen enough data to be aware of extremes. 

# Exercises 

1. 
