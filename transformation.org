* Transformation
:PROPERTIES:
:header-args: :session R-session :results output value :colnames yes
:END:

#+NAME: round-tbl
#+BEGIN_SRC emacs-lisp :var tbl="" fmt="%.1f"
(mapcar (lambda (row)
          (mapcar (lambda (cell)
                    (if (numberp cell)
                        (format fmt cell)
                      cell))
                  row))
        tbl)
#+end_src

#+RESULTS: round-tbl

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(tidyverse)
library(faraway)
library(broom)
library(magrittr)
#+END_SRC

** Transforming the Response 

Suppose we want to use a logged response variable: $\log{y} = \beta_0 + \betax + \epsilon$. Then $y = \exp{\beta_0 + \beta_1 x} \cdot \exp{\epsilon}$.

In this case, the errors enter multiplicatively and not additively as they usually do. 

In practice, we may not know how the errors enter the model, additively, multiplicatively, or otherwise. The best approach is to try different transforms to get the structural form of the model right and then worry aobut the error component later. 

Regression coefficients will need to be interpreted with respect the the transformed scale. There is no straightforward way of back-transforming them to values that can be interpreted in the original scale. You can not directly compare regression coefficients for models where the response transformation is different. 

When you use a log transformation on the response, the regression coefficients have a particular interpretation: 

$\log \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + ... + \hat{\beta_p}x_p$
$\hat{y} = e^{\hat{\beta_0}}e^{\hat{\beta_1 x_1}}...e^{\hat{\beta_p}x_p}$

An increase of 1 in $x_1$ would multiply the predicted response (in the original scale) by $e^{\hat{\beta_1}}$. 

** Box-Cox Transformation 

The Box-Cox method is a popular way to determine a transformation on the response. It is designed for strictly positive responses and chooses the transformation to find the best fit to the data. 

The method transforms the response $y \to g_{\lambda}(y)$ where the family of transformations indexed by $\lambda$ is $g_{\lambda}(y) = \frac{y^{\lambda} - 1}{\lambda}$ if $\lambda \neq 0$ and $\log(y)$ otherwise.

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(savings, package = "faraway")
(savings %<>% as_tibble(rownames = "country"))

# fit model 
lmod <- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings)
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
p1 <- boxcox(lmod, plotit = T)
p2 <- boxcox(lmod, plotit = T, lambda = seq(0.5, 1.5, by = 0.1))
p2
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


The confidence interval for $\lambda$ runs from about 0.6 to 1.4. There is no good reason to transform. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# consider the Galapagos Islands dataset
data(gala, package = "faraway")
gala %<>% as_tibble(rownames = "Location")

(lmod <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
(bc <- boxcox(lmod, lambda = seq(-0.25, 0.75, by = 0.05), plotit = TRUE))
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

In the boxcox transformation, a cube root or square root transformation might be a good choice. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod %>% summary()

bc_out <- tibble("lambda" = bc$x,
                 "log_likelihood" = bc$y)

bc_out %>%
    arrange(desc(log_likelihood)) %>%
    pluck(1, 1) -> lambda_out

(lmod2 <- lm(((Species^lambda_out - 1) / lambda_out) ~ Area + Elevation + Nearest + Scruz + Adjacent, gala))
lmod2 %>% summary()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
p1 <- qq_plot(gala, Species, "Species")
p2 <- qq_plot(gala, ((Species^lambda_out - 1) / lambda_out), "Species Transformed")
p1 + p2
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

Some general considerations regarding the Box-Cox Transformation: 

1. The Box-Cox method gets upset by outliers. If $\hat{\lambda} = 5$, then this is probably the reason -- there can be little justification for such an extreme transformation. 
2. If some $y_i < 0$ is small, we can add a constant to all y. This is inelegant, but usable.
3. If $\frac{\max_i y_i}{\min_i y_i}$ is small, then the Box-Cox will not have much real effect because power transforms are well approximated by linear transformations over short intervals far from the origin.
4. There is some doubt whether the estimation of $\lambda$ counts as an extra parameter to be considered in the degrees of freedom. 

The Box-Cox method is not the only way of transforming the predictors. Another family of transformations is given by $g_{\alpha}(y) = \log(y + \alpha)$. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod <- lm(burntime ~ nitrogen + chlorine + potassium, leafburn)
lmod %>% summary()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
(lt_out <- logtrans(lmod, plotit = TRUE, alpha = seq(-min(leafburn$burntime) + 0.001, 0, by = 0.01)))
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

The recommended $\hat{\alpha}$ value is 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(tibble("lambda" = lt_out$x,
        "log_likelihood" = lt_out$y) %>%
 arrange(desc(log_likelihood)) %>%
 pluck(1, 1))
#+END_SRC

** Broken Stick Regression 

Sometimes we have reason to believe that different linear regression models apply to different regions of data. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# we could fit two regression models on different subsets of the data
lmod1 <- lm(sr ~ pop15, savings, subset = (pop15 < 35))
lmod2 <- lm(sr ~ pop15, savings, subset = (pop15 >= 35))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
savings %>%
    ggplot(aes(x = pop15, y = sr)) +
    geom_point() +
    ## geom_segment(intercept = lmod1$coefficients[[1]],
    ##             slope = lmod1$coefficients[[2]],
    ##             color = "blue") +
    geom_vline(xintercept = 35, lty = 2) + 
    geom_abline(intercept = lmod2$coefficients[[1]],
                slope = lmod2$coefficients[[2]],
                color = "green")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

A possible objection is that two two parts of the fit do not meet at the join. If we believe that the fit should be continuous as the predictor varies, we should consider the broken stick regression fit. 

Define two basis functions: 

$B_l(x) = c - x$ if $x < c$ and 0 otherwise
$B_r(x) = x - c$ if $x > c$ and 0 otherwise 

where $c$ marks the division between the two groups. We can now fit a model of the form: 

$y = \beta_0 + \beta_1 B_l(x) + \beta_2 B_r(x) + \epsilon$

The two linear parts are guaranteed to meet at $c$. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
hockey_stick_lm <- function(data, formula, cutoff) {
    lhs <- function(x) ifelse(x < cutoff, cutoff - x, 0)
    rhs <- function(x) ifelse(x >= cutoff, x - cutoff, 0)
    # such hack, much wow
    new_formula <- as.formula(paste0(deparse(formula[[2]]),
                                     " ~ lhs(",
                                     deparse(formula[[3]]),
                                     ") + rhs(",
                                     deparse(formula[[3]]), ")"))
    lm(new_formula, data)
}

(hslmod <- hockey_stick_lm(savings, as.formula(sr ~ pop15), 35))
#+END_SRC


#+BEGIN_SRC R :file plot.svg :results graphics file
savings %>%
    ggplot(aes(x = pop15, y = sr)) +
    geom_point() +
    geom_vline(xintercept = 35, color = "blue", alpha = 0.1) + 
    geom_line(data = tibble("sr" = hslmod$fitted.values,
                            "pop15" = savings$pop15),
              aes(x = pop15, y = sr),
              lty = 2)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** Polynomials 

Another way of generalizing the $X \beta$ part of the model is to add polynomial terms. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
boost <- function(formula, order) {
    term <- formula[[3]]
    bup <- function(formula, count) {
        if (order == (count - 1)) return(formula)
        else {
            formula[[3]] <- bquote(.(formula[[3]]) + I(.(term)^.(count)))
            bup(formula, count + 1)
        }
    }
    bup(formula, 2)
}

# takes data, a formula, and a cutoff and returns a polynomial regression at the order for which the predictors stop being significant.
# Assumes an order 1 formula with only 1 predictor
# THIS DOES NOT WORK :( but time is finite
build_you_a_polynomial_regression <- function(data, formula, cutoff = 0.05, order = 1) {    
    # make lmod, get p-value and test if below cutoff for predictor
    lmod <- lm(formula, data)
    p_val <- tidy(lmod) %>% pluck(ncol(.), nrow(.))
    if (p_val < cutoff) {
        # recurse and increase the order of the formula
        cat("\n--------------\nOrder:", order, "\np-value:", p_val, "\nFormula:", deparse(formula), "\n--------------\n")
        new_form <- boost(formula, order)
        build_you_a_polynomial_regression(data, new_form, order = (order + 1), cutoff = cutoff)
    } else {
        # return the linear model of order - 1
        cat("here we goooo\n\n")
        lmod
    }
}
 
frm2 <- as.formula(y ~ x)
build_you_a_polynomial_regression(data = plt, formula = boost(as.formula(y ~ x), 2))

tidy(build_you_a_polynomial_regression(savings, as.formula(sr ~ ddpi)))

# test against manual
tidy(lm(frm, savings)) %>% pluck(ncol(.), nrow(.))
tidy(lm(boost(frm, 2), savings)) %>% pluck(ncol(.), nrow(.))
tidy(lm(boost(frm, 3), savings)) %>% pluck(ncol(.), nrow(.))
#+END_SRC

If you remove lower order terms from a polynomial, do note that it has special meaning. Setting the intercept to 0 means the regression passes through the origin, which setting the linear term to 0 means that the response is optimized at a predictor value of 0. 

You have to refit the model each time a term is removed. This is inconvenient, and for large $d$ there can be a problem with numerical stability. Orthogonal polynomials get around this problem by defining: 

$z_1 = a_1 + b_1 x$
$z_2 = a_2 + b_2x + c_2x^2$
$z_3 = a_3 + b_3x + c_3 c^2 + d_3 x^3$
$z_n = a_1 + b_1x + ... + \xi_n x^n$

where the coefficients $a, b, ...$ are chosen so that $z_i^T z_j = 0$ when $i \neq j$. The expressions $z$ are called orthogonal polynomials. The poly() function constructs orthogonal polynomials. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(lmod <- lm(sr ~ poly(ddpi, 4), savings))
summary(lmod)
#+END_SRC

We can also define polynomials in more than one variable. These are sometimes called response surface models.

A second degree model would be $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{11}x_1^2 + \beta_{22}x_2^2 + \beta_{12}x_1x_2$

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(lmod <- lm(sr ~ polym(pop15, ddpi, degree = 2), savings))
lmod %>% tidy()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
library(plotly)

# rows and columns describe a grid, and the cell value describes surface height
savings

pop15r <- seq(20, 50, len = 50)
ddpir <- seq(0, 20, len = 50)
pgrid <- expand.grid(pop15 = pop15r,
                     ddpi = ddpir)
pv <- predict(lmod, pgrid)
(outp <- matrix(pv, 50, 50))

plot_ly(z = outp, y = pop15r, x = ddpir, 
        type = "surface",
        contours = list(x = list(show = TRUE),
                        y = list(show = TRUE))) %>%
    layout(title = "Perspective Plot of Quadratic Surface",
           scene = list(
               yaxis = list(title = "Popn Under 15"),
               xaxis = list(title = "Growth"),
               zaxis = list(title = "Savings Rate")))
#+END_SRC

** Splines 

Polynomials have the advantage of smoothness, but each data point affects the fit globally. This is because the power functions used for the polynomials take nonzero values across the whole range of the predictor. In contrast, broken stick regression localizes the influence of each data point to its particular segemtn, but we do not have the same smoothness of the polynomials. We can combine the beneficial aspects of both of these methods - smoothness and local influence - by using B-spline basis functions. 

Suppose we know the true model is: 

$y = \sin^3(2 \pi x^3) + \epsilon$
$\epsilon \sim N(0, (0.1)^2)$


#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
funky <- function(x) sin(2 * pi * x^3)^3

x <- seq(0, 1, by = 0.01)
y <- funky(x) + 0.1 * rnorm(101)

plt <- tibble("x" = seq(0, 1, by = 0.01),
              "y" = funky(x) + 0.1 * rnorm(101))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
matplot(x, cbind(y, funky(x)), type = "pl", ylab = "y", pch = 20, lty = 1, col = 1)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :file plot.svg :results graphics file
plt %>%
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    geom_line(aes(x = x, y = funky(x)), color = "blue", alpha = 0.5) +
    geom_line(aes(x = x, y = predict(lm(boost(y ~ x, 4), plt))), lty = 2, alpha = 0.5, color = "forestgreen") +
    geom_line(aes(x = x, y = predict(lm(boost(y ~ x, 12), plt))), lty = 5, alpha = 0.5, color = "mediumpurple")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
build_you_a_polynomial_regression(data = plt, formula = boost(as.formula(y ~ x), 2))

boost(y ~ x, 4)

(lmod2 <- lm(boost(y ~ x, 5), plt))

tidy(lm(boost(y ~ x, 2), plt)) %>% pluck(ncol(.), nrow(.))
#+END_SRC

We may define a cubic B-spline basis on the interval [a, b] by the following requirements on the interior basis functions with knotpoints at $t_1, ..., t_k$: 

1. A given basis function is nonzero on an interval defined by four successive knots and zero elsewhere. This property ensures the local influence property
2. The basis function is a cubic polynomial for each subinterval between successive knots
3. The basis function is continuous and is also continuous in its first and second derivatives at each knotpoint. This property ensures the smoothness of the fit.
4. The basis function integrates to one over its support. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
require(splines)

# 12 basis functions, 3 additional knots at the start and end
knots <- c(0, 0, 0, 0,
           0.2, 0.4, 0.5, 0.6,
           0.7, 0.8, 0.85, 0.9,
           1, 1, 1, 1)

(bx <- splineDesign(knots, x))

lmodb <- lm(y ~ bx -1)
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
matplot(x, bx, type = "l", col = 1)

bx2 <- as.data.frame(bx)
bx2$id <- 1:nrow(bx2)

bx2 <- melt(bx2, id.var = "id")

ggplot(bx2, aes(x = id, y = value, group = variable, color = variable)) +
    geom_line(lty = 2, alpha = 0.8) +
    theme(legend.position = "none")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :file plot.svg :results graphics file
ggplot(plt, aes(x = x, y = y)) +
    geom_point() +
    geom_line(aes(x = x, y = lmodb$fit), lty = 2, color = "blue", alpha = 0.5)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

A related alternative to regression splines is smoothing splines. Suppose we choose $\hat{f}$ to minimize a modified least squares criterion 
$\frac{1}{n} \sum (Y_i - f(x_i))^2 + \lambda \int [f^{''}(x)]^2 dx$

where $\lambda > 0$ controls the amount of smoothing and $\int [f^{''}(x)]^2 dx$ is a roughness penalty. When $f$ is rough, the penalty is large, but when $f$ is smooth the penalty is small. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(ssf <- smooth.spline(x, y))
#+END_SRC 

#+BEGIN_SRC R :file plot.svg :results graphics file
ggplot(plt, aes(x = x, y = y)) +
    geom_point() +
    geom_line(aes(x = x, y = ssf$y), lty = 2, alpha = 0.5, color = "blue")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** Additive Models 

Searching for good transformations on predictors is difficult when there are multiple predictors. Changing the transformation on one predictor may change the best choice of transformation on another predictor. Fortunately, there is a way to simultaneously choose the transformations: 

An additive model takes the form: 

$y = \alpha + f_1(X_1) + f_2(X_2) + ... + f_p(X_p) + \epsilon$

The linear terms of the form $\beta_i X_i$ have been replaced with more flexible functional forms $f_i(X_i)$.

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
require(mgcv)

(gamod <- gam(sr ~ s(pop15) + s(pop75) + s(dpi) + s(ddpi), data = savings))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
library(mgcViz)

print(plot(getViz(gamod), allterms = T), pages = 1)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** Exercises

*** 1. 

**** Pre 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(aatemp, package = "faraway")
(aatemp %<>% as_tibble())

aatemp %>% skimr::skim()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
aatemp %>%
    ggpairs()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

**** a.  

Is there a linear trend?

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(lmod <- lm(temp ~ year, aatemp))
#+END_SRC

Not really. 

#+BEGIN_SRC R :file plot.svg :results graphics file
aatemp %>%
    ggplot(aes(x = year, y = temp)) +
    geom_point() +
    geom_abline(intercept = lmod$coefficients[[1]],
                slope = lmod$coefficients[[2]],
                lty = 2, color = "blue")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

**** b. 

Observations in successive years may be correlated. Fit a model that estimates this correlation. Does this change your opinion about the trend? 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(glmod <- gls(temp ~ year, correlation = corAR1(form = ~year), aatemp))
#+END_SRC

Our generalized least squares model with an ARMA(1, 0) coefficient for the yearly correlation gives a phi value of .23, or roughly the previous points account for 23% of the variance of the next point. 

#+BEGIN_SRC R :file plot.svg :results graphics file
aatemp %>%
    ggplot(aes(x = year, y = temp)) +
    geom_point() +
    geom_abline(intercept = glmod$coefficients[[1]],
                slope = glmod$coefficients[[2]],
                lty = 2, color = "blue")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

This doesn't change my opinion. 

**** c. 

Fit a polynomial model with degree 10 and use backward elimination to reduce the degree of the model. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
boost(temp ~ year, order = 10)

aatemp_mods <- map(10:1, ~ lm(boost(temp ~ year, order = .x), data = aatemp))

predict(aatemp_mods[[1]], newdata = tibble("year" = 2020))



plot_all_mods <- function(data, models) {
    plot_recurse <- function(plot_base, count) {
        if (count == length(models)) {
            plot_base
        } else {
            plot_base +
                geom_line(aes(x = year, y = predict(models[[count]], data)), color = "blue", alpha = (1 / count)) -> plot_base
            plot_recurse(plot_base, (count + 1))
        }
    }

    data %>%
        ggplot(aes(x = year, y = temp)) +
        geom_point(alpha = 0.3) -> supah_base
    
    plot_recurse(supah_base, 1)
}
#+END_SRC

Plot your fitted model on top of the data. 

#+BEGIN_SRC R :file plot.svg :results graphics file
(plot_all_mods(aatemp, aatemp_mods) +
    ggtitle("Polynomial Fits on Top of aayears") -> all_polynomials)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

Use this model to predict the temperature in 2020 

#+BEGIN_SRC R :file plot.svg :results graphics file
aatemp_mods %>%
    map(., ~ predict(.x, newdata = tibble("year" = 2020))) %>%
    flatten_dbl() %>%
    as_tibble() %>%
    mutate("year" = 2020, "degree" = 10:1) %>% 
    rename("temp" = value) -> new_preds

all_polynomials +
    geom_point(data = new_preds, aes(x = year, y = temp, group = degree, color = degree)) +
    scale_color_continuous() +
    ggforce::facet_zoom(xlim = c(2020, 2020),
                        zoom.size = 3)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

**** d. 

Suppose someone claims that a temperature was constant until 1930 and then began a linear trend. Fit a model corresponding to this claim. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
hslm <- function(data, formula, cutoff, formula2 = NULL) {
    lhs <- function(x) ifelse(x < cutoff, cutoff - x, 0)
    rhs <- function(x) ifelse(x >= cutoff, x - cutoff, 0)
    if (!is.null(formula2)) {
        new_formula <- as.formula(paste0(deparse(formula[[2]]),
                                  " ~ lhs(",
                                  deparse(formula[[3]]),
                                  ") + rhs(",
                                  deparse(formula2[[3]]),
                                  ")"))
        lm(new_formula, data)
    } else {
        hockey_stick_lm(data, formula, cutoff)
    }
}

(hslm_out <- hslm(data = aatemp, formula = as.formula("temp ~ year"), formula2 = as.formula("temp ~ year"), cutoff = 1930))
#+END_SRC

Use this model to predict the temperature in 2020. 

#+BEGIN_SRC R :file plot.svg :results graphics file
aatemp %>%
    ggplot(aes(x = year, y = temp)) +
    geom_point() +
    geom_vline(xintercept = 1930, color = "blue", alpha = 0.1) + 
    geom_line(data = tibble("temp" = hslm_out$fitted.values,
                            "year" = aatemp$year),
              aes(x = year, y = temp),
              lty = 2)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

The fitted model says that the rate of temperature increase has decreased roughly 3 fold in the period after 1930 from the period before 1930. 

**** e. 

Make a cubic spline fit with six basis functions evenly spaced on the range. Plot the fit in comparison to the previous fits. Does this model fit better than the straight line model? 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
knots <- c(0, 0, 0, 0,
           0.2, 0.4, 0.5, 0.6,
           0.7, 0.8, 0.85, 0.9,
           1, 1, 1, 1)

knots <- seq(from = extract2(range(aatemp$year), 1), to = extract2(range(aatemp$year), 2), length.out = 7)

(bx <- splineDesign(knots, aatemp$year, outer.ok = TRUE))

(lmodb <- lm(aatemp$temp ~ bx -1))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
ggplot(aatemp, aes(x = year, y = temp)) +
    geom_point() +
    geom_line(aes(x = year, y = lmodb$fit), lty = 2, color = "blue", alpha = 0.5)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

Wow this looks bad. I think something was wrong 
