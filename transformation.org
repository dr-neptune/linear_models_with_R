* Transformation
:PROPERTIES:
:header-args: :session R-session :results output value :colnames yes
:END:

#+NAME: round-tbl
#+BEGIN_SRC emacs-lisp :var tbl="" fmt="%.1f"
(mapcar (lambda (row)
          (mapcar (lambda (cell)
                    (if (numberp cell)
                        (format fmt cell)
                      cell))
                  row))
        tbl)
#+end_src

#+RESULTS: round-tbl

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(tidyverse)
library(faraway)
library(broom)
library(magrittr)
#+END_SRC

** Transforming the Response 

Suppose we want to use a logged response variable: $\log{y} = \beta_0 + \betax + \epsilon$. Then $y = \exp{\beta_0 + \beta_1 x} \cdot \exp{\epsilon}$.

In this case, the errors enter multiplicatively and not additively as they usually do. 

In practice, we may not know how the errors enter the model, additively, multiplicatively, or otherwise. The best approach is to try different transforms to get the structural form of the model right and then worry aobut the error component later. 

Regression coefficients will need to be interpreted with respect the the transformed scale. There is no straightforward way of back-transforming them to values that can be interpreted in the original scale. You can not directly compare regression coefficients for models where the response transformation is different. 

When you use a log transformation on the response, the regression coefficients have a particular interpretation: 

$\log \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + ... + \hat{\beta_p}x_p$
$\hat{y} = e^{\hat{\beta_0}}e^{\hat{\beta_1 x_1}}...e^{\hat{\beta_p}x_p}$

An increase of 1 in $x_1$ would multiply the predicted response (in the original scale) by $e^{\hat{\beta_1}}$. 

** Box-Cox Transformation 

The Box-Cox method is a popular way to determine a transformation on the response. It is designed for strictly positive responses and chooses the transformation to find the best fit to the data. 

The method transforms the response $y \to g_{\lambda}(y)$ where the family of transformations indexed by $\lambda$ is $g_{\lambda}(y) = \frac{y^{\lambda} - 1}{\lambda}$ if $\lambda \neq 0$ and $\log(y)$ otherwise.

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(savings, package = "faraway")
(savings %<>% as_tibble(rownames = "country"))

# fit model 
lmod <- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings)
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
p1 <- boxcox(lmod, plotit = T)
p2 <- boxcox(lmod, plotit = T, lambda = seq(0.5, 1.5, by = 0.1))
p2
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


The confidence interval for $\lambda$ runs from about 0.6 to 1.4. There is no good reason to transform. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# consider the Galapagos Islands dataset
data(gala, package = "faraway")
gala %<>% as_tibble(rownames = "Location")

(lmod <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
(bc <- boxcox(lmod, lambda = seq(-0.25, 0.75, by = 0.05), plotit = TRUE))
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

In the boxcox transformation, a cube root or square root transformation might be a good choice. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod %>% summary()

bc_out <- tibble("lambda" = bc$x,
                 "log_likelihood" = bc$y)

bc_out %>%
    arrange(desc(log_likelihood)) %>%
    pluck(1, 1) -> lambda_out

(lmod2 <- lm(((Species^lambda_out - 1) / lambda_out) ~ Area + Elevation + Nearest + Scruz + Adjacent, gala))
lmod2 %>% summary()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
p1 <- qq_plot(gala, Species, "Species")
p2 <- qq_plot(gala, ((Species^lambda_out - 1) / lambda_out), "Species Transformed")
p1 + p2
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

Some general considerations regarding the Box-Cox Transformation: 

1. The Box-Cox method gets upset by outliers. If $\hat{\lambda} = 5$, then this is probably the reason -- there can be little justification for such an extreme transformation. 
2. If some $y_i < 0$ is small, we can add a constant to all y. This is inelegant, but usable.
3. If $\frac{\max_i y_i}{\min_i y_i}$ is small, then the Box-Cox will not have much real effect because power transforms are well approximated by linear transformations over short intervals far from the origin.
4. There is some doubt whether the estimation of $\lambda$ counts as an extra parameter to be considered in the degrees of freedom. 

The Box-Cox method is not the only way of transforming the predictors. Another family of transformations is given by $g_{\alpha}(y) = \log(y + \alpha)$. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod <- lm(burntime ~ nitrogen + chlorine + potassium, leafburn)
lmod %>% summary()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
(lt_out <- logtrans(lmod, plotit = TRUE, alpha = seq(-min(leafburn$burntime) + 0.001, 0, by = 0.01)))
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

The recommended $\hat{\alpha}$ value is 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(tibble("lambda" = lt_out$x,
        "log_likelihood" = lt_out$y) %>%
 arrange(desc(log_likelihood)) %>%
 pluck(1, 1))
#+END_SRC

** Broken Stick Regression 

Sometimes we have reason to believe that different linear regression models apply to different regions of data. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# we could fit two regression models on different subsets of the data
lmod1 <- lm(sr ~ pop15, savings, subset = (pop15 < 35))
lmod2 <- lm(sr ~ pop15, savings, subset = (pop15 >= 35))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
savings %>%
    ggplot(aes(x = pop15, y = sr)) +
    geom_point() +
    geom_segment(intercept = lmod1$coefficients[[1]],
                slope = lmod1$coefficients[[2]],
                color = "blue") +
    geom_vline(xintercept = 35, lty = 2) + 
    geom_abline(intercept = lmod2$coefficients[[1]],
                slope = lmod2$coefficients[[2]],
                color = "green")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

A possible objection is that two two parts of the fit do not meet at the join. If we believe that the fit should be continuous as the predictor varies, we should consider the broken stick regression fit. 

Define two basis functions: 

$B_l(x) = c - x$ if $x < c$ and 0 otherwise
$B_r(x) = x - c$ if $x > c$ and 0 otherwise 

where $c$ marks the division between the two groups. We can now fit a model of the form: 

$y = \beta_0 + \beta_1 B_l(x) + \beta_2 B_r(x) + \epsilon$

The two linear parts are guaranteed to meet at $c$. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
hockey_stick_lm <- function(data, formula, cutoff) {
    lhs <- function(x) ifelse(x < cutoff, cutoff - x, 0)
    rhs <- function(x) ifelse(x >= cutoff, x - cutoff, 0)
    # such hack, much wow
    new_formula <- as.formula(paste0(deparse(formula[[2]]),
                                     " ~ lhs(",
                                     deparse(formula[[3]]),
                                     ") + rhs(",
                                     deparse(formula[[3]]), ")"))
    lm(new_formula, data)
}

(hslmod <- hockey_stick_lm(savings, as.formula(sr ~ pop15), 35))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
savings %>%
    ggplot(aes(x = pop15, y = sr)) +
    geom_point() +
    geom_vline(xintercept = 35, color = "blue", alpha = 0.1) + 
    geom_line(data = tibble("sr" = hslmod$fitted.values,
                            "pop15" = savings$pop15),
              aes(x = pop15, y = sr),
              lty = 2)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** Polynomials 

Another way of generalizing the $X \beta$ part of the model is to add polynomial terms. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
tidy(lm(sr ~ ddpi, savings))

# wooow
boost <- function(formula, order, count = 2, base_frm = NULL) {
    term <- formula[[3]]
    if (order == (count - 1)) return(formula)
    else {
        formula[[3]] <- bquote(.(formula[[3]]) + I(.(term)^.(count)))
        boost(formula, order, count + 1)
    }
}

boost <- function(formula, order) {
    term <- formula[[3]]
    bup <- function(formula, count) {
        if (order == (count - 1)) return(formula)
        else {
            formula[[3]] <- bquote(.(formula[[3]]) + I(.(term)^.(count)))
            bup(formula, count + 1)
        }
    }
    bup(formula, 2)
}

(frm <- as.formula(sr ~ ddpi))

(form2 <- boost(frm, 5))

# takes data, a formula, and a cutoff and returns a polynomial regression at the order for which the predictors stop being significant.
# Assumes an order 1 formula with only 1 predictor
build_you_a_polynomial_regression <- function(data, formula, cutoff = 0.05, order = 1, old_model = NULL) {    
    # make lmod, get p-value and test if below cutoff for predictor
    lmod <- lm(formula, data)
    cat("yes lmod\n\n")
    p_val <- tidy(lmod) %>% pluck(ncol(.), nrow(.))
    cat("yes pval\n\n")
    if (p_val < cutoff) {
        # recurse and increase the order of the formula
        cat("Woo-eee, here is the order!", order, "\n\n")
        build_you_a_polynomial_regression(data, boost(formula, order + 1), order = (order + 1), cutoff = cutoff, old_model = lmod)
    } else {
        # return the linear model of order - 1
        cat("here we goooo\n\n")
        if (order == 1) lmod
        else old_model
    }
}

tidy(build_you_a_polynomial_regression(savings, as.formula(sr ~ ddpi)))

# test against manual
tidy(lm(frm, savings)) %>% pluck(ncol(.), nrow(.))
tidy(lm(boost(frm, 2), savings)) %>% pluck(ncol(.), nrow(.))
tidy(lm(boost(frm, 3), savings)) %>% pluck(ncol(.), nrow(.))
#+END_SRC

If you remove lower order terms from a polynomial, do note that it has special meaning. Setting the intercept to 0 means the regression passes through the origin, which setting the linear term to 0 means that the response is optimized at a predictor value of 0. 

You have to refit the model each time a term is removed. This is inconvenient, and for large $d$ there can be a problem with numerical stability. Orthogonal polynomials get around this problem by defining: 

$z_1 = a_1 + b_1 x$
$z_2 = a_2 + b_2x + c_2x^2$
$z_3 = a_3 + b_3x + c_3 c^2 + d_3 x^3$
$z_n = a_1 + b_1x + ... + \xi_n x^n$

where the coefficients $a, b, ...$ are chosen so that $z_i^T z_j = 0$ when $i \neq j$. The expressions $z$ are called orthogonal polynomials. The poly() function constructs orthogonal polynomials. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(lmod <- lm(sr ~ poly(ddpi, 4), savings))
summary(lmod)
#+END_SRC

We can also define polynomials in more than one variable. These are sometimes called response surface models.

A second degree model would be $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_{11}x_1^2 + \beta_{22}x_2^2 + \beta_{12}x_1x_2$

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(lmod <- lm(sr ~ polym(pop15, ddpi, degree = 2), savings))
lmod %>% tidy()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
library(plotly)

# rows and columns describe a grid, and the cell value describes surface height
savings

pop15r <- seq(20, 50, len = 50)
ddpir <- seq(0, 20, len = 50)
pgrid <- expand.grid(pop15 = pop15r,
                     ddpi = ddpir)
pv <- predict(lmod, pgrid)
(outp <- matrix(pv, 50, 50))

plot_ly(z = outp, y = pop15r, x = ddpir, 
        type = "surface",
        contours = list(x = list(show = TRUE),
                        y = list(show = TRUE))) %>%
    layout(title = "Perspective Plot of Quadratic Surface",
           scene = list(
               yaxis = list(title = "Popn Under 15"),
               xaxis = list(title = "Growth"),
               zaxis = list(title = "Savings Rate")))
#+END_SRC

** Splines 

Polynomials have the advantage of smoothness, but each data point affects the fit globally. This is because the power functions used for the polynomials take nonzero values across the whole range of the predictor. In contrast, broken stick regression localizes the influence of each data point to its particular segemtn, but we do not have the same smoothness of the polynomials. We can combine the beneficial aspects of both of these methods - smoothness and local influence - by using B-spline basis functions. 

Suppose we know the true model is: 

$y = \sin^3(2 \pi x^3) + \epsilon$
$\epsilon \sim N(0, (0.1)^2)$


#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
funky <- function(x) sin(2 * pi * x^3)^3

plt <- tibble("x" = seq(0, 1, by = 0.01),
              "y" = funky(x) + 0.1 * rnorm(101))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
matplot(x, cbind(y, funky(x)), type = "pl", ylab = "y", pch = 20, lty = 1, col = 1)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :file plot.svg :results graphics file
plt %>%
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    geom_line(aes(x = x, y = funky(x)), color = "blue", alpha = 0.3)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
build_you_a_polynomial_regression(data = plt, formula = boost(as.formula(y ~ x), 2))

boost(y ~ x, 5)

tidy(lm(boost(y ~ x, 2), plt)) %>% pluck(ncol(.), nrow(.))
#+END_SRC
