* Transformation
:PROPERTIES:
:header-args: :session R-session :results output value :colnames yes
:END:

#+NAME: round-tbl
#+BEGIN_SRC emacs-lisp :var tbl="" fmt="%.1f"
(mapcar (lambda (row)
          (mapcar (lambda (cell)
                    (if (numberp cell)
                        (format fmt cell)
                      cell))
                  row))
        tbl)
#+end_src

#+RESULTS: round-tbl

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(tidyverse)
library(faraway)
library(broom)
library(magrittr)
#+END_SRC

** Transforming the Response 

Suppose we want to use a logged response variable: $\log{y} = \beta_0 + \betax + \epsilon$. Then $y = \exp{\beta_0 + \beta_1 x} \cdot \exp{\epsilon}$.

In this case, the errors enter multiplicatively and not additively as they usually do. 

In practice, we may not know how the errors enter the model, additively, multiplicatively, or otherwise. The best approach is to try different transforms to get the structural form of the model right and then worry aobut the error component later. 

Regression coefficients will need to be interpreted with respect the the transformed scale. There is no straightforward way of back-transforming them to values that can be interpreted in the original scale. You can not directly compare regression coefficients for models where the response transformation is different. 

When you use a log transformation on the response, the regression coefficients have a particular interpretation: 

$\log \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + ... + \hat{\beta_p}x_p$
$\hat{y} = e^{\hat{\beta_0}}e^{\hat{\beta_1 x_1}}...e^{\hat{\beta_p}x_p}$

An increase of 1 in $x_1$ would multiply the predicted response (in the original scale) by $e^{\hat{\beta_1}}$. 

** Box-Cox Transformation 

The Box-Cox method is a popular way to determine a transformation on the response. It is designed for strictly positive responses and chooses the transformation to find the best fit to the data. 

The method transforms the response $y \to g_{\lambda}(y)$ where the family of transformations indexed by $\lambda$ is $g_{\lambda}(y) = \frac{y^{\lambda} - 1}{\lambda}$ if $\lambda \neq 0$ and $\log(y)$ otherwise.

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(savings, package = "faraway")
(savings %<>% as_tibble(rownames = "country"))

# fit model 
lmod <- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings)
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
p1 <- boxcox(lmod, plotit = T)
p2 <- boxcox(lmod, plotit = T, lambda = seq(0.5, 1.5, by = 0.1))
p2
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


The confidence interval for $\lambda$ runs from about 0.6 to 1.4. There is no good reason to transform. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# consider the Galapagos Islands dataset
data(gala, package = "faraway")
gala %<>% as_tibble(rownames = "Location")

(lmod <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
(bc <- boxcox(lmod, lambda = seq(-0.25, 0.75, by = 0.05), plotit = TRUE))
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

In the boxcox transformation, a cube root or square root transformation might be a good choice. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod %>% summary()

bc_out <- tibble("lambda" = bc$x,
                 "log_likelihood" = bc$y)

bc_out %>%
    arrange(desc(log_likelihood)) %>%
    pluck(1, 1) -> lambda_out

(lmod2 <- lm(((Species^lambda_out - 1) / lambda_out) ~ Area + Elevation + Nearest + Scruz + Adjacent, gala))
lmod2 %>% summary()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
p1 <- qq_plot(gala, Species, "Species")
p2 <- qq_plot(gala, ((Species^lambda_out - 1) / lambda_out), "Species Transformed")
p1 + p2
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

Some general considerations regarding the Box-Cox Transformation: 

1. The Box-Cox method gets upset by outliers. If $\hat{\lambda} = 5$, then this is probably the reason -- there can be little justification for such an extreme transformation. 
2. If some $y_i < 0$ is small, we can add a constant to all y. This is inelegant, but usable.
3. If $\frac{\max_i y_i}{\min_i y_i}$ is small, then the Box-Cox will not have much real effect because power transforms are well approximated by linear transformations over short intervals far from the origin.
4. There is some doubt whether the estimation of $\lambda$ counts as an extra parameter to be considered in the degrees of freedom. 

The Box-Cox method is not the only way of transforming the predictors. Another family of transformations is given by $g_{\alpha}(y) = \log(y + \alpha)$. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod <- lm(burntime ~ nitrogen + chlorine + potassium, leafburn)
lmod %>% summary()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
(lt_out <- logtrans(lmod, plotit = TRUE, alpha = seq(-min(leafburn$burntime) + 0.001, 0, by = 0.01)))
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

The recommended $\hat{\alpha}$ value is 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(tibble("lambda" = lt_out$x,
        "log_likelihood" = lt_out$y) %>%
 arrange(desc(log_likelihood)) %>%
 pluck(1, 1))
#+END_SRC

** Broken Stick Regression 

Sometimes we have reason to believe that different linear regression models apply to different regions of data. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# we could fit two regression models on different subsets of the data
lmod1 <- lm(sr ~ pop15, savings, subset = (pop15 < 35))
lmod2 <- lm(sr ~ pop15, savings, subset = (pop15 >= 35))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
savings %>%
    ggplot(aes(x = pop15, y = sr)) +
    geom_point() +
    geom_segment(intercept = lmod1$coefficients[[1]],
                slope = lmod1$coefficients[[2]],
                color = "blue") +
    geom_vline(xintercept = 35, lty = 2) + 
    geom_abline(intercept = lmod2$coefficients[[1]],
                slope = lmod2$coefficients[[2]],
                color = "green")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

A possible objection is that two two parts of the fit do not meet at the join. If we believe that the fit should be continuous as the predictor varies, we should consider the broken stick regression fit. 

Define two basis functions: 

$B_l(x) = c - x$ if $x < c$ and 0 otherwise
$B_r(x) = x - c$ if $x > c$ and 0 otherwise 

where $c$ marks the division between the two groups. We can now fit a model of the form: 

$y = \beta_0 + \beta_1 B_l(x) + \beta_2 B_r(x) + \epsilon$

The two linear parts are guaranteed to meet at $c$. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
hockey_stick_lm <- function(data, formula, cutoff) {
    lhs <- function(x) ifelse(x < cutoff, cutoff - x, 0)
    rhs <- function(x) ifelse(x >= cutoff, x - cutoff, 0)
    # such hack, much wow
    new_formula <- as.formula(paste0(deparse(formula[[2]]),
                                     " ~ lhs(",
                                     deparse(formula[[3]]),
                                     ") + rhs(",
                                     deparse(formula[[3]]), ")"))
    lm(new_formula, data)
}

(hslmod <- hockey_stick_lm(savings, as.formula(sr ~ pop15), 35))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
savings %>%
    ggplot(aes(x = pop15, y = sr)) +
    geom_point() +
    geom_vline(xintercept = 35, color = "blue", alpha = 0.1) + 
    geom_line(data = tibble("sr" = hslmod$fitted.values,
                            "pop15" = savings$pop15),
              aes(x = pop15, y = sr),
              lty = 2)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** Polynomials 

Another way of generalizing the $X \beta$ part of the model is to add polynomial terms. 

