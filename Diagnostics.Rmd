```{r}
library(tidyverse)
library(faraway)
library(magrittr)
library(broom)
library(gganimate)
library(furrr)
library(cowplot)
plan(multiprocess)
```

# Diagnostics 

The estimation of and inference from the regression model depend on several assumptions. These assumptions should be checked using regression diagnostics before using the model in earnest. 

We can divide the problems into three categories: 

**Error** : we have assumed that our errors are independent, have equal variance, and are normally distributed. 

**Model** : we have assumed that the structural part of the model, $E[y] = X\beta$, is correct. 

**Unusual Observations** : Sometimes a few observations do not fit the model and they might change the choice and fit of the model. 

# Checking Error Assumptions

We wish to check for independence, constant variance, and normality of the errors. We can examine the residuals, $\hat{\epsilon}$.

Recall that $\hat{y} = X(X^TX)^{-1}XTy = Hy$ where $H$ is the hat matrix, so that 

<center>
$\hat{\epsilon} = y - \hat{y} = (I - H)y = (I - H)X\beta + (I - H)\epsilon = (I - H)\epsilon$
</center>

Therefore, var $\hat{\epsilon}$ = var $(I - H)\epsilon$ = $(I - H)\sigma^2$ assuming var ${\epsilon} = \sigma^2 I$. 

We see that while the errors may have equal variance and be uncorrelated, the residuals do not. Fortuneately, correlation of errors usually has little imact and can be applied to residuals in order to check the assumptions on the error. 

## Constant Variance 

We can not check the assumption of constant variance just by examining the residuals alone. We need to check whether the variance in the residuals is related to some other quantity. 

The most useful diagnostic is a plot of $\hat{\epsilon}$ against $\hat{y}$, or residuals against fitted. If all is well, we should see constant symmetrical variation (known as homoscedasticity) in the vertical $\hat{\epsilon}$ direction. **Nonconstant** variance is called heteroscedasticity. Nonlinearity of the structural part of the model can also be detected in this plot. 

It is also worthwhile to plot $\hat{\epsilon}$ against $x_i$ for potential predictors that are in the current model as well as those that are not used. For plots of residuals against predictors that are not in the model, any observed structure may indicate that this predictor should be included in the model. 

```{r}
data(savings, package = "faraway")
savings %<>% as_tibble()

(lmod <- lm(sr ~ ., savings))

lm_aug <- lmod %>% augment()

diag_plot <- function(data, x, y) {
    data %>%
        ggplot(aes(x = {{x}}, y = {{y}})) +
        geom_point() +
        geom_hline(yintercept = 0, lty = 2, alpha = 0.5)
}

lm_aug %>%
    diag_plot(x = .fitted, y = .resid)
```

The plot above shows that there likely no heteroscedasticity, or nonconstant variance in the error. We can examine the constant variance more closely by plotting $\sqrt{|\hat{\epsilon}|}$ against $\hat{y}$. Considering the absolute value of the residuals roughly doubles the resolution. For truly normal errors, $|\hat{\epsilon}|$ would follow what is known as a half normal distribution (since it has a density which is simply the upper half of a normal density). Such a distribution is quite skewed, and the skewness can be reduced by the square root transformation.

```{r}
lm_aug %>%
    diag_plot(x = .fitted, y = sqrt(abs(.resid)))
```

We see approximately constant variation in the plot above. 

We can also perform a numerical test to check nonconstant variance

```{r}
lm(sqrt(abs(.resid)) ~ .fitted, lm_aug) %>% summary()
```

This test checks for a linear trend in the variation. As a result, it might be good at detecting a particular kind of nonconstant variance, but have no power to detect another (think Simpson's paradox). Residual plots are generally more versatile here because unanticipated problems may be spotted.

It is often hard to judge residual plots. Here are four plots which show:

1. Constant Variance 
2. Strong Nonconstant Variance 
3. Mild Nonconstant Variance 
4. Nonlinearity 

```{r}
map(seq_len(10), ~ tibble("x" = runif(100),
                          "a" = rnorm(100),
                          "b" = x * rnorm(100),
                          "c" = sqrt(x) * rnorm(100),
                          "d" = cos(x * pi / 25) +
                              rcauchy(n = 100, location = 5, scale = 5),
                          "gen_group" = .x)) %>%
    bind_rows() -> resid_gen

animate_resids <- function(data, x, y, title) {
    data %>%
        diag_plot(x = {{x}}, y = {{y}}) +
        ggtitle(as.character(title)) +
        transition_states(
            gen_group,
            transition_length = 10,
            state_length = 1) +
        enter_fade() +
        exit_shrink()
}

resid_gen %>%
    animate_resids(x = x, y = a, title = "Constant Variance") -> p1g

resid_gen %>%
    diag_plot(x, a) +
    ggtitle("Constant Variance") -> p1

resid_gen %>%
    animate_resids(x = x, y = b, title = "Strong Nonconstant Variance") -> p2g

resid_gen %>%
    diag_plot(x, b) +
    ggtitle("Strong Nonconstant Variance") -> p2

resid_gen %>%
    animate_resids(x = x, y = c, title = "Mild Nonconstant Variance") -> p3g

resid_gen %>%
    diag_plot(x, c) +
    ggtitle("Mild Nonconstant Variance") -> p3

resid_gen %>%
    animate_resids(x = x, y = d, title = "Nonlinearity") -> p4g

resid_gen %>%
    diag_plot(x, d) +
    ggtitle("Nonlinearity") -> p4

cowplot::plot_grid(p1, p2, p3, p4, ncol = 2)

list("constant_var" = p1g, "strong_nonconstant_var" = p2g,
     "mild_nonconstant_var" = p3g, "nonlinearity" = p4g) %>%
    imap(., ~ anim_save(path = getwd(),
                       filename = paste0(.y, ".gif"),
                       animation = .x))
```

Now we can look at some residuals against predictor plots for the savings data 

```{r}
resid_plots <- with(lm_aug, list("pop15" = pop15, "pop75" = pop75,
                  "dpi" = dpi, "ddpi" = ddpi)) %>%
    imap(., ~ lm_aug %>% diag_plot(x = .x, y = .resid) +
        xlab(.y) + ylab("residuals"))

cowplot::plot_grid(plotlist = resid_plots, ncol = 2)
```

In the first plot, for population under 15, we can see two groups. We can compare and test the variances in these groups. 

Given two independent samples from normal distributions, we can test for equal variance using the test statistic of the ratio of the two variances. The null distribution is an F with degrees of freedom given by the two samples

```{r}
var.test(lm_aug$.resid[lm_aug$pop15 > 35], lm_aug$.resid[lm_aug$pop15 <= 35])
```

We see that there is a significant difference with our p-value of 0.01358 and we fail to reject our alternative hypothesis that the true ratio of variances is not equal to 1. 

When problems are seen in the diagnostic plots, some modification of the model is suggested. 

- If some nonlinearity is observed, perhaps in conjunction with nonconstant variance, a transformation of the variables should be considered. 

- If the problem is solely one of nonconstant variance with no suggestion of nonlinearity, then the use of weighted least squares may be appropriate.

Alternatively, when non constant variance is seen in the plot of the residuals against fitted values, a transformation of the response y to h(y) where h() can be chosen so that var h(y) is constant should be considered. 

To see how to choose h: 

$h(y) = h(Ey) + (y - Ey)h'(Ey) + ...$
$var h(y) = 0 + h'(Ey)^2 var y + ...$
$h'(Ey) \propto (var y)^{-1/2}$
$h(y) = \int \frac{dy}{\sqrt{var y}} = \int \frac{dy}{SD(y)}$

For example, if $var y = var \epsilon \propto (Ey)^2$, then $h(y) = \log y$ is suggested, while if $var \epsilon \propto (Ey)$, then $h(y) = \sqrt{y}$

Sometimes it can be difficult to find a good transformation. For example when $y_i \leq 0$ for some $i$, square root or log transformations will fail. 

Consider the residuals vs fitted plot for the Galapagos data:

```{r}
data(gala, package = "faraway")
lmod <- lm(Species ~ Area + Elevation + Scruz + Nearest + Adjacent, gala)

(lmod %>% augment() %>%
    diag_plot(x = .fitted, y = .resid) -> p1)
```

The first plot has non constant variance (and evidence of nonlinearity). The square root transformation is often appropriate for count response data. The Poisson distribution is a good model for counts and that distribution has the property that the mean is equal to the variance, thus suggesting the square root transformation. 

```{r}
lmod <- lm(sqrt(Species) ~ Area + Elevation + Scruz + Nearest + Adjacent, gala)

(lmod %>% augment %>%
    diag_plot(x = .fitted, y = .resid) -> p2)
```

We see in the second plot that the variance is now constant and the signs of nonlinearity have gone. Our guess at a variance stabilizing transformation worked here, but we could have always tried something else 

```{r}
cowplot::plot_grid(p1, p2, ncol = 2)
```

# Normality 

The tests and confidence intervals we use are based on the assumption of normally distributed errors. The residuals can be assessed for normality using a QQ plot. This compares the residuals to ideal normal observations. 

```{r}
lmod <- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings)

lmod %>%
    augment() %>%
    ggplot(aes(sample = .resid)) +
    stat_qq() +
    stat_qq_line() -> p1

lmod %>%
    augment() %>%
    ggplot(aes(x = .resid)) +
    geom_histogram(bins = 30, fill = "skyblue", color = "black") -> p2

cowplot::plot_grid(p1, p2, ncol = 1)
```

Normal residuals should follow the line approximately. The histogram should be the familiar bell shape. 

We can get an idea of the variation to be expected in QQ plots in the following simulation. We generate data from different distributions:

1. Normal 
2. Log-Normal (an example of a skewed distribution)
3. Cauchy (an example of a long tailed (leptokurtic) distribution)
4. Uniform (an example of a short tailed (platykurtic) distribution)

```{r}
map(seq_len(10), ~ tibble("x" = rnorm(100),
                          "a" = rnorm(100),
                          "b" = exp(rnorm(100)),
                          "c" = rcauchy(100),
                          "d" = runif(100),
                          "gen_group" = .x)) %>%
    bind_rows() -> resid_gen

qq_plot <- function(data, choice, title) {
    data %>%
        ggplot(aes(sample = {{choice}})) +
        stat_qq() +
        stat_qq_line() +
        ggtitle(title)
}

qq_plots <- with(resid_gen, list("Normal" = a,
                                 "Log-Normal | Skewed" = b,
                                 "Cauchy | Long-Tail" = c,
                                 "Uniform | Short-Tail" = d)) %>%
    imap(., ~ resid_gen %>%
         qq_plot(choice = .x, title = .y))

cowplot::plot_grid(plotlist = qq_plots, ncol = 2)
```


Sometimes extreme cases may be the sign of a long tailed error like the Cauchy distribution, or they can just be outliers. If removing outliers results in other points becoming more prominent in the plot, the problem is likely due to a long tail error.

When the errors are not normal, least squares estimates may not be optimal. They will still be the best linear unbiased estimates, but other robust estimators may be more effective. Tests and confidence intervals will not be exact, but we can still appeal to the central limit theorem to assert that intervals constructed will be increasingly accurate approximations for larger sample sizes. 

When non-normality is found, the resolution depends on the type of problem found:

- Short tailed distributions are not serious usually and can generally be ignored 
- Skewed errors may be ameliorated by a transformation 
- Long tailed errors may require us accepting non-normality and basing the inference on the assumption of another distribution or use resampling methods like the bootstrap or permutation tests. Alternatively, we can use robust methods which lend less weight to outliers but may again require resampling for the inference. 

A formal test for normality is the Shapiro-Wilk test:

```{r}
shapiro.test(residuals(lmod))
```

In this test, the null hypothesis is that the residuals are normal. In our case, we have a p-value of 0.8524 and thus we do not reject the null hypothesis. The p-value is not helpful as an indicator of what action to take, so it is reccommended to use this in conjunction with a QQ plot. 

# Correlated Errors

It is difficult to check for correlated errors in general because there are too many possible patterns of correlation that may occur. Some types of data have a structure which suggests where to look for problems. 

- Data collected over time may have some correlation in successive errors 
- Spatial data may have correlation in the errors of nearby measurements 
- Data collected in blocks may show correlated errors within those blocks

We can illustrate the methods with a time series analysis, but we will use methods more specific to the regression diagnostics problem. 

```{r}
data(globwarm, package = "faraway")
globwarm %<>% as_tibble()

# fit model
(lmod <- lm(nhtemp ~ ., globwarm))

# grab augment df
lm_aug <- lmod %>% augment()
```

For temporal data like these, it is sensible to plot the residuals against the time index. 

```{r}
make_convex_hull <- function(data, x, y,
                      plot = TRUE,
                      alpha = 0.3,
                      fill = "cadetblue1",
                      color = "black") {
    hull <- data %>%
        slice(chull({{x}}, {{y}}))

    if (plot) {
        geom_polygon(data = hull,
                     alpha = alpha,
                     fill = fill,
                     color = color)
    } else {
        hull
    }
}

(lm_aug %>%
    na.omit() %>%
    ggplot(aes(x = year, y = .resid)) +
    geom_point() +
    geom_hline(yintercept = 0, lty = 2) +
    make_convex_hull(data = lm_aug,
                     x = year, y = .resid) +
    ggalt::geom_xspline(alpha = 0.3) -> p1)
```

If the errors were uncorrelated, we would expect a random scatter of points above and below the epsilon = 0 line. This looks like a time series pattern, so we have an indicator of positive serial correlation.

Another way to approach checking for serial correlation is to plot successive pairs of residuals

```{r}
n <- length(residuals(lmod))
plot(tail(residuals(lmod), n - 1) ~ head(residuals(lmod), n - 1),
     xlab = expression(hat(epsilon)[i]),
     ylab = expression(hat(epsilon)[i + 1]))
abline(h = 0, v = 0, col = grey(0.75))

lm_aug %>% nrow()

make_successive_res_plot <- function(data) {
    # get data length
    n <- data %>% nrow()

    # add lead lag
    data %<>%
    mutate(lead = .resid[2:nrow(.)] %>% append(0),
           lag = .resid[1:(nrow(.) - 1)] %>% append(0))

    # fit slope
    lm_in <- lm(lead ~ lag, data)

    data %>%
        ggplot(aes(x = lead, y = lag)) +
        geom_point() +
        geom_hline(yintercept = 0, lty = 2, alpha = 0.3) +
        geom_vline(xintercept = 0, lty = 2, alpha = 0.3) +
        geom_abline(slope = lm_in$coefficients[[2]],
                    intercept = lm_in$coefficients[[1]],
                    color = "blue",
                    alpha = 0.3)
}

(lm_aug %>%
    make_successive_res_plot() -> p2)

cowplot::plot_grid(p1, p2, ncol = 2)
```
We can also model the significance of the correlation directly

```{r}
lm_aug %<>%
    mutate(lead = .resid[2:nrow(.)] %>% append(0),
           lag = .resid[1:(nrow(.) - 1)] %>% append(0))

lm(lead ~ lag -1, lm_aug) %>% summary()
```

We omitted the intercept because the residuals have mean zero. The serial correlation is confirmed, as the estimate is nonzero and the p-value is very small. 

We can also plot more than just successive pairs if we suspect a more complex dependence. 

The Durbin-Watson test uses the statistic 

$DW = \frac{\sum_{i=2}^n (\hat{\epsilon_i} - \hat{\epsilon_{j-1}})^2}{\sum_{i = 1}^n \hat{\epsilon_i}^2}$

It essentially looks at the successive residuals, squares them, and normalizes their sum by the sum of squared residuals.

The null hypothesis is that the errors are uncorrelated. The null distribution based on the assumption of uncorrelated errors follows a linear combination of Chi Squared distributions. 

```{r}
# run Durbin-Watson statistic
lmtest::dwtest(nhtemp ~ wusa + jasper + westgreen +
                   chesapeake + tornetrask + urals +
                   mongolia + tasman, data = globwarm)
```

In this case the null hypothesis is that the true autocorrelation is zero, and the alternative hypothesis is that the true autocorrelation is greater than zero. Our p-value is really small (1.402 * 10^-15), so we can reject the null hypothesis and assert that we have autocorrelation. 

Sometimes serial correlations can be caused by a missing covariate. Although it is possible that difficulties with correlated errors can be removed by changing the structural part of the model, sometimes we must build the correlation directly into the model. This can be achieved with the method of generalized least squares.

# Finding Unusual Observations

Outliers do not fit the model well. Influential observations can change the fit of a model in a substantive manner. A leverage point is extreme in the predictor space. It is important to identify such points.

# Leverage 

$h_i = H_{ii}$ are called leverages and are useful diagnostics. Since var $\hat{\epsilon_i} = \sigma^2(1 - h_i)$, a large leverage $h_i\$ will make the var $\hat{\epsilon_i}$ small. The fit will be attracted toward $y_i\$. Large values of $h_i$ are due to extreme values in the X space. $h_i\$ corresponds to a squared Mahalonobis distance defined by X which is $(x - \bar{x})^T \hat{\Sigma}^{-1}(x - \bar{x})$ where $\hat{\Sigma}\$ is the estimated covariance of X. The value of $h_i$ depends only on X and not y, so leverages contain only partial information about a case. 

Since $\sum_i h_i = p$, an average value for $h_i$ is $p/n$. A rough rule is that leverages of more than $2p / n$ should be looked at more closely. 

```{r}
lmod <- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings)

lm_aug <- lmod %>% augment()
lm_aug %>% select(1:5, .hat)

lm_aug %>% select(.hat) %>% summarize("sum of leverages" = sum(.))
```

We verify that the sum of the leverages is 5, the number of parameters in the model. Without making assumptions about the distributions of the predictors that would often be unreasonable, we cannot say how the leverages would be distributed. 

In order to identify unusually large values of the leverage, we can use the half-normal plot. These are designed for the assessment of positive data. 

The idea is to plot the data against the positive normal quantiles.

1. Sort the data $x_{[1]} \leq ... \leq x_{[n]}$ 
2. Compute $u_i = /Phi^{-1}(\frac{n + i}{2n + 1})$
3. Plot $x_{[i]} ahainst u_i$

We do not expect a straight line relationship since we do not necessarily expect a positive normal distribution for quantities like leverages. We are mainly looking for outliers that stand out. 

```{r}
countries <- row.names(savings)
gghalfnorm::gghalfnorm(lm_aug$.hat, labs = countries, ylab = "leverages")
```

Leverages can also be used in scaling residuals. We have var $\hat{\epsilon_i} = \sigma^2(1 - h_i)$ which suggests the use of $r_i = \frac{\hat{\epsilon_i}}{\hat{\sigma} \sqrt{1 - h_i}}$.

These are called standardized residuals. If the model assumptions are correct, var $r_i$ = 1 and corr($r_i, r_j$) tends to be small. Standardized residuals have equal variance. 

Standardization can only correct for the natural non constant variance in residuals when the errors have constant variance. If there is some underlying heteroscedasticity in the errors, standardization cannot correct for it. 

```{r}
lm_aug %>%
    qq_plot(choice = .std.resid, title = "Standardized Residuals")
```

Since the residuals have been standardized, we expect the points to approximately follow the y = x line if normality holds. An advantage of the standardized form is that we can judge the size easily. An absolute value of 2 would be large, but not exceptional for a standardized residual, whereas an absolute value of 4 would be very unusual under the standard normal. 

# Outliers 

An outlier test is useful because it enables us to distinguish between truly unusual observations and residuals that are large, but not exceptional. 

```{r}
set.seed(8888)

testdata <- tibble(x = 1:15 + rnorm(15),
                   y = 1:15 + rnorm(15)) %>%
    add_row(x = 1:15 + rnorm(15),
                   y = 1:15 + rnorm(15))

leverage_plot <- function(data, x, y, levg_x, levg_y,
                   colors = c("mediumpurple", "firebrick1",
                              "skyblue", "orange")) {
    set.seed(8888)
    pred_tib <- tibble(x = levg_x, y = NA)

    data %<>%
        add_row(x = levg_x, y = levg_y)

    (lm_1 <- lm(y ~ x, data[1:(nrow(data) - 1), ]))
    (lm_2 <- lm(y ~ x, data))

    data %>%
        ggplot(aes(x = x, y = y)) +
        geom_point() +
        geom_point(data = tibble(x = c(levg_x, levg_x),
                                 y = c(levg_y, predict(lm_2,
                                                       pred_tib))),
                   aes(x = x, y = y),
                   color = colors[[1]],
                   shape = 13,
                   size = 5) +
        geom_segment(aes(x = levg_x,
                         y = levg_y,
                         xend = levg_x,
                         yend = predict(lm_2, pred_tib)),
                     lty = 2, alpha = 0.5, color = colors[[2]]) +
        geom_segment(aes(x = levg_x,
                         y = predict(lm_1, pred_tib),
                         xend = levg_x,
                         yend = predict(lm_2, pred_tib)),
                     lty = 2, alpha = 0.5, color = colors[[3]]) +
        geom_segment(aes(x = levg_x,
                         y = predict(lm_2, pred_tib),
                         xend = (predict(lm_2, pred_tib) -
                             lm_1$coefficients[[1]]) / lm_1$coefficients[[2]],
                         yend = predict(lm_2, pred_tib)),
                     lty = 2, alpha = 0.5, color = colors[[4]]) +
        geom_abline(intercept = lm_1$coefficients[[1]],
                    slope = lm_1$coefficients[[2]]) +
        geom_abline(intercept = lm_2$coefficients[[1]],
                    slope = lm_2$coefficients[[2]],
                    lty = 2) +
        xlim(c(0, 15)) + ylim(c(0, 15)) + 
        ggtitle("Leverage Difference in Linear Model")
}

leverage_plot(data = testdata, x = x, y = y, levg_x = 1.5, levg_y = 12)

# generate leverage plot gif
## sequence_levg <- seq(0, 15, 0.5)

## grid_seq <- expand.grid(sequence_levg,
##                         sequence_levg)

## output_plots <- future_map2(.x = grid_seq[,1],
##                             .y = grid_seq[,2],
##                             ~ leverage_plot(data = testdata,
##                                             x = x, y = y,
##                                             levg_x = .x,
##                                             levg_y = .y))

## dir.create("levgs")

## output_plots %>%
##     future_map(seq_len(length(output_plots)), ~ output_plots[[.x]] %>%
##                        ggsave(plot = .,
##                               filename = glue::glue("levgs/frame{.x}.png")))

## system("convert -delay 50 levgs/*.png levgs.gif")
```

To detect points with a lot of influence, we exclude point i and recompute the estimates to get $\hat{\beta_{(i)}}$ and $\hat{\sigma_{(i)}}^2$ where $(i)$ denotes that the $i^{th}$ case has been excluded. 

Hence $\hat{y_{(i)}} = x_i^T \hat{\beta_{(i)}}$. If $\hat{y_{(i)}} - y_i$ is large, then case $i$ is an outlier. To judge the size of a potential outlier, we need an appropriate scaling. 

We find: $\hat{var}(y_i - \hat{y_{(i)}}) = \hat{\sigma_{(i)}}^2 (1 + x_i^T(X_{(i)}^TX_{(i)})^{-1}x_i)$, and so we define the studentized (sometimes called jackknife or crossvalidated) residuals as 

$t_i = 
\frac{y_i - \hat{y_{(i)}}}{\hat{\sigma_{(i)}}(1 + x_i^T(X_{(i)}^TX_{(i)})^{-1}x_i)^{1/2}}=
\frac{\hat{\epsilon_i}}{\hat{\sigma}_{(i)}\sqrt{1 - h_i}} = 
r_i(\frac{n - p - 1}{n - p - r_i^2})^{1/2}
$

Which are distributed $t_{n-p-1}$ if the model is correct and $\epsilon ~ N(0, \sigma^2 I)$.

Since $t_i ~ t_{n - p - 1}$, we can calculate a p-value to test whether case $i$ is an outlier. However, we don't want to fall victim to statistical noise -- if we tested 100 pts with a 5% significance level, we would expect 5 outliers. 

Some adjustment of the level of the test is necessary to avoid identifying an excess of outliers. Suppose we want a level $\alpha$ test. Now P(all tests accept) = 1 - p(at least one rejects) $\geq 1 - \sum_j$ P(test i rejects) = $1 - n\alpha$. This suggests that if an overall level $\alpha$ test is required, then a level $\alpha / n$ should be used in each of the tests. This is called the **Bonferroni Correction** and it is used in contexts other than outliers. The larger n is, the more conservative it gets. 

```{r}
# generate studentized residuals
studentized_resid <- rstudent(lmod)

# get max of the studentized residuals
studentized_resid[which.max(abs(studentized_resid))]

# This is large for a standard normal scale, but is it an outlier? Compute Bonferroni
qt(.05 / (50 * 2), 44)
```

Since 2.85 is less than 3.53, we conclude that observation 46 is not an outlier. This indicates that it is not worth the trouble of comptuing the outlier test p-value unless the studentized residual exceeds about 3.5 in absolute value. 

**Points to Consider**

1. Two or more outliers next to each other can hide each other. 
2. An outlier in one model may not be an outlier in another when the variables have been changed or transformed. We will need to reinvestigate the question of outliers when we change the model. 
3. The error distribution may not be normal, so larger residuals may be expected. 
4. Individual outliers have less of an effect as we add more data. In large datasets, we generally only need to worry about clusters of outliers. Such clusters are less likely to occur by chance and more likely to represent actual structure.

**What to do**

1. Check for data entry error 
2. Examine the physical context. We may want to look for outliers, e.g. for things like fraud 
3. Exclude the point, but try reincluding it later if the model is changed. The exclusion of one or more outliers may make the difference between statistical significance or not. Be aware that casual or dishonest exclusion of outliers is regarded as serious research malpractice. 
4. Suppose we find outliers which are viewed as naturally occurring. Particularly if there are substantial numbers of such points, it is more efficient and reliable to use robust regression. Some adjustment to the inferential methods is necessary in such cases; in particular the uncertainty assessment for prediction needs to reflect the fact that extreme values can occur. 
5. It is dangerous to exclude outliers in an automatic manner. 

Here is an example of a dataset with multiple outliers

```{r}
data(star, package = "faraway")
star %<>% as_tibble()

(star %>%
    ggplot(aes(x = temp, y = light)) +
    geom_point() +
    xlab("log(Temperature)") + ylab("log(Light Intensity)") -> base_plot)

# fit a regression and add it to the plot
lmod <- lm(light ~ temp, star)

base_plot +
    geom_abline(intercept = lmod$coefficients[[1]],
                slope = lmod$coefficients[[2]],
                lty = 4, color = "mediumpurple")

# test for outliers
range(rstudent(lmod))
qt(.05 / (47 * 2), 45)
```

No outliers are found, even though we can clearly see them in the plot. 

Let's see what happens when we exclude the four upper left points (giants)

```{r}
lmodi <- lm(light ~ temp, data = star, subset = (temp > 3.6))

base_plot +
    geom_abline(intercept = lmodi$coefficients[[1]],
                slope = lmodi$coefficients[[2]],
                lty = 4, color = "mediumpurple") +
    geom_abline(intercept = lmod$coefficients[[1]],
                slope = lmod$coefficients[[2]],
                lty = 4, color = "skyblue")
```

This illustrates the problem of multiple outliers. We can visualize the problem here, but it would be much more difficult with higher dimensions. Generally, robust regression methods would be superior here.

# Influential Observations 

An influential point is one whose removal from the dataset would cause a large change in the fit. It may or may not be an outlier or have a large leverage, but it tends to have at least one of these properties.

We can consider the Cook statistics. They reduce the information to a single value for each case, and are defined as 

$D_i = \frac{(\hat{y} - \hat{y_{(i)}})^T(\hat{y} - \hat{y_{(i)}})}{p \hat{\sigma}^2} = \frac{1}{p}r_i^2 \frac{h_i}{1 - h_i}$


Where the first term $r_i^2$ is the residual effect and the second is the leverage. 

We can use the halfnormal plot of $D_i$ to identify influential observations

```{r}
lmod <- lm(sr ~ ., savings)

# generate halfnorm plot
(lmod %>%
    augment() %>%
    .$.cooksd %>%
    gghalfnorm::gghalfnorm() -> p_cooksd)


# exclude the largest influence point and refit
lmodi <- lm(sr ~ ., savings %>% slice(-46))

lmodi %>% tidy()

# compared to the full data fit
lmod %>% tidy()

lmod %>% augment() %>% glimpse()

dfbeta(lmod) %>%
    as_tibble() %>%
    select(pop15) %>%
    nrow()

## loo_diff <- function(model, param) {
##     differences <- dfbeta(model) %>%
##         as_tibble() %>%
##         select({{param}})

##     differences %>%
##         mutate(Index = row_number()) %>%
##         ggplot(aes(x = Index, y = {{param}})) +
##         geom_point() +
##         geom_hline(yintercept = 0, lty = 2) +
##         ggtitle(latex2exp::TeX("$\\hat{\\beta} - \\hat{\\beta_{(i)}}$"),
##                 subtitle = "Changes in Coefficient per Datum") +
##         scale_x_continuous(breaks = seq_len(nrow(differences)))
## }

loo_diff <- function(model, param, top_n = 3) {
    quo_param <- expr(param) %>% eval()

    (differences <- dfbeta(model) %>%
         as_tibble() %>%
         select({{quo_param}}) %>%
         rename(Value = 1) %>%
         mutate(Index = row_number()))

    # get max differences
    differences %>%
        mutate(abs_max = abs(Value)) %>%
        arrange(desc(abs_max)) %>%
        head(top_n) %>%
        select(Value, Index) -> top_x

    differences %>%
        ggplot(aes(x = Index, y = Value)) +
        geom_point() +
        geom_point(data = top_x, aes(x = Index, y = Value),
                   color = "red", shape = 1, size = 5, alpha = 0.7) +
        geom_hline(yintercept = 0, lty = 2, alpha = 0.5) +
#        ylab(param) +
        ggtitle(subtitle = latex2exp::TeX("$\\hat{\\beta} - \\hat{\\beta_{(i)}}$"),
                label = paste0(param, " | Changes in Coefficient per Datum")) +
        scale_x_continuous(breaks = seq_len(nrow(differences)))
}

loo_params <- function(model, top_n = 3) {
    model %>%
        tidy() %>%
        pull(1) %>%
        .[2:length(.)] %>%
        map(., ~ loo_diff(model, .x, top_n)) -> plots_out

    cowplot::plot_grid(plotlist = plots_out, ncol = 2)
}

lmod %>% loo_params()
```

The 23rd row sticks out as causing a large shift in the coefficient of pop_15. We can examine the effect of removing it 

```{r}
lmodj <- lm(sr ~ ., savings %>% slice(-23))

lmod %>% summary()
lmodj %>% summary()
```

By removing the 23rd observation, we see a few changes: 

- ddpi is no longer significant 
- our R squared value has dropped by about 6%. 

```{r}
plot(lmodj)
```

# Checking the Structure of the Model 

We can also use diagnostics to detect deficiencies in the structural part of the model, given by $E[Y] = X \beta$. The residuals are the best clues. 

We have already discussed plots of $\hat{\epsilon}$ against $\hat{y}$ and $x_i$. We used these plots to check the assumptions on the error, but they can also suggest transformations of the variables which might improve the structural form of the model. 

Normally we check the plots of y against each $x_i$ before model fitting. The drawback to this is that the other predictors often affect the relationship between a given predictor and the response. 

**Partial Regression** or **Added Variable** plots can help isolate the effects of $x_i$ on y. We regress y on all x except \$x_i$ and get residuals $\hat{\delta}$. These represent y with all other X-effects taken out. 

Similarly if we regress $x_i$ on all x except $x_i$, and get residuals $\hat{\gamma}$, we have the effect of each $x_i$ with the other X-effect taken out. 

```{r}
partial_regression_plot <- function(data, response, var) {
    # get vars that aren't our var
    vars <- colnames(data) %>%
        str_remove(c({{response}}, {{var}})) %>%
        .[. != ""] %>%
        paste(collapse = " + ")

    formula_1 <- paste0(response, " ~ ", vars) %>% as.formula()
    formula_2 <- paste0(var, " ~ ", vars) %>% as.formula()

    # fit lmod
    lmod_1 <- lm(formula_1, data)
    lmod_2 <- lm(formula_2, data)

    # get residuals
    response_resid <- lmod_1 %>%
        augment() %>%
        select(.resid) %>%
        flatten_dbl()

    var_resid <- lmod_2 %>%
        augment() %>%
        select(.resid) %>%
        flatten_dbl()

    resids <- tibble("var" = var_resid,
                     "response" = response_resid)
    resids %>%
        ggplot(aes(x = var, y = response)) +
        geom_point() +
        geom_smooth(method = "lm", se = FALSE,
                    lty = 2, color = "mediumpurple", alpha = 0.3) +
        ggtitle(var)
}

partial_regression_plot(savings, response = "sr", var = "pop15")

gen_all_partial <- function(data, response) {
    plots <- data %>%
        names() %>%
        .[!. %in% response] %>%
        map(., ~ partial_regression_plot(data, response = response, var = .x))

    plot_grid(plotlist = plots)
}

savings %>%
    gen_all_partial(response = "sr")
```

The partial regression plot provides some intuition about the meanings of regression coefficients. We are looking at the marginal relationship between the response and the predictor after the effect of the other predictors have been removed. Multiple regression is difficult because of visualization limitations, but partial regression plots allow us to view the relationship between one predictor and the response, much as in simple regression. 

**Partial Residual Plots** serve as an alternative to added variable (or partial regression) plots. We construct the response with the predicted effect of the other X removed:

$y - \sum_{j \dne i} x_j \hat{\beta_j} = \hat{y} + \hat{\epsilon} - \sum_{j \dne i} x_j \hat{\beta_j} = x_i \hat{\beta_i} + \hat{\epsilon}$

The partial residual plot is then $\hat{\epsilon} + \hat{\beta_i}x_i$ against $x_i$. Partial residual plots are believed to be better for nonlinearity detection while added variable plots are better for outlier / influential point detection. 

```{r}

partial_residual_plot <- function(model, predictor) {
    # get terms
    terms <- predict(model, type = "terms") %>%
        as_tibble() %>%
        select({{predictor}}) %>%
        flatten_dbl()

    model %>%
        augment() %>%
        mutate(partial_resid = terms + .resid) %>%
        ggplot(aes(x = {{predictor}}, y = partial_resid)) +
        geom_point() +
        geom_smooth(method = "lm", se = FALSE,
                    color = "mediumpurple", lty = 2) +
        ylab("Partial Residuals")
}

gen_all_partial_resids <- function(model) {
    model %>%
        tidy() %>%
        .[2:nrow(.), 1] %>%
        flatten_chr() %>%
        map(sym) %>%
        map(., ~ partial_residual_plot(model, !!.x)) -> pr_plots

    plot_grid(plotlist = pr_plots, ncol = 2)
}

lmod %>%
    gen_all_partial_resids()
```

For pop15, we can see two distinct groups in the plot. It suggests that there may be a different relationship in the two groups. 

```{r}
lm_g1 <- savings %>%
    filter(pop15 > 35) %>%
    lm(data = ., formula = sr ~ .)

lm_g2 <- savings %>%
    filter(pop15 <= 35) %>%
    lm(data = ., formula = sr ~ .)

lm_g1 %>%
    tidy() %>%
    mutate_if(is.double, round, 4)

compare_summaries <- function(m1, m2) {
    require(rlang)

    m1 %<>% tidy() %>%
        set_names(paste(colnames(.), "1", sep = "_")) %>%
        mutate_if(is.double, round, 4)

    m2 %<>% tidy() %>%
        set_names(paste(colnames(.), "2", sep = "_")) %>%
        mutate_if(is.double, round, 4)

    m_tbl <- bind_cols(m1, m2)

    create_eqn <- function(name) {
        evaluator <- function(exp) {
            return(eval_tidy(parse_expr(exp),
                             data = m_tbl))
        }

        name_1 <- paste0(name, "_1")
        name_2 <- paste0(name, "_2")

        return(paste0("|",
                      evaluator(name_1), " - ",
                      evaluator(name_2), "| = ",
                      abs(evaluator(name_1) - evaluator(name_2))))
    }

    out_tbl <- m_tbl %>%
        mutate(estimate = create_eqn("estimate"),
               std.error = create_eqn("std.error"),
               statistic = create_eqn("statistic"),
               p.value = create_eqn("p.value")) %>%
        select(term = term_1, estimate, std.error, statistic, p.value)

    # get sums of digits
    out_tbl %>%
        map(., ~ str_extract(.x, pattern = "[[:digit:]]+\\.[[:digit:]]+$") %>%
            as.double() %>% na.omit()) -> digitz

    sod <- digitz %>% map(., ~.x %>% sum())
    coef_sod <- digitz %>% map(., ~ .x %>% tail(-1) %>% sum())

    out_tbl %<>%
        add_row("term" = "Sum of Differences",
                "estimate" = sod$estimate,
                "std.error" = sod$std.error,
                "statistic" = sod$statistic,
                "p.value" = sod$p.value) %>%
        add_row("term" = "Coefficient Sum of Differences",
                "estimate" = coef_sod$estimate,
                "std.error" = coef_sod$std.error,
                "statistic" = coef_sod$statistic,
                "p.value" = coef_sod$p.value)

    return(out_tbl)
}

compare_summaries(lm_g1, lm_g2)
```

From our comparison, we see that separating the pop15 into two separate groups causes large deviations in estimate, std.error, statistic, and p value. We can also view a graphical interpretation of the difference between the groups

```{r}
lm_g1

savings %>%
    filter(pop15 <= 35) %>%
    GGally::ggpairs()

savings %>%
    filter(pop15 > 35) %>%
    GGally::ggpairs()
```

# Discussion 

Some assumptions are more important than others because some violations are more serious in that they can lead to very misleading conclusions. Ordered by importance: 

0. The data at hand should be relevant to the question of interest. 

1. Systematic form of the model. If this is seriously wrong, then predictions will be inaccurate and we have no way to explain relationships 

2. Dependence of Errors: The presence of strong dependence means that there is less information in the data than the sample size may suggest. 

3. Nonconstant variance: A failure to address this will lead to inaccurate inferences. In particular, prediction uncertainty may not be properly quantified. 

4. Normality: This is the least important simply because with large datasets, the inference will become more robust to the lack of normality as the central limit theorem will mean that the approximations will tend to be adequate. 

# Exercises

1. 

```{r}
# load data
data(sat, package = "faraway")
sat %<>% as_tibble()

# fit model
(lmod <- lm(total ~ expend + salary + ratio + takers, sat))
```

a. Check the constant variance assumption for the errors

```{r}
lm_aug <- lmod %>% augment()

lm_aug %>%
    diag_plot(x = .fitted, y = .resid) -> p1

lm_aug %>%
    diag_plot(x = .fitted, y = sqrt(abs(.resid))) -> p2

plot_grid(p1, p2, ncol = 2)

# perform numerical test to check for nonconstant variance
nc_test <- lm(sqrt(abs(.resid)) ~ .fitted, lm_aug)

nc_test %>% summary()
```

In the plots above, we see that there may be two different groups in the residuals, totals above and below 950. Let's perform an F test to compare the variances of the two samples

```{r}
var.test(lm_aug$.resid[lm_aug$total > 950], lm_aug$.resid[lm_aug$total <= 950])
```

In the test above, our null hypothesis is that the true ratio of the variances of the total group split at 950 are equal to 1, i.e. they have the same variance. Our alternative hypothesis is that the true ratio of variances is not equal to 1. Since our p-value is 0.4888, we fail to reject the null hypothesis and accept that they have the same variance.

In the numerical test above, we check for a distinctive slope on the linear model of sqrt(abs(.resid)) against our fitted values.

```{r}
p2 +
    geom_abline(intercept = nc_test$coefficients[[1]],
                slope = nc_test$coefficients[[2]],
                color = "mediumpurple", lty = 3)
```

We see that there is no linear trend in our values. 

b. Check the normality assumption

```{r}
lm_aug %>%
    ggplot(aes(sample = .resid)) +
    stat_qq() +
    stat_qq_line() -> p1

lm_aug %>%
    ggplot(aes(x = .resid)) +
    geom_histogram(bins = 10, fill = "skyblue", color = "black") -> p2

cowplot::plot_grid(p1, p2, ncol = 1)
```

From our plots above, we see the following:

- The residuals (mostly) follow the line, which charts a theoretical perfect normal distribution
- Our residual histogram is close to the ideal bell curve shape

We can also do a statistical test for normality:

```{r}
shapiro.test(residuals(lmod))
```

In this case, our null hypothesis is that our residuals are normally distributed, and we fail to reject the null hypothesis. 

c. Check for large leverage points

We can see if we need to look for large leverages closely by checking for leverages of more than 2p / n

```{r}
# get number of predictors in lmod
sum(lm_aug$.hat)

# calculate 2p / n
2 * 5 / nrow(lm_aug)

# filter on points with more leverage than 2p / n
lm_aug %>%
    filter(abs(.hat) >= 0.2)
```

We can plot the leverage points by using the .hat values from the augmented lm model and making a halfnorm plot.

```{r}
gghalfnorm::gghalfnorm(lm_aug$.hat)
```

We don't find any points which diverge significantly from the rest of the data. 

We can also check the standardized residual qqplot to look for outliers >= 2 sd 

```{r}
lm_aug %>%
    qq_plot(choice = .std.resid, title = "Standardized Residuals")
```

Since the residuals have been standardized, we expect the points to approximately follow the y = x line if normality holds. In this case, there is one out of place point to the far left.

```{r}
lm_aug %>%
    arrange(.std.resid) %>%
    slice(1)
```

This corresponds to the point above.

d. Check for outliers

```{r}
get_adj_outliers <- function(lmod,
                      base_p_value = 0.05,
                      correction_strength = NA,
                      top_vals = 3) {
    # get studentized resids
    st_res <- rstudent(lmod)

    # set correction strength
    if (is.na(correction_strength)) {
        correction_strength <- lmod %>%
            augment() %>%
            nrow()
    } else {
        correction_strength
    }

    # compute bonferroni
    dof <- lmod %>%
        glance() %>%
        pull(df.residual)

    base_p_value <- base_p_value

    # compute conferroni
    bf_val <- qt(p = base_p_value / correction_strength,
                 df = dof)

    # get top_n absolute value studentized residuals
    top_n_vals <- st_res %>%
        enframe() %>%
        top_n(abs(value), n = top_vals) %>%
        add_row(name = "Bonferroni Correction Threshold",
                value = bf_val) %>%
        rowwise() %>%
        mutate(over_thresh = ifelse(abs(value) > abs(bf_val),
                                    TRUE,
                                    FALSE))

    # get outlier values
    top_n_vals %>%
        pluck(1) %>%
        head(-1) %>%
        map_df(., ~ lmod %>%
            augment() %>%
            slice(as.integer(.x)) %>%
            mutate(name = .x)) -> outlier_values

    outlier_values %>%
        left_join(top_n_vals, by = "name") %>%
        select("Index" = name, over_thresh, value, everything())
}

lmod %>%
    get_adj_outliers()
```

From the above, with the Bonferroni correction we do not come across any outliers.

e. Check for influential points

```{r}
# generate halfnorm plot
(lm_aug %>%
    .$.cooksd %>%
    gghalfnorm::gghalfnorm() -> p_cooksd)
```

From the halfnormal plot above, we see that the 44th observation seems influential.

```{r}
# exclude the largest influence point and refit
(lmodi <- lm(total ~ expend + salary + ratio + takers, sat %>% slice(-44)))

# compared to the full data fit
lmodi %>% tidy()
lmod %>% tidy()

# compare summaries
compare_summaries(lmod, lmodi) %>%
    slice(2:6)
```

When removing the influential point, we not see that the ratio is no longer significant. 

f. Check the structure of the relationship between the predictors and the response

In the model we are looking at, our response variable is total

```{r}
# partial regression plots
sat %>%
    select(c("total", "expend", "salary", "ratio", "takers")) %>%
    gen_all_partial("total")
```

The plot above shows us the effect of regression y on all x except the variable. This lends interpretation to the coefficients of the variables.

For example, here are the coefficients for each variable:

```{r}
lmod %>%
    tidy() %>%
    select(1:2)
```

partial regression plots are considered to be better for influential point detection. In the plots above we don't see any largely influential points.

We can also look at the partial residual plots, which show the difference in residuals when each predictor is held out 

```{r}
lmod %>%
    gen_all_partial_resids()
```

partial residual plots are considered better for nonlinearity detection. In the plot above we see a weird effect with takers. 

```{r}
sat %>%
    select(total, ratio, salary, takers) %>%
    GGally::ggpairs()
```

What we are seeing is the trend that as the number of takers goes up, the average score goes down. 

Lets look into the change when we split takers into two groups, +- 40

```{r}
lm_1 <- sat %>%
    select(total, ratio, salary, takers, expend) %>%
    filter(takers > 40) %>%
    lm(total ~ ., .)

lm_2 <- sat %>%
    select(total, ratio, salary, takers, expend) %>%
    filter(takers <= 40) %>%
    lm(total ~ ., .)

compare_summaries(lm_1, lm_2)
```

Overall we see a fairly large change in the coefficient sum of differences. 

2. 

```{r}
data(teengamb, package = "faraway")

teengamb %<>% as_tibble()

lmod <- lm(gamble ~ ., teengamb)
lmod %>% augment() -> lm_aug

lmod %>%
    summary() %>%
    tidy()

# a) check constant variance assumption for errors
lmod %>%
    diag_plot(x = .fitted, y = .resid)

lmod %>%
    diag_plot(x = .fitted, y = sqrt(abs(.resid)))

## these are inconclusive - use numerical test
lm(.resid ~ .fitted, lm_aug) %>% summary()

## the numerical test seems fine. Generate partial residual plots to go over relationships of each predictor on residuals
lmod %>%
    gen_all_partial_resids()

## in the plots above we see mild nonconstant variance for the income and verbal predictors

# b) check the normality assumption
lm_aug %>%
    qq_plot(choice = .resid, title = "Residuals for LM")

## looks good. Let's back it up with a Shapiro-Wilk's test
shapiro.test(x = lm_aug$.resid)

## this is saying it is very non normal. Let's try again without the outlier (likely mistaken) value of 156.
lm_aug %>%
    slice(-24) %>%
    qq_plot(.resid, title = "Residuals without 156 Gamble outlier")

shapiro.test(lm_aug %>%
             slice(-24) %>%
             .$.resid)

## we get a p-value of 0.51, indicating that we fail to reject the null hypothesis that the residuals are normally distributed

# c) check for large leverage points

## Check for hat values greater than 2p / n
teengamb %>%
    dim() -> tg_dim

val <- 2 * tg_dim[[2]] / tg_dim[[1]]

lm_aug %>%
    mutate(index = row_number()) %>%
    filter(.hat >= val) %>%
    select(index, everything())

## so we have 4 possible points. Lets look at our halfnormal and QQ plots
lm_aug %>%
    .$.hat %>%
    gghalfnorm::gghalfnorm() +
    ggtitle("Half Normal Values for Hat Distances") -> p1

lm_aug %>%
    qq_plot(.std.resid, "Standardized Residuals") -> p2

cowplot::plot_grid(p1, p2, ncol = 1)

## as we see above, there is a lot of leverage present. Let's look again without the points with greater than 2p/n leverage
teengamb %>%
    slice(-c(31, 33, 35, 42)) %>%
    lm(data = ., gamble ~ .) %>%
    augment() -> lm_aug_nolev

lm_aug_nolev %>%
    .$.hat %>%
    gghalfnorm::gghalfnorm() +
    ggtitle("Half Normal Values without High Leverage") -> p1

lm_aug_nolev %>%
    qq_plot(.std.resid, "Standardized Residuals") -> p2

cowplot::plot_grid(p1, p2, ncol = 1)

## out plots still look weird, but not nearly as bad

# d) Check for Outliers
lmod %>%
    get_adj_outliers()

## so we have 1 outlier. Let's see the effects of fit both with and without this point
teengamb %>%
    slice(24)

## This is likely an error. We should discard it

teengamb %>%
    slice(-24) %>%
    lm(gamble ~ ., data = .) -> lmod_2

compare_summaries(m1 = lmod, m2 = lmod_2) %>%
    select(1:2)

## there is a big difference in the fit with and without this point
gen_all_partial(data = teengamb, response = "gamble")
gen_all_partial(data = teengamb %>% slice(-24), response = "gamble")

## discard it
teengamb %<>% slice(-24)

# e) Check for Influntial Points
lm_aug %>%
    .$.cooksd %>%
    gghalfnorm::gghalfnorm()

## our most influential point is observation 30
lmod_2 <- lm(gamble ~ ., teengamb %>% slice(-30))

compare_summaries(lmod, lmod_2) %>%
    select(1, 2)

## the significance of none of the variables changed and the coefficients only changed slightly

lmod %>%
    loo_params()

# f) Check the structure of the relationship between the predictors and the response
gen_all_partial(teengamb, response = "gamble")
gen_all_partial_resids(lmod)

## No major outliers of nonlinearities seem to jump out
```

3. 

```{r}
data(prostate, package = "faraway")
prostate %<>% as_tibble()
lmod <- lm(lpsa ~ ., prostate)
lm_aug <- lmod %>% augment()

# a) Check the constant variance assumption for errors
lm_aug %>%
    diag_plot(x = .fitted, y = .resid)

lm_aug %>%
    diag_plot(x = .fitted, y = sqrt(abs(.resid)))

## Both look great

# b) Check the normality assumption
lm_aug %>%
    qq_plot(choice = .resid, title = "Normality for LM")

shapiro.test(lm_aug$.resid)

## Looks great

# c) Check for large leverage points
large_lev_cutoff <- function(model) {
    # get number of predictors
    model %>% tidy() %>% nrow() - 1 -> pred_count

    # get amount of data
    model %>% augment() %>% nrow() -> data_count

    # print message
    cat(paste0("Returning Leverage Points Over ",
               round(2 * pred_count / data_count, 2), "\n"))

    # get metric
    return(2 * pred_count / data_count)
}

get_large_lev_points <- function(model) {
    # get cutoff
    model %>% large_lev_cutoff() -> cutoff_point

    # return hat values
    model %>%
        augment() %>%
        mutate(index = row_number()) %>% 
        filter(.hat >= cutoff_point) %>%
        select(index, .hat, everything()) %>%
        arrange(desc(.hat))
}

gen_leverage_plots <- function(model) {
    model %>%
        augment() -> lm_aug

    lm_aug$.hat %>%
        gghalfnorm::gghalfnorm() -> p1

    lm_aug %>%
        qq_plot(.std.resid, title = "Standardized Residuals") -> p2

    cowplot::plot_grid(p1, p2, ncol = 1)
}

lmod %>%
    get_large_lev_points()

lmod %>%
    gen_leverage_plots()

## a few points were identified as high leverage, but everything seems fine

# d) check for outliers
lmod %>%
    get_adj_outliers()

## everything seems fine

# e) Check for Influential Points
lm_aug$.cooksd %>%
    gghalfnorm::gghalfnorm()

## our most influential point is index 32
lmod_2 <- lm(lpsa ~ ., prostate %>% slice(-32))

compare_summaries(lmod, lmod_2) %>%
    select(1, 5)

## very little change

lmod %>%
    loo_params()

# f) check the structure of the model
prostate %>% gen_all_partial(response = "lpsa") # looks good
lmod %>% gen_all_partial_resids()

## something weird seems to be going on with lbph, lcp, pgg45. They all have a bottom number which clumps.
prostate %>%
    select(lbph) %>%
    ggplot(aes(y = seq_len(nrow(prostate)),
               x = lbph)) +
    geom_point() +
    geom_jitter() +
    geom_rect(aes(xmin = -1.6, xmax = -1.2, ymin = -1, ymax = 100),
              fill = "blue", alpha = 0.002)

prostate %>%
    filter(lbph < -1.2) %>%
    select(lbph) %>%
    distinct()

## we have 43 rows in which everyone has an lbph of -1.39. Let's see if there is a different relationship between the two groups

# drop the lbph of -1.39
lmod_2 <- lm(lpsa ~ ., prostate %>% filter(lbph >= -1.2))

# compare to the model with all the data points
compare_summaries(lmod, lmod_2) %>%
    select(1, 5)

## removing those points made lcavol and svi non significant, and lweight and age less significant.
```

4. 

```{r}
swiss <- datasets::swiss %>% as_tibble()
lmod <- lm(Fertility ~ ., swiss)
lm_aug <- lmod %>% augment()

# a) check for constant variance assumption
lm_aug %>% diag_plot(y = .resid, x = .fitted)
lm_aug %>% diag_plot(y = sqrt(abs(.resid)), x = .fitted)

# b) Check the normality assumption
lm_aug %>%
    qq_plot(choice = .resid,
            title = "Residuals")

shapiro.test(lm_aug$.resid)

# c) Check for large leverage points
lmod %>%
    get_large_lev_points()

lmod %>%
    gen_leverage_plots()

## 45 stands out

lmod_2 <- lm(Fertility ~ ., data = swiss %>% slice(-45))

compare_summaries(lmod, lmod_2)

## There is very very little change

# d) Check for outliers
lmod %>% get_adj_outliers()

# e) Check for influential points
lm_aug$.cooksd %>%
    gghalfnorm::gghalfnorm()

## our most influential point is index 6
lmod_2 <- lm(Fertility ~ ., swiss %>% slice(-6))

compare_summaries(lmod, lmod_2) %>%
    select(1, 5)

## they all remained just as significant

# f) check the structure of the relationship between predictors and response
swiss %>% gen_all_partial(response = "Fertility")

## education seems to have 3 influential points in the lower right corner
swiss %>%
    group_by(Education) %>%
    tally(sort = TRUE)

## these are likely the 53, 32 and 28 points.
swiss %>%
    filter(Education < 25) %>%
    gen_all_partial(response = "Fertility")

## this looks much more reasonable

## look at partial residuals
lmod %>%
    gen_all_partial_resids()

## the model seems to be underpredicting lower number catholic values and overpredicting higher number catholic values
```

5.

```{r}
data(cheddar, package = "faraway")
cheddar %<>% as_tibble()
lmod <- lm(taste ~ ., cheddar)
lm_aug <- lmod %>% augment()

# a) check the constant variance assumption
lm_aug %>%
    diag_plot(x = .fitted, y = .resid)

# b) check the normality assumption
lm_aug %>%
    qq_plot(choice = .resid, title = "Residuals")

shapiro.test(lm_aug$.resid)

# c) check for large leverage points
lm_aug %>%
    .$.hat %>% 
    gghalfnorm::gghalfnorm()

lmod %>% get_large_lev_points()

## there are some points which are a bit larger than our cutoff point.

lmod %>% gen_leverage_plots()

## lets see the difference removing point 26 makes
lmod_2 <- lm(taste ~ ., cheddar %>% slice(-26))
lmod_2 %>% gen_leverage_plots()

# d) check for outliers
lmod %>% get_adj_outliers()

# e) check for influential points
lm_aug$.cooksd %>%
    gghalfnorm::gghalfnorm()

## our most influential point is number 15
lmod_2 <- lm(taste ~ ., cheddar %>% slice(-15))

compare_summaries(lmod, lmod_2) %>%
    select(1, 5)

# f) check for structure of the relationship between the predictors and response
cheddar %>% gen_all_partial("taste")
lmod %>% gen_all_partial_resids()
```

6. 

```{r}
data(happy, package = "faraway")
happy %<>% as_tibble()
lmod <- lm(happy ~ ., happy)
lm_aug <- lmod %>% augment()

# a) check the constant variance assumption
lm_aug %>%
    diag_plot(.fitted, .resid)

lm_aug %>%
    diag_plot(.fitted, sqrt(abs(.resid)))

lm(sqrt(abs(.resid)) ~ .fitted, lm_aug) %>% summary()

# b) check the normality assumption
lm_aug %>%
    qq_plot(choice = .resid, title = "Residuals")
shapiro.test(lm_aug$.resid)

# c) check for large leverage points
lmod %>% get_large_lev_points()
lmod %>% gen_leverage_plots()

lmod_2 <- lm(happy ~ ., happy %>% slice(-6, -7))
lmod_2 %>% gen_leverage_plots()

# d) check for outliers
lmod %>% get_adj_outliers()

happy %>%
    slice(36)

lmod_2 <- lm(happy ~ ., happy %>% slice(-36))

compare_summaries(lmod, lmod_2) %>%
    select(1, 5)

## without that particular point both money and work lose significance

# e) check for influential points
lm_aug %>%
    .$.cooksd %>%
    gghalfnorm::gghalfnorm()

lmod %>%
    loo_params()

## 36 is influential in sex, money and work, but not love

# f) Check the structure of the relationship between parameters and response
gen_all_partial(happy, "happy")

## everything looks linear

gen_all_partial_resids(lmod)

## none of the residuals look influential
```

7. 

```{r}
data(tvdoctor, package = "faraway")

tvdoctor %<>%
    as_tibble()

lmod <- lm(life ~ ., tvdoctor)
lm_aug <- lmod %>% augment()

# standardize it all
tvd <- tvdoctor %>%
    scale() %>%
    as_tibble()

lmod <- lm(life ~ ., tvdoctor)
lm_aug <- lmod %>% augment()

# Check for nonconstant variance
lm_aug %>%
    diag_plot(.fitted, .resid)

lm_aug %>%
    filter(between(.fitted, 68, 70.3)) %>%
    diag_plot(.fitted, sqrt(abs(.resid)))

# the residuals look nonconstant because they are clustered about 70. This is due to the intercept coefficient being 70, and the beta coefficients for our predictors being small and negative.

# check the normality assumption
lm_aug %>%
    qq_plot(.resid, "Residuals")

shapiro.test(lm_aug$.resid)

# check for large leverage points
lmod %>% get_large_lev_points()
lmod %>% gen_leverage_plots()

# index 1 is an enormous leverage point
lmod_2 <- lm(life ~ ., tvdoctor %>% slice(-8))
compare_summaries(lmod, lmod_2)
lmod_2 %>% gen_leverage_plots()

# the majority of the change from eliminating the point is in the statistic, with a change on the intercept term of about 3.7. Given the magnitude of the intercept compared to the other coefficients, this could be a reasonably large change

# check for outliers
lmod %>% get_adj_outliers()

## our only outlier with the bonferroni correction is the one at index 8
tvdoctor %>% slice(8)
tvdoctor %>% summary()
## this is an insane amount of tv and doctor

# check for influential points
lm_aug %>%
    .$.cooksd %>%
    gghalfnorm::gghalfnorm()

lmod %>%
    loo_params()

# check the relationship between the structure of the model and the data
gen_all_partial(tvdoctor, "life")
gen_all_partial(tvdoctor %>% filter(tv < 20), "life")

# the linearity in tv only makes sense when tv < 20
lmod %>% gen_all_partial_resids()

tvdoc_2 <- tvdoctor %>%
    filter(tv < 25 &
           doctor < 2900)

tvdoc_2 %>% summary()

lmod_2 <- lm(life ~ tv + doctor, data = tvdoc_2)

lmod_2 %>% gen_all_partial_resids()
```

8. 

```{r}
data(divusa, package = "faraway")
divusa %<>% as_tibble()
lmod <- lm(divorce ~ unemployed + femlab + marriage + birth + military, divusa)
lm_aug <- lmod %>% augment()
```
