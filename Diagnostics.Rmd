```{r}
library(tidyverse)
library(faraway)
library(magrittr)
library(broom)
library(gganimate)
library(furrr)
plan(multiprocess)
```

# Diagnostics 

The estimation of and inference from the regression model depend on several assumptions. These assumptions should be checked using regression diagnostics before using the model in earnest. 

We can divide the problems into three categories: 

**Error** : we have assumed that our errors are independent, have equal variance, and are normally distributed. 

**Model** : we have assumed that the structural part of the model, $E[y] = X\beta$, is correct. 

**Unusual Observations** : Sometimes a few observations do not fit the model and they might change the choice and fit of the model. 

# Checking Error Assumptions

We wish to check for independence, constant variance, and normality of the errors. We can examine the residuals, $\hat{\epsilon}$.

Recall that $\hat{y} = X(X^TX)^{-1}XTy = Hy$ where $H$ is the hat matrix, so that 

<center>
$\hat{\epsilon} = y - \hat{y} = (I - H)y = (I - H)X\beta + (I - H)\epsilon = (I - H)\epsilon$
</center>

Therefore, var $\hat{\epsilon}$ = var $(I - H)\epsilon$ = $(I - H)\sigma^2$ assuming var ${\epsilon} = \sigma^2 I$. 

We see that while the errors may have equal variance and be uncorrelated, the residuals do not. Fortuneately, correlation of errors usually has little imact and can be applied to residuals in order to check the assumptions on the error. 

## Constant Variance 

We can not check the assumption of constant variance just by examining the residuals alone. We need to check whether the variance in the residuals is related to some other quantity. 

The most useful diagnostic is a plot of $\hat{\epsilon}$ against $\hat{y}$, or residuals against fitted. If all is well, we should see constant symmetrical variation (known as homoscedasticity) in the vertical $\hat{\epsilon}$ direction. **Nonconstant** variance is called heterscedasticity. Nonlinearity of the structural part of the model can also be detected in this plot. 

It is also worthwhile to plot $\hat{\epsilon}$ against $x_i$ for potential predictors that are in the current model as well as those that are not used. For plots of residuals against predictors that are not in the model, any observed structure may indicate that this predictor should be included in the model. 

```{r}
data(savings, package = "faraway")
savings %<>% as_tibble()

(lmod <- lm(sr ~ ., savings))

lm_aug <- lmod %>% augment()

diag_plot <- function(data, x, y) {
    data %>%
        ggplot(aes(x = {{x}}, y = {{y}})) +
        geom_point() +
        geom_hline(yintercept = 0, lty = 2, alpha = 0.5)
}

lm_aug %>%
    diag_plot(x = .fitted, y = .resid)
```

The plot above shows that there likely no heteroscedasticity, or nonconstant variance in the error. We can examine the constant variance more closely by plotting $\sqrt{|\hat{\epsilon}|}$ against $\hat{y}$. Considering the absolute value of the residuals roughly doubles the resolution. For truly normal errors, $|\hat{\epsilon}|$ would follow what is known as a half normal distribution (since it has a density which is simply the upper half of a normal density). Such a distribution is quite skewed, and the skewness can be reduced by the square root transformation.

```{r}
lm_aug %>%
    diag_plot(x = .fitted, y = sqrt(abs(.resid)))
```

We see approximately constant variation in the plot above. 

We can also perform a numerical test to check nonconstant variance

```{r}
lm(sqrt(abs(.resid)) ~ .fitted, lm_aug) %>% summary()
```

This test checks for a linear trend in the variation. As a result, it might be good at detecting a particular kind of nonconstant variance, but have no power to detect another (think Simpson's paradox). Residual plots are generally more versatile here because unanticipated problems may be spotted.

It is often hard to judge residual plots. Here are four plots which show:

1. Constant Variance 
2. Strong Nonconstant Variance 
3. Mild Nonconstant Variance 
4. Nonlinearity 

```{r}
map(seq_len(10), ~ tibble("x" = runif(100),
                          "a" = rnorm(100),
                          "b" = x * rnorm(100),
                          "c" = sqrt(x) * rnorm(100),
                          "d" = cos(x * pi / 25) +
                              rcauchy(n = 100, location = 5, scale = 5),
                          "gen_group" = .x)) %>%
    bind_rows() -> resid_gen

animate_resids <- function(data, x, y, title) {
    data %>%
        diag_plot(x = {{x}}, y = {{y}}) +
        ggtitle(as.character(title)) +
        transition_states(
            gen_group,
            transition_length = 10,
            state_length = 1) +
        enter_fade() +
        exit_shrink()
}

resid_gen %>%
    animate_resids(x = x, y = a, title = "Constant Variance") -> p1g

resid_gen %>%
    diag_plot(x, a) +
    ggtitle("Constant Variance") -> p1

resid_gen %>%
    animate_resids(x = x, y = b, title = "Strong Nonconstant Variance") -> p2g

resid_gen %>%
    diag_plot(x, b) +
    ggtitle("Strong Nonconstant Variance") -> p2

resid_gen %>%
    animate_resids(x = x, y = c, title = "Mild Nonconstant Variance") -> p3g

resid_gen %>%
    diag_plot(x, c) +
    ggtitle("Mild Nonconstant Variance") -> p3

resid_gen %>%
    animate_resids(x = x, y = d, title = "Nonlinearity") -> p4g

resid_gen %>%
    diag_plot(x, d) +
    ggtitle("Nonlinearity") -> p4

cowplot::plot_grid(p1, p2, p3, p4, ncol = 2)

list("constant_var" = p1g, "strong_nonconstant_var" = p2g,
     "mild_nonconstant_var" = p3g, "nonlinearity" = p4g) %>%
    imap(., ~ anim_save(path = getwd(),
                       filename = paste0(.y, ".gif"),
                       animation = .x))
```

Now we can look at some residuals against predictor plots for the savings data 

```{r}
resid_plots <- with(lm_aug, list("pop15" = pop15, "pop75" = pop75,
                  "dpi" = dpi, "ddpi" = ddpi)) %>%
    imap(., ~ lm_aug %>% diag_plot(x = .x, y = .resid) +
        xlab(.y) + ylab("residuals"))

cowplot::plot_grid(plotlist = resid_plots, ncol = 2)
```

In the first plot, for population under 15, we can see two groups. We can compare and test the variances in these groups. 

Given two independent samples from normal distributions, we can test for equal variance using the test statistic of the ratio of the two variances. The null distribution is an F with degrees of freedom given by the two samples

```{r}
var.test(lm_aug$.resid[lm_aug$pop15 > 35], lm_aug$.resid[lm_aug$pop15 <= 35])
```

We see that there is a significant difference with our p-value of 0.01358 and we fail to reject our alternative hypothesis that the true ratio of variances is not equal to 1. 

When problems are seen in the diagnostic plots, some modification of the model is suggested. 

- If some nonlinearity is observed, perhaps in conjunction with nonconstant variance, a transformation of the variables should be considered. 

- If the problem is solely one of nonconstant variance with no suggestion of nonlinearity, then the use of weighted least squares may be appropriate.

Alternatively, when non constant variance is seen in the plot of the residuals against fitted values, a transformation of the response y to h(y) where h() can be chosen so that var h(y) is constant should be considered. 

To see how to choose h: 

$h(y) = h(Ey) + (y - Ey)h'(Ey) + ...$
$var h(y) = 0 + h'(Ey)^2 var y + ...$
$h'(Ey) \propto (var y)^{-1/2}$
$h(y) = \int \frac{dy}{\sqrt{var y}} = \int \frac{dy}{SD(y)}$

For example, if $var y = var \epsilon \propto (Ey)^2$, then $h(y) = \log y$ is suggested, while if $var \epsilon \propto (Ey)$, then $h(y) = \sqrt{y}$

Sometimes it can be difficult to find a good transformation. For example when $y_i \leq 0$ for some $i$, square root or log transformations will fail. 

Consider the residuals vs fitted plot for the Galapagos data:

```{r}
data(gala, package = "faraway")
lmod <- lm(Species ~ Area + Elevation + Scruz + Nearest + Adjacent, gala)

(lmod %>% augment() %>%
    diag_plot(x = .fitted, y = .resid) -> p1)
```

The first plot has non constant variance (and evidence of nonlinearity). The square root transformation is often appropriate for count response data. The Poisson distribution is a good model for counts and that distribution has the property that the mean is equal to the variance, thus suggesting the square root transformation. 

```{r}
lmod <- lm(sqrt(Species) ~ Area + Elevation + Scruz + Nearest + Adjacent, gala)

(lmod %>% augment %>%
    diag_plot(x = .fitted, y = .resid) -> p2)
```

We see in the second plot that the variance is now constant and the signs of nonlinearity have gone. Our guess at a variance stabilizing transformation worked here, but we could have always tried something else 

```{r}
cowplot::plot_grid(p1, p2, ncol = 2)
```

# Normality 

The tests and confidence intervals we use are based on the assumption of normally distributed errors. The residuals can be assessed for normality using a QQ plot. This compares the residuals to ideal normal observations. 

```{r}
lmod <- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings)

lmod %>%
    augment() %>%
    ggplot(aes(sample = .resid)) +
    stat_qq() +
    stat_qq_line() -> p1

lmod %>%
    augment() %>%
    ggplot(aes(x = .resid)) +
    geom_histogram(bins = 30, fill = "skyblue", color = "black") -> p2

cowplot::plot_grid(p1, p2, ncol = 1)
```

Normal residuals should follow the line approximately. The histogram should be the familiar bell shape. 

We can get an idea of the variation to be expected in QQ plots in the following simulation. We generate data from different distributions:

1. Normal 
2. Log-Normal (an example of a skewed distribution)
3. Cauchy (an example of a long tailed (leptokurtic) distribution)
4. Uniform (an example of a short tailed (platykurtic) distribution)

```{r}
map(seq_len(10), ~ tibble("x" = rnorm(100),
                          "a" = rnorm(100),
                          "b" = exp(rnorm(100)),
                          "c" = rcauchy(100),
                          "d" = runif(100),
                          "gen_group" = .x)) %>%
    bind_rows() -> resid_gen

qq_plot <- function(data, choice, title) {
    data %>%
        ggplot(aes(sample = {{choice}})) +
        stat_qq() +
        stat_qq_line() +
        ggtitle(title)
}

qq_plots <- with(resid_gen, list("Normal" = a,
                                 "Log-Normal | Skewed" = b,
                                 "Cauchy | Long-Tail" = c,
                                 "Uniform | Short-Tail" = d)) %>%
    imap(., ~ resid_gen %>%
         qq_plot(choice = .x, title = .y))

cowplot::plot_grid(plotlist = qq_plots, ncol = 2)
```


Sometimes extreme cases may be the sign of a long tailed error like the Cauchy distribution, or they can just be outliers. If removing outliers results in other points becoming more prominent in the plot, the problem is likely due to a long tail error.

When the errors are not normal, least squares estimates may not be optimal. They will still be the best linear unbiased estimates, but other robust estimators may be more effective. Tests and confidence intervals will not be exact, but we can still appeal to the central limit theorem to assert that intervals constructed will be increasingly accurate approximations for larger sample sizes. 

When non-normality is found, the resolution depends on the type of problem found:

- Short tailed distributions are not serious usually and can generally be ignored 
- Skewed errors may be ameliorated by a transformation 
- Long tailed errors may require us accepting non-normality and basing the inference on the assumption of another distribution or use resampling methods like the bootstrap or permutation tests. Alternatively, we can use robust methods which lend less weight to outliers but may again require resampling for the inference. 

A formal test for normality is the Shapiro-Wilk test:

```{r}
shapiro.test(residuals(lmod))
```

In this test, the null hypothesis is that the residuals are normal. In our case, we have a p-value of 0.8524 and thus we do not reject the null hypothesis. The p-value is not helpful as an indicator of what action to take, so it is reccommended to use this in conjunction with a QQ plot. 

# Correlated Errors

It is difficult to check for correlated errors in general because there are too many possible patterns of correlation that may occur. Some types of data have a structure which suggests where to look for problems. 

- Data collected over time may have some correlation in successive errors 
- Spatial data may have correlation in the errors of nearby measurements 
- Data collected in blocks may show correlated errors within those blocks

We can illustrate the methods with a time series analysis, but we will use methods more specific to the regression diagnostics problem. 

```{r}
data(globwarm, package = "faraway")
globwarm %<>% as_tibble()

# fit model
(lmod <- lm(nhtemp ~ ., globwarm))

# grab augment df
lm_aug <- lmod %>% augment()
```

For temporal data like these, it is sensible to plot the residuals against the time index. 

```{r}
make_convex_hull <- function(data, x, y,
                      plot = TRUE,
                      alpha = 0.3,
                      fill = "cadetblue1",
                      color = "black") {
    hull <- data %>%
        slice(chull({{x}}, {{y}}))

    if (plot) {
        geom_polygon(data = hull,
                     alpha = alpha,
                     fill = fill,
                     color = color)
    } else {
        hull
    }
}

(lm_aug %>%
    na.omit() %>%
    ggplot(aes(x = year, y = .resid)) +
    geom_point() +
    geom_hline(yintercept = 0, lty = 2) +
    make_convex_hull(data = lm_aug,
                     x = year, y = .resid) +
    ggalt::geom_xspline(alpha = 0.3) -> p1)
```

If the errors were uncorrelated, we would expect a random scatter of points above and below the epsilon = 0 line. This looks like a time series pattern, so we have an indicator of positive serial correlation.

Another way to approach checking for serial correlation is to plot successive pairs of residuals

```{r}
n <- length(residuals(lmod))
plot(tail(residuals(lmod), n - 1) ~ head(residuals(lmod), n - 1),
     xlab = expression(hat(epsilon)[i]),
     ylab = expression(hat(epsilon)[i + 1]))
abline(h = 0, v = 0, col = grey(0.75))

lm_aug %>% nrow()

make_successive_res_plot <- function(data) {
    # get data length
    n <- data %>% nrow()

    # add lead lag
    data %<>%
    mutate(lead = .resid[2:nrow(.)] %>% append(0),
           lag = .resid[1:(nrow(.) - 1)] %>% append(0))

    # fit slope
    lm_in <- lm(lead ~ lag, data)

    data %>%
        ggplot(aes(x = lead, y = lag)) +
        geom_point() +
        geom_hline(yintercept = 0, lty = 2, alpha = 0.3) +
        geom_vline(xintercept = 0, lty = 2, alpha = 0.3) +
        geom_abline(slope = lm_in$coefficients[[2]],
                    intercept = lm_in$coefficients[[1]],
                    color = "blue",
                    alpha = 0.3)
}

(lm_aug %>%
    make_successive_res_plot() -> p2)

cowplot::plot_grid(p1, p2, ncol = 2)
```
We can also model the significance of the correlation directly

```{r}
lm_aug %<>%
    mutate(lead = .resid[2:nrow(.)] %>% append(0),
           lag = .resid[1:(nrow(.) - 1)] %>% append(0))

lm(lead ~ lag -1, lm_aug) %>% summary()
```

We omitted the intercept because the residuals have mean zero. The serial correlation is confirmed, as the estimate is nonzero and the p-value is very small. 

We can also plot more than just successive pairs if we suspect a more complex dependence. 

The Durbin-Watson test uses the statistic 

$DW = \frac{\sum_{i=2}^n (\hat{\epsilon_i} - \hat{\epsilon_{j-1}})^2}{\sum_{i = 1}^n \hat{\epsilon_i}^2}$

It essentially looks at the successive residuals, squares them, and normalizes their sum by the sum of squared residuals.

The null hypothesis is that the errors are uncorrelated. The null distribution based on the assumption of uncorrelated errors follows a linear combination of Chi Squared distributions. 

```{r}
# run Durbin-Watson statistic
lmtest::dwtest(nhtemp ~ wusa + jasper + westgreen +
                   chesapeake + tornetrask + urals +
                   mongolia + tasman, data = globwarm)
```

In this case the null hypothesis is that the true autocorrelation is zero, and the alternative hypothesis is that the true autocorrelation is greater than zero. Our p-value is really small (1.402 * 10^-15), so we can reject the null hypothesis and assert that we have autocorrelation. 

Sometimes serial correlations can be caused by a missing covariate. Although it is possible that difficulties with correlated errors can be removed by changing the structural part of the model, sometimes we must build the correlation directly into the model. This can be achieved with the method of generalized least squares.

# Finding Unusual Observations

Outliers do not fit the model well. Influential observations can change the fit of a model in a substantive manner. A leverage point is extreme in the predictor space. It is important to identify such points.

# Leverage 

$h_i = H_{ii}$ are called leverages and are useful diagnostics. Since var $\hat{\epsilon_i} = \sigma^2(1 - h_i)$, a large leverage $h_i\$ will make the var $\hat{\epsilon_i}$ small. The fit will be attracted toward $y_i\$. Large values of $h_i$ are due to extreme values in the X space. $h_i\$ corresponds to a squared Mahalonobis distance defined by X which is $(x - \bar{x})^T \hat{\Sigma}^{-1}(x - \bar{x})$ where $\hat{\Sigma}\$ is the estimated covariance of X. The value of $h_i$ depends only on X and not y, so leverages contain only partial information about a case. 

Since $\sum_i h_i = p$, an average value for $h_i$ is $p/n$. A rough rule is that leverages of more than $2p / n$ should be looked at more closely. 

```{r}
lmod <- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings)

lm_aug <- lmod %>% augment()
lm_aug %>% select(1:5, .hat)

lm_aug %>% select(.hat) %>% summarize("sum of leverages" = sum(.))
```

We verify that the sum of the leverages is 5, the number of parameters in the model. Without making assumptions about the distributions of the predictors that would often be unreasonable, we cannot say how the leverages would be distributed. 

In order to identify unusually large values of the leverage, we can use the half-normal plot. These are designed for the assessment of positive data. 

The idea is to plot the data against the positive normal quantiles.

1. Sort the data $x_{[1]} \leq ... \leq x_{[n]}$ 
2. Compute $u_i = /Phi^{-1}(\frac{n + i}{2n + 1})$
3. Plot $x_{[i]} ahainst u_i$

We do not expect a straight line relationship since we do not necessarily expect a positive normal distribution for quantities like leverages. We are mainly looking for outliers that stand out. 

```{r}
countries <- row.names(savings)
gghalfnorm::gghalfnorm(lm_aug$.hat, labs = countries, ylab = "leverages")
```

Leverages can also be used in scaling residuals. We have var $\hat{\epsilon_i} = \sigma^2(1 - h_i)$ which suggests the use of $r_i = \frac{\hat{\epsilon_i}}{\hat{\sigma} \sqrt{1 - h_i}}$.

These are called standardized residuals. If the model assumptions are correct, var $r_i$ = 1 and corr($r_i, r_j$) tends to be small. Standardized residuals have equal variance. 

Standardization can only correct for the natural non constant variance in residuals when the errors have constant variance. If there is some underlying heteroscedasticity in the errors, standardization cannot correct for it. 

```{r}
lm_aug %>%
    qq_plot(choice = .std.resid, title = "Standardized Residuals")
```

Since the residuals have been standardized, we expect the points to approximately follow the y = x line if normality holds. An advantage of the standardized form is that we can judge the size easily. An absolute value of 2 would be large, but not exceptional for a standardized residual, whereas an absolute value of 4 would be very unusual under the standard normal. 

# Outliers 

An outlier test is useful because it enables us to distinguish between truly unusual observations and residuals that are large, but not exceptional. 

```{r}
set.seed(8888)

testdata <- tibble(x = 1:15 + rnorm(15),
                   y = 1:15 + rnorm(15)) %>%
    add_row(x = 1:15 + rnorm(15),
                   y = 1:15 + rnorm(15))

leverage_plot <- function(data, x, y, levg_x, levg_y,
                   colors = c("mediumpurple", "firebrick1",
                              "skyblue", "orange")) {
    set.seed(8888)
    pred_tib <- tibble(x = levg_x, y = NA)

    data %<>%
        add_row(x = levg_x, y = levg_y)

    (lm_1 <- lm(y ~ x, data[1:(nrow(data) - 1), ]))
    (lm_2 <- lm(y ~ x, data))

    data %>%
        ggplot(aes(x = x, y = y)) +
        geom_point() +
        geom_point(data = tibble(x = c(levg_x, levg_x),
                                 y = c(levg_y, predict(lm_2,
                                                       pred_tib))),
                   aes(x = x, y = y),
                   color = colors[[1]],
                   shape = 13,
                   size = 5) +
        geom_segment(aes(x = levg_x,
                         y = levg_y,
                         xend = levg_x,
                         yend = predict(lm_2, pred_tib)),
                     lty = 2, alpha = 0.5, color = colors[[2]]) +
        geom_segment(aes(x = levg_x,
                         y = predict(lm_1, pred_tib),
                         xend = levg_x,
                         yend = predict(lm_2, pred_tib)),
                     lty = 2, alpha = 0.5, color = colors[[3]]) +
        geom_segment(aes(x = levg_x,
                         y = predict(lm_2, pred_tib),
                         xend = (predict(lm_2, pred_tib) -
                             lm_1$coefficients[[1]]) / lm_1$coefficients[[2]],
                         yend = predict(lm_2, pred_tib)),
                     lty = 2, alpha = 0.5, color = colors[[4]]) +
        geom_abline(intercept = lm_1$coefficients[[1]],
                    slope = lm_1$coefficients[[2]]) +
        geom_abline(intercept = lm_2$coefficients[[1]],
                    slope = lm_2$coefficients[[2]],
                    lty = 2) +
        xlim(c(0, 15)) + ylim(c(0, 15)) + 
        ggtitle("Leverage Difference in Linear Model")
}

leverage_plot(data = testdata, x = x, y = y, levg_x = 5.5, levg_y = 12)

# generate leverage plot gif
## sequence_levg <- seq(0, 15, 0.5)

## grid_seq <- expand.grid(sequence_levg,
##                         sequence_levg)

## output_plots <- future_map2(.x = grid_seq[,1],
##                             .y = grid_seq[,2],
##                             ~ leverage_plot(data = testdata,
##                                             x = x, y = y,
##                                             levg_x = .x,
##                                             levg_y = .y))

## dir.create("levgs")

## output_plots %>%
##     future_map(seq_len(length(output_plots)), ~ output_plots[[.x]] %>%
##                        ggsave(plot = .,
##                               filename = glue::glue("levgs/frame{.x}.png")))

## system("convert -delay 50 levgs/*.png levgs.gif")
```

To detect points with a lot of influence, we exclude point i and recompute the estimates to get $\hat{\beta_{(i)}}$ and $\hat{\sigma_{(i)}}^2$ where $(i)$ denotes that the $i^{th}$ case has been excluded. 

Hence $\hat{y_{(i)}} = x_i^T \hat{\beta_{(i)}}$. If $\hat{y_{(i)}} - y_i$ is large, then case $i$ is an outlier. To judge the size of a potential outlier, we need an appropriate scaling. 

We find: $\hat{var}(y_i - \hat{y_{(i)}}) = \hat{\sigma_{(i)}}^2 (1 + x_i^T(X_{(i)}^TX_{(i)})^{-1}x_i)$, and so we define the studentized (sometimes called jackknife or crossvalidated) residuals as 

$t_i = 
\frac{y_i - \hat{y_{(i)}}}{\hat{\sigma_{(i)}}(1 + x_i^T(X_{(i)}^TX_{(i)})^{-1}x_i)^{1/2}}=
\frac{\hat{\epsilon_i}}{\hat{\sigma}_{(i)}\sqrt{1 - h_i}} = 
r_i(\frac{n - p - 1}{n - p - r_i^2})^{1/2}
$

Which are distributed $t_{n-p-1}$ if the model is correct and $\epsilon ~ N(0, \sigma^2 I)$.

Since $t_i ~ t_{n - p - 1}$, we can calculate a p-value to test whether case $i$ is an outlier. However, we don't want to fall victim to statistical noise -- if we tested 100 pts with a 5% significance level, we would expect 5 outliers. 

Some adjustment of the level of the test is necessary to avoid identifying an excess of outliers. Suppose we want a level $\alpha$ test. Now P(all tests accept) = 1 - p(at least one rejects) $\geq 1 - \sum_j$ P(test i rejects) = $1 - n\alpha$. This suggests that if an overall level $\alpha$ test is required, then a level $\alpha / n$ should be used in each of the tests. This is called the **Bonferroni Correction** and it is used in contexts other than outliers. The larger n is, the more conservative it gets. 

```{r}
# generate studentized residuals
studentized_resid <- rstudent(lmod)

# get max of the studentized residuals
studentized_resid[which.max(abs(studentized_resid))]

# This is large for a standard normal scale, but is it an outlier? Compute Bonferroni
qt(.05 / (50 * 2), 44)
```

Since 2.85 is less than 3.53, we conclude that observation 46 is not an outlier. This indicates that it is not worth the trouble of comptuing the outlier test p-value unless the studentized residual exceeds about 3.5 in absolute value. 

**Points to Consider**

1. Two or more outliers next to each other can hide each other. 
2. An outlier in one model may not be an outlier in another when the variables have been changed or transformed. We will need to reinvestigate the question of outliers when we change the model. 
3. The error distribution may not be normal, so larger residuals may be expected. 
4. Individual outliers have less of an effect as we add more data. In large datasets, we generally only need to worry about clusters of outliers. Such clusters are less likely to occur by chance and more likely to represent actual structure.

**What to do**

1. Check for data entry error 
2. Examine the physical context. We may want to look for outliers, e.g. for things like fraud 
3. Exclude the point, but try reincluding it later if the model is changed. The exclusion of one or more outliers may make the difference between statistical significance or not. Be aware that casual or dishonest exclusion of outliers is regarded as serious research malpractice. 
4. Suppose we find outliers which are viewed as naturally occurring. Particularly if there are substantial numbers of such points, it is more efficient and reliable to use robust regression. Some adjustment to the inferential methods is necessary in such cases; in particular the uncertainty assessment for prediction needs to reflect the fact that extreme values can occur. 
5. It is dangerous to exclude outliers in an automatic manner. 

Here is an example of a dataset with multiple outliers

```{r}
data(star, package = "faraway")
star %<>% as_tibble()

(star %>%
    ggplot(aes(x = temp, y = light)) +
    geom_point() +
    xlab("log(Temperature)") + ylab("log(Light Intensity)") -> base_plot)

# fit a regression and add it to the plot
lmod <- lm(light ~ temp, star)

base_plot +
    geom_abline(intercept = lmod$coefficients[[1]],
                slope = lmod$coefficients[[2]],
                lty = 4, color = "mediumpurple")

# test for outliers
range(rstudent(lmod))
qt(.05 / (47 * 2), 45)
```

No outliers are found, even though we can clearly see them in the plot. 

Let's see what happens when we exclude the four upper left points (giants)

```{r}
lmodi <- lm(light ~ temp, data = star, subset = (temp > 3.6))

base_plot +
    geom_abline(intercept = lmodi$coefficients[[1]],
                slope = lmodi$coefficients[[2]],
                lty = 4, color = "mediumpurple") +
    geom_abline(intercept = lmod$coefficients[[1]],
                slope = lmod$coefficients[[2]],
                lty = 4, color = "skyblue")
```

This illustrates the problem of multiple outliers. We can visualize the problem here, but it would be much more difficult with higher dimensions. Generally, robust regression methods would be superior here.

# Influential Observations 

An influential point is one whose removal from the dataset would cause a large change in the fit. It may or may not be an outlier or have a large leverage, but it tends to have at least one of these properties.

We can consider the Cook statistics. They reduce the information to a single value for each case, and are defined as 

$D_i = \frac{(\hat{y} - \hat{y_{(i)}})^T(\hat{y} - \hat{y_{(i)}})}{p \hat{\sigma}^2} = \frac{1}{p}r_i^2 \frac{h_i}{1 - h_i}$


Where the first term $r_i^2$ is the residual effect and the second is the leverage. 

We can use the halfnormal plot of $D_i$ to identify influential observations

```{r}
lmod <- lm(sr ~ ., savings)

# generate halfnorm plot 
(lmod %>%
    augment() %>%
    .$.cooksd %>% 
    gghalfnorm::gghalfnorm() -> p_cooksd)

# exclude the largest influence point and refit
lmodi <- lm(sr ~ ., savings %>% slice(-46))

lmodi %>% augment()

tidy_diff <- function(lm1, lm2) {
    lm1 <- tidy(lm1)
    lm2 <- tidy(lm2)

    # add code here to get the difference between two tidy df outputs 
}

lmod %>% tidy() -> lm_1
lmodi %>% tidy() -> lm_2

```
