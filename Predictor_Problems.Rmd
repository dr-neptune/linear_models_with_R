# Problems with the Predictors

```{r}
library(tidyverse)
library(faraway)
library(magrittr)
library(broom)
library(furrr)
library(simex)

# make code run in parallel
num_cores <- availableCores() - 1
plan(multiprocess,  workers = num_cores)
```

## Errors in the Predictors

The regression model $Y = X\beta + \epsilon$ allows for Y being measured with error by having the $\epsilon$ term, but what if the X is measured with error? We could have errors in measuring X. 

We must not confused errors in predictors with treating X as a random variable. With observational data we can regard X as a random variable, but the regression inference will nonetheless proceed conditionally on a fixed value of X. 

Suppose that what we observe is ($x_i^O, y_i^O$) for $i = 1, ..., n$ which are related to the true values ($x_i^A, y_i^A$): $y_i^A = y_i^A + \epsilon_i$, $x_i^A = x_i^A + \delta_i$ where the errors $\epsilon_i$ and $\delta_i$ are independent. 

The true underlying relationship is $y_I^A = \beta_0 + \beta_1 x_i^A$, but since we only see our observed values what we get is $y_i^O = \beta_0 + \beta_i x_i^O + (\epsilon_i - \beta_1 \delta_1)$. 

Suppose we use least squares to estimate $\beta_0$ and $\beta_1$. Let's assume that $E\epsilon_i = E\delta_i = 0$ and that $var \epsilon_i = \sigma_\epsilon^2$, $var \delta_i = \sigma_\delta^2$. Let $\sigma_x^2 = \sum (x_i^A - \bar{x}^A)^2 / n$ and $\sigma_{x\delta} = cov(x^A, \delta)$. For observational data, $\sigma_x^2$ is almost the sample variance of $X^A$ while for a controlled experiment we can view it as a numerical measure of the spread of the design. A similar distinction should be made for $\sigma_{x\delta}$, but most often we can just assume that this is zero. 

Now $\hat{\beta_1} = \frac{\sum (x_i - \bar{x} y_i)}{\sum (x_i - \bar{x})^2}$ and after some calculation we get $E \hat{\beta_1} = \beta_1 \frac{\sigma_x^2 + \sigma_{x\delta}}{\sigma_x^2 + \sigma_\delta^2 + 2\sigma_{x\delta}}$. 

There are two main special cases: 

1. If there is no relationship between $X^A$ and $\delta$ and $\sigma_{x\delta} = 0$, this simplifies to $E\hat{\beta_1} = \beta_1 \frac{1}{1 + \sigma_\delta^2 / \sigma_x^2}$. So $\hat{\beta_1}$ will be biased toward zero, regardless of the sample size. If $\sigma_\delta^2$ is small relative to $\sigma_x^2$, we can effectively ignore the problem. In other words, if the variability in the errors of observation of $X$ is small relative to the range of $X$, then we needn't worry. For multiple predictors, measurement erros also bias the $\hat{\beta}$ in the direction of 0. 

2. In controlled experiments, we need to distinguish two ways in which error in x may arise. In the first case, we measure x with a true value $x^A$ and observed value $x^O$. If we repeat the measurement, we would have the same true value $x^A$, but a different $x^O$. In the second case, we fix $x^O$. Now if we repeat this, we would get the same $x^O$, but our true value $x^A$ would be different. In this latter case we have $\sigma_{x\delta} = cov(X^0 - \delta, \delta) = -\sigma_\delta^2$, and then we would have $E\hat{\beta_1} = \beta_1$,  and unbiased estimate. What this is doing is effectively reversing the roles of $x^A$ and $X^O$, and if we get to observe the true X, then we will get an unbiased estimate of $\beta_1$.

In cases where the error in X can not be ignored, we should consider alternatives to the least squares estimation of $\beta$. We can write the simple least squares regression equation as $\frac{y - \bar{y}}{SD_y} = r\frac{x - \bar{x}}{SD_x}$ such that $\hat{\beta_1} = r\frac{SD_y}{SD_x}$. 

Since we have errors in both x and y in our problem, we can argue that neither one in particular deserves the role of response or predictor and so the equation should be the same either way. One way to achive this is to set $\hat{\beta_1} = \frac{SD_y}{SD_x}$. This is known as the *geometric mean functional relationship*. 

Another approach is to use the SIMEX method of Cook and Stefanski (1994).

```{r}
data(cars)
cars %<>% as_tibble()
lmod <- lm(dist ~ speed, cars)

plot_lmod <- function(lmod, x, y) {
    lmod$model %>%
        as_tibble() %>%
        ggplot(aes(x = {{x}}, y = {{y}})) +
        geom_point() +
        geom_abline(intercept = lmod$coefficients[[1]],
                    slope = lmod$coefficients[[2]],
                    color = "mediumpurple",
                    lty = 2,
                    alpha = 0.8)
}

lmod %>%
    plot_lmod(speed, dist)
```

Let's focus on the effects of adding measurement error to the predictor.

```{r}
# fit model with no noise as a baseline 
lmod_2 <- lm(dist ~ I(speed + rnorm(50)), cars)

gen_noise <- function(noise_amt) {
    lmod <- lm(dist ~ I(speed + noise_amt * rnorm(50)), cars)
    lmod$noise_amount <- noise_amt
    return(lmod)
}

# fit models with noise
seq_len(10) %>%
    map(., ~ gen_noise(.x)) -> noise_lmods 

# save base plot
lmod %>%
    plot_lmod(speed, dist) -> p1

# create noise plots with additional lines 
noise_lmods %>%
    map(., ~ p1 +
               geom_abline(intercept = .x$coefficients[[1]],
                           slope = .x$coefficients[[2]],
                           color = "firebrick",
                           lty = 2,
                           alpha = 0.8) +
        ggtitle(glue::glue("Noise : {.x$noise_amount} * rnorm(50) on Speed"))) -> pred_errors

show_variance_of_plot <- function(lmods) {    
    add_lines <- function(lmod) {
        zoop <- geom_abline(intercept = lmod$coefficients[[1]],
                            slope = lmod$coefficients[[2]],
                            color = "firebrick",
                            lty = 2,
                            alpha = 0.3)
        
        return(zoop)
    }

    p2 <- p1
    for (i in seq_len(length(noise_lmods))) {
        p2 <- p2 + 
            add_lines(noise_lmods[[i]])
    }

    p2
}

noise_lmods %>%
    show_variance_of_plot()

create_ggplot_mapped_gif <- function(plotlist, title = "output") {
    require(gganimate)
    require(glue)
    
    dir.create("temp")
    
    plotlist %>%
        future_map2(.x = ., .y = seq_len(length(plotlist)),
                    ~ plotlist[[.y]] %>%
                        ggsave(plot = .,
                               filename = glue("temp/frame{.y}.png")))

    system(glue("convert -delay 50 temp/*.png {title}.gif"))

    fs::dir_delete("temp")
    
    gif_file(glue("{title}.gif"))
}

## pred_errors %>%
##     create_ggplot_mapped_gif(title = "pred_errors")
```

We see that the slope becomes shallower as the amount of noise increases. 

Suppose we knew that a predictor in the original data had been measured with a known error variance. Given what we have seen in the known measurement error models, we might extrapolate back to suggest an estimate of the slope under no measurement error. This is the idea behind SIMEX.

```{r}
# simulate the effects of adding normal random error with variances from 0.1:0.5
set.seed(8888)

slopes <- future_map(rep(1:5/10, each = 1000),
              ~ lm(dist ~ I(speed + sqrt(.x) * rnorm(50)),
                   cars)$coef[2]) %>%
    set_names(rep(1:5/10, each = 1000)) %>%
    enframe() %>%
    pivot_wider() %>%
    unnest() %>%
    unnest()

slopes %>%
    colMeans() %>%
    prepend(coef(lmod)[2]) -> betas

variances <- c(0, 1:5 / 10) + 0.5

gv_tib <- tibble("betas" = betas,
                 "variances" = variances)

gv <- lm(betas ~ variances, gv_tib)

gv %>%
    plot_lmod(x = variances, y = betas) +
    xlim(c(0, 1)) + ylim(c(3.86, 4))
```

From the above we see that the value of $\hat{\beta}$ at variance equal to 0 (no measurement error) is almost 4.

```{r}
lmod <- lm(dist ~ speed, cars, x = TRUE)
(simout <- simex(lmod, "speed", 0.5, B = 1000))
```

Essentially: 

- Assume fixed measurement error variance 
- Simulate adding more variance 
- Fit a linear model to extrapolate true intercept 

```{r}
# create a function which shows extrapolations.
# input a list of variances and output actual intercepts and plots of differences
## plot(simout)
## lmod_2 <- lm(mpg ~ ., mtcars)
## lmod_2 %>% tidy() %>% slice(2:nrow(.)) %>% pull(1) %>% simex(lmod_2, SIMEXvariable = ., measurement.error = rep(0.5, 10))
```

## Changes of Scale

A change of scale is often helpful when the variables take values which are all very large or very small. It can also be helpful to ensure numerical stability. 

Suppose we re-express $x_i$ as $(x_i + a)/b$. Rescaling $x_i$ leaves the t and F-tests and $\hat{\sigma}^2$ and $R^2$ unchanged, and $\hat{\beta_i} \to b \hat{\beta_i}$.

```{r}
data(savings, package = "faraway")
savings %<>% as_tibble()
lmod <- lm(sr ~ ., savings)
lmod %>% tidy()
```

Our coefficient for dpi (income) is very small. We can measure in thousands of dollars instead.

```{r}
savings %>%
    mutate(dpi = dpi / 1000) %>%
    lm(sr ~ ., .) -> lmod_2

lmod_2 %>% tidy()
```

We could also convert all the variables to standard units (mean 0 and variance 1) using scale()

```{r}
savings %>%
    mutate_all(scale) %>%
    lm(sr ~ ., .) -> lmod_3

lmod_3 %>% tidy()
```

In the model above, we see that the intercept is essentially zero. This is because the regression plane always runs through the point of the averages, which because of the centering, is now at the origin. 

This allows the predictors and the response to be placed on a comparable scale. It also allows the coefficients to be viewed as a kind of partial correlation, bound in [-1, 1]. The interpretation effect of this scaling is that the regression coefficients now represent the effect of one standard unit increase in the predictor on the response in standard units. 

When the predictors are on comparable scales, it can be helpful to construct a plot of the estimates with confidence intervals.

```{r}
coef_confint_plot <- function(lmod, title = "",
                              prescale = TRUE,
                              compare = FALSE) {
    if (compare) {
        prescale <- FALSE
    }
    
    if (prescale) {
        lmod$model %>%
            scale() %>%
            as_tibble() %>%
            lm(eval(lmod$call[[2]]), .) -> lmod
    }

    lmod %>%
        tidy(conf.int = TRUE) %>%
        slice(2:nrow(.)) -> tidy_lmod

    tidy_lmod %>%
        ggplot(aes(x = term, y = estimate,
                   ymin = conf.low, ymax = conf.high)) +
        geom_pointrange() +
        ggtitle(title,
                subtitle = "95% Confidence Interval") -> p1
    
    if (compare) {
        tidy_lmod %>%
            mutate(lower_bound = conf.low + estimate,
                   upper_bound = conf.high + estimate) %>%
            summarize(min = min(lower_bound),
                      max = max(upper_bound)) -> bounds 
        
        p1 <- p1 + ylim(c(bounds[[1]], bounds[[2]])) +
            ggtitle(title,
                    subtitle = "95% Confidence Interval")
        p2 <- coef_confint_plot(lmod, prescale = TRUE, compare = FALSE,
                                title = glue::glue("{title}_scaled")) +
            ylim(c(bounds[[1]], bounds[[2]]))
        
        cowplot::plot_grid(p1, p2, ncol = 2)
    } else {
        p1
    }
}

lmod %>% coef_confint_plot(compare = TRUE, title = "Coef Confint")

lmod %>% coef_confint_plot(prescale = FALSE) +
    ggtitle("Unscaled") +
    ylim(c(-5, 1)) -> p1

lmod %>% coef_confint_plot(prescale = TRUE) + ggtitle("Scaled") +
    ylim(c(-5, 1)) -> p2
```

In the presence of binary predictors, scaling might be done differently. We notice that the countries in the savings data divide into two clusters based on age. 

```{r}
# set a division at 35% for pop15
savings %<>%
    mutate(age = ifelse(pop15 > 35, 0, 1))
```

We encoded younger countries as 0 and older countries as 1. A binary predictor taking the values of 0/1 with equal probability has a standard deviation of 1/2. This suggests scaling the other continuous predictors by two SDs rather than one

```{r}
savings %<>%
    mutate(dpis = (dpi - mean(dpi)) / (2 * sd(dpi)),
           ddpis = (ddpi - mean(ddpi)) / (2 * sd(ddpi)))

lm(sr ~ age + dpis + ddpis, savings) %>% tidy()
```

Now the interpretation is easier: 

- The predicted difference between older and younger countries is that older countries have a 5.28% higher savings rate. This is a difference of two standard deviations. 
- A typical country with a high growth rate has a savings rate 2.47% higher than one with a low growth rate.

Another way to achieve a similar effect is to use a +-1 coding rather than 0/1 so that the standard scaling can be used on the continuous predictors.

## Collinearity

