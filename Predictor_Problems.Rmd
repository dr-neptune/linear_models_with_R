# Problems with the Predictors

```{r}
library(tidyverse)
library(faraway)
library(magrittr)
library(broom)
library(furrr)
library(simex)

# make code run in parallel
num_cores <- availableCores() - 1
plan(multiprocess,  workers = num_cores)
```

## Errors in the Predictors

The regression model $Y = X\beta + \epsilon$ allows for Y being measured with error by having the $\epsilon$ term, but what if the X is measured with error? We could have errors in measuring X. 

We must not confused errors in predictors with treating X as a random variable. With observational data we can regard X as a random variable, but the regression inference will nonetheless proceed conditionally on a fixed value of X. 

Suppose that what we observe is ($x_i^O, y_i^O$) for $i = 1, ..., n$ which are related to the true values ($x_i^A, y_i^A$): $y_i^A = y_i^A + \epsilon_i$, $x_i^A = x_i^A + \delta_i$ where the errors $\epsilon_i$ and $\delta_i$ are independent. 

The true underlying relationship is $y_I^A = \beta_0 + \beta_1 x_i^A$, but since we only see our observed values what we get is $y_i^O = \beta_0 + \beta_i x_i^O + (\epsilon_i - \beta_1 \delta_1)$. 

Suppose we use least squares to estimate $\beta_0$ and $\beta_1$. Let's assume that $E\epsilon_i = E\delta_i = 0$ and that $var \epsilon_i = \sigma_\epsilon^2$, $var \delta_i = \sigma_\delta^2$. Let $\sigma_x^2 = \sum (x_i^A - \bar{x}^A)^2 / n$ and $\sigma_{x\delta} = cov(x^A, \delta)$. For observational data, $\sigma_x^2$ is almost the sample variance of $X^A$ while for a controlled experiment we can view it as a numerical measure of the spread of the design. A similar distinction should be made for $\sigma_{x\delta}$, but most often we can just assume that this is zero. 

Now $\hat{\beta_1} = \frac{\sum (x_i - \bar{x} y_i)}{\sum (x_i - \bar{x})^2}$ and after some calculation we get $E \hat{\beta_1} = \beta_1 \frac{\sigma_x^2 + \sigma_{x\delta}}{\sigma_x^2 + \sigma_\delta^2 + 2\sigma_{x\delta}}$. 

There are two main special cases: 

1. If there is no relationship between $X^A$ and $\delta$ and $\sigma_{x\delta} = 0$, this simplifies to $E\hat{\beta_1} = \beta_1 \frac{1}{1 + \sigma_\delta^2 / \sigma_x^2}$. So $\hat{\beta_1}$ will be biased toward zero, regardless of the sample size. If $\sigma_\delta^2$ is small relative to $\sigma_x^2$, we can effectively ignore the problem. In other words, if the variability in the errors of observation of $X$ is small relative to the range of $X$, then we needn't worry. For multiple predictors, measurement erros also bias the $\hat{\beta}$ in the direction of 0. 

2. In controlled experiments, we need to distinguish two ways in which error in x may arise. In the first case, we measure x with a true value $x^A$ and observed value $x^O$. If we repeat the measurement, we would have the same true value $x^A$, but a different $x^O$. In the second case, we fix $x^O$. Now if we repeat this, we would get the same $x^O$, but our true value $x^A$ would be different. In this latter case we have $\sigma_{x\delta} = cov(X^0 - \delta, \delta) = -\sigma_\delta^2$, and then we would have $E\hat{\beta_1} = \beta_1$,  and unbiased estimate. What this is doing is effectively reversing the roles of $x^A$ and $X^O$, and if we get to observe the true X, then we will get an unbiased estimate of $\beta_1$.

In cases where the error in X can not be ignored, we should consider alternatives to the least squares estimation of $\beta$. We can write the simple least squares regression equation as $\frac{y - \bar{y}}{SD_y} = r\frac{x - \bar{x}}{SD_x}$ such that $\hat{\beta_1} = r\frac{SD_y}{SD_x}$. 

Since we have errors in both x and y in our problem, we can argue that neither one in particular deserves the role of response or predictor and so the equation should be the same either way. One way to achive this is to set $\hat{\beta_1} = \frac{SD_y}{SD_x}$. This is known as the *geometric mean functional relationship*. 

Another approach is to use the SIMEX method of Cook and Stefanski (1994).

```{r}
data(cars)
cars %<>% as_tibble()
lmod <- lm(dist ~ speed, cars)

plot_lmod <- function(lmod, x, y) {
    lmod$model %>%
        as_tibble() %>%
        ggplot(aes(x = {{x}}, y = {{y}})) +
        geom_point() +
        geom_abline(intercept = lmod$coefficients[[1]],
                    slope = lmod$coefficients[[2]],
                    color = "mediumpurple",
                    lty = 2,
                    alpha = 0.8)
}

lmod %>%
    plot_lmod(speed, dist)
```

Let's focus on the effects of adding measurement error to the predictor.

```{r}
# fit model with no noise as a baseline 
lmod_2 <- lm(dist ~ I(speed + rnorm(50)), cars)

gen_noise <- function(noise_amt) {
    lmod <- lm(dist ~ I(speed + noise_amt * rnorm(50)), cars)
    lmod$noise_amount <- noise_amt
    return(lmod)
}

# fit models with noise
seq_len(10) %>%
    map(., ~ gen_noise(.x)) -> noise_lmods 

# save base plot
lmod %>%
    plot_lmod(speed, dist) -> p1

# create noise plots with additional lines 
noise_lmods %>%
    map(., ~ p1 +
               geom_abline(intercept = .x$coefficients[[1]],
                           slope = .x$coefficients[[2]],
                           color = "firebrick",
                           lty = 2,
                           alpha = 0.8) +
        ggtitle(glue::glue("Noise : {.x$noise_amount} * rnorm(50) on Speed"))) -> pred_errors

show_variance_of_plot <- function(lmods) {    
    add_lines <- function(lmod) {
        zoop <- geom_abline(intercept = lmod$coefficients[[1]],
                            slope = lmod$coefficients[[2]],
                            color = "firebrick",
                            lty = 2,
                            alpha = 0.3)
        
        return(zoop)
    }

    p2 <- p1
    for (i in seq_len(length(noise_lmods))) {
        p2 <- p2 + 
            add_lines(noise_lmods[[i]])
    }

    p2
}

noise_lmods %>%
    show_variance_of_plot()

create_ggplot_mapped_gif <- function(plotlist, title = "output") {
    require(gganimate)
    require(glue)
    
    dir.create("temp")
    
    plotlist %>%
        future_map2(.x = ., .y = seq_len(length(plotlist)),
                    ~ plotlist[[.y]] %>%
                        ggsave(plot = .,
                               filename = glue("temp/frame{.y}.png")))

    system(glue("convert -delay 50 temp/*.png {title}.gif"))

    fs::dir_delete("temp")
    
    gif_file(glue("{title}.gif"))
}

## pred_errors %>%
# simulate the effects of adding normal random error with variances from 0.1:0.5
set.seed(8888)

slopes <- future_map(rep(1:5/10, each = 1000),
              ~ lm(dist ~ I(speed + sqrt(.x) * rnorm(50)),
                   cars)$coef[2]) %>%
    set_names(rep(1:5/10, each = 1000)) %>%
    enframe() %>%
    pivot_wider() %>%
    unnest() %>%
    unnest()

slopes %>%
    colMeans() %>%
    prepend(coef(lmod)[2]) -> betas

variances <- c(0, 1:5 / 10) + 0.5

gv_tib <- tibble("betas" = betas,
                 "variances" = variances)

gv <- lm(betas ~ variances, gv_tib)

gv %>%
    plot_lmod(x = variances, y = betas) +
    xlim(c(0, 1)) + ylim(c(3.86, 4))

merr <- 30

merr * c(1.1, 1.2, 1.3, 1.4, 1.5)

lmod$call[[2]][[2]]

##FIX THIS 
homemade_simex <- function(lmod, variable, measurement_err, numreps) {
    set.seed(8888)

    measurement_err * c(1.1, 1.2, 1.3, 1.4, 1.5) -> err_vec
    response <- as.character(lmod$call[[2]][[2]])
    form <- glue::glue("{response} ~ I({variable} + sqrt({.x}) * rnorm(50))")
    
    slopes <- future_map(rep(err_vec, each = numreps),
                         ~ lm(as.formula(form),
                              cars)$coef[2]) %>%
        set_names(rep(err_vec, each = numreps)) %>%
        enframe() %>%
        pivot_wider() %>%
        unnest() %>%
        unnest()
    
    slopes %>%
        colMeans() %>%
        prepend(coef(lmod)[2]) -> betas
    
    variances <- c(0, err_vec) + measurement_err
    
    gv_tib <- tibble("betas" = betas,
                     "variances" = variances)
    
    gv <- lm(betas ~ variances, gv_tib)
    
    gv %>%
        plot_lmod(x = variances, y = betas) +
        xlim(c(0, 1)) + ylim(c(3.86, 4))
}

homemade_simex(lmod, "speed", 0.5, 1000)
```

From the above we see that the value of $\hat{\beta}$ at variance equal to 0 (no measurement error) is almost 4.

```{r}
lmod <- lm(dist ~ speed, cars, x = TRUE)
(simout <- simex(lmod, "speed", 0.5, B = 1000))
```

Essentially: 

- Assume fixed measurement error variance 
- Simulate adding more variance 
- Fit a linear model to extrapolate true intercept 

```{r}
##     create_ggplot_mapped_gif(title = "pred_errors")
```

We see that the slope becomes shallower as the amount of noise increases. 

Suppose we knew that a predictor in the original data had been measured with a known error variance. Given what we have seen in the known measurement error models, we might extrapolate back to suggest an estimate of the slope under no measurement error. This is the idea behind SIMEX.

```{r}
# create a function which shows extrapolations.
# input a list of variances and output actual intercepts and plots of differences
## plot(simout)
## lmod_2 <- lm(mpg ~ ., mtcars)
## lmod_2 %>% tidy() %>% slice(2:nrow(.)) %>% pull(1) %>% simex(lmod_2, SIMEXvariable = ., measurement.error = rep(0.5, 10))
```

## Changes of Scale

A change of scale is often helpful when the variables take values which are all very large or very small. It can also be helpful to ensure numerical stability. 

Suppose we re-express $x_i$ as $(x_i + a)/b$. Rescaling $x_i$ leaves the t and F-tests and $\hat{\sigma}^2$ and $R^2$ unchanged, and $\hat{\beta_i} \to b \hat{\beta_i}$.

```{r}
data(savings, package = "faraway")
savings %<>% as_tibble()
lmod <- lm(sr ~ ., savings)
lmod %>% tidy()
```

Our coefficient for dpi (income) is very small. We can measure in thousands of dollars instead.

```{r}
savings %>%
    mutate(dpi = dpi / 1000) %>%
    lm(sr ~ ., .) -> lmod_2

lmod_2 %>% tidy()
```

We could also convert all the variables to standard units (mean 0 and variance 1) using scale()

```{r}
savings %>%
    mutate_all(scale) %>%
    lm(sr ~ ., .) -> lmod_3

lmod_3 %>% tidy()
```

In the model above, we see that the intercept is essentially zero. This is because the regression plane always runs through the point of the averages, which because of the centering, is now at the origin. 

This allows the predictors and the response to be placed on a comparable scale. It also allows the coefficients to be viewed as a kind of partial correlation, bound in [-1, 1]. The interpretation effect of this scaling is that the regression coefficients now represent the effect of one standard unit increase in the predictor on the response in standard units. 

When the predictors are on comparable scales, it can be helpful to construct a plot of the estimates with confidence intervals.

```{r}
coef_confint_plot <- function(lmod, title = "",
                              prescale = TRUE,
                              compare = FALSE) {
    if (compare) {
        prescale <- FALSE
    }
    
    if (prescale) {
        lmod$model %>%
            scale() %>%
            as_tibble() %>%
            lm(eval(lmod$call[[2]]), .) -> lmod
    }

    lmod %>%
        tidy(conf.int = TRUE) %>%
        slice(2:nrow(.)) -> tidy_lmod

    tidy_lmod %>%
        ggplot(aes(x = term, y = estimate,
                   ymin = conf.low, ymax = conf.high)) +
        geom_pointrange() +
        ggtitle(title,
                subtitle = "95% Confidence Interval") -> p1
    
    if (compare) {
        tidy_lmod %>%
            mutate(lower_bound = conf.low + estimate,
                   upper_bound = conf.high + estimate) %>%
            summarize(min = min(lower_bound),
                      max = max(upper_bound)) -> bounds 
        
        p1 <- p1 + ylim(c(bounds[[1]], bounds[[2]])) +
            ggtitle(title,
                    subtitle = "95% Confidence Interval")
        p2 <- coef_confint_plot(lmod, prescale = TRUE, compare = FALSE,
                                title = glue::glue("{title}_scaled")) +
            ylim(c(bounds[[1]], bounds[[2]]))
        
        cowplot::plot_grid(p1, p2, ncol = 2)
    } else {
        p1
    }
}

lmod %>% coef_confint_plot(compare = TRUE, title = "Coef Confint")

lmod %>% coef_confint_plot(prescale = FALSE) +
    ggtitle("Unscaled") +
    ylim(c(-5, 1)) -> p1

lmod %>% coef_confint_plot(prescale = TRUE) + ggtitle("Scaled") +
    ylim(c(-5, 1)) -> p2
```

In the presence of binary predictors, scaling might be done differently. We notice that the countries in the savings data divide into two clusters based on age. 

```{r}
# set a division at 35% for pop15
savings %<>%
    mutate(age = ifelse(pop15 > 35, 0, 1))
```

We encoded younger countries as 0 and older countries as 1. A binary predictor taking the values of 0/1 with equal probability has a standard deviation of 1/2. This suggests scaling the other continuous predictors by two SDs rather than one

```{r}
savings %<>%
    mutate(dpis = (dpi - mean(dpi)) / (2 * sd(dpi)),
           ddpis = (ddpi - mean(ddpi)) / (2 * sd(ddpi)))

lm(sr ~ age + dpis + ddpis, savings) %>% tidy()
```

Now the interpretation is easier: 

- The predicted difference between older and younger countries is that older countries have a 5.28% higher savings rate. This is a difference of two standard deviations. 
- A typical country with a high growth rate has a savings rate 2.47% higher than one with a low growth rate.

Another way to achieve a similar effect is to use a +-1 coding rather than 0/1 so that the standard scaling can be used on the continuous predictors.

## Collinearity

When some predictors are linear combinations of others, then $X^TX$ is singular and we have a lack of identifiability. Another name for this problem is exact collinearity -- there is no unique least squared estimate of $\beta$. The solution may require removing some predictors.

A more challenging problem arises when $X^TX$ is close to singular, but not exactly so. This is collinearity or multicollinearity. This leads to imprecise estimates of $\beta$. Generally we have problems identifying the correct sign of beta, the standard errors become inflated and the fit becomes sensitive to small changes in y. 

We can detect it in several ways: 

1. Examine the correlation matrix of the predictors for large pairwise collinearities
2. A regression of $x_i$ on all other predictors gives $R_i^2$. A value close to one indicates that one predictor can almost be predicted exactly by a linear combination of other predictors. We can repeat this for all predictors and possible discover the offending linear combinations by examining the regression coefficients of these regressions.
3. Examine the eigenvalues of $X^TX, \lambda_1 \geq ... \geq \lambda_p \geq 0$. Zero eigenvalues denote exact collinearity, which the presence of some small eigenvalues indicate multicollinearity. The condition number $\Kappa$ measures the relative sizes of the eigenvalues and is defined as $\Kappa = \sqrt(\frac{\lambda_1}{\lambda_p})$, where $\Kappa \geq 30$ is considered large. Other condition numbers like $\sqrt(\lambda_1 / \lambda_i)$ might also be useful because they indicate whether more than just one independent linear combination is to blame. 

We can see the effect of collinearity with this expression for $\hat{\beta_j}$:

$var \hat{\beta_j} = \sigma^2 (\frac{1}{1 - R_j^2}) \frac{1}{\sum_i (x_{ij} - \bar{x})^2}$

We see that if the predictor $x_j$ does not vary much, then the variance of $\hat{\beta_j}$ will be large. If $R_j^2$ is close to one, then the variance inflation factor $(1 - R_j^2)^{-1}$ will be large and so var $\hat{\beta_j}$ will also be large. 

```{r}
data(seatpos, package = "faraway")
seatpos %<>% as_tibble()
lmod <- lm(hipcenter ~ ., seatpos)
lmod %>% glance()
lmod %>% tidy()
```

We see that none of the predictors is significant, but the R^2 is reasonably large. This model shows signs of collinearity.

Let's take a look at pairwise correlations:

```{r}
library(corrr)

seatpos %>%
    correlate() %>%
    focus(-hipcenter, mirror = TRUE) %>%
    rearrange() %>%
    shave() -> cor_table

cor_table %>% fashion()
cor_table %>% rplot()
```

We see several large pairwise correlations between predictors.

We can test the effect of regression on x_i for all predictors

```{r}
seatpos %>% names() -> d_names

get_collinearity_r_sq <- function(data) {
    d_names <- data %>% names()

    d_names %>%
        paste("~ .") %>% 
        map(as.formula) %>%
        map_dbl(., ~ lm(.x, data) %>%
                       glance() %>%
                       pull(1)) %>%
        tibble("predictor" = d_names, "r_squared" = .) %>%
        mutate(VIF = (1 / (1 - r_squared))) %>% 
        arrange(desc(r_squared))
}

seatpos %>%
    get_collinearity_r_sq()
```

Things are not looking good for Ht and HtShoes.

We can interpret `r sqrt(308)` as telling us that the standard error for height with shoes is 17.5 times larger than it would have been without collinearity. We cannot apply this as a correction because we did not actually observe orthogonal data, but it does give us a sense of the size of the effect. 

Now we can check the eigendecomposition of $X^TX$ (not including the intercept in X)

```{r}
eigen_decomposition <- function(model) {
    mm <- model %>% model.matrix() %>% .[, -1]
    e <- eigen(t(mm) %*% mm)
    cat("Values:\n")
    print(e$values)
    cat("Proportions:\n")
    print(sqrt(e$values[1] / e$values))
}

lmod %>% eigen_decomposition()
```

There is a very large range in the eigenvalues and several condition numbers are large. This means that problems are being caused by more than just one linear  combination. 

There is substantial instability in these estimates. Suppose the measurement for hipcenter has a SD of 10mm. Let's see what happens when we add a random perturbation of this size to the response:

```{r}
compare_summaries <- function(m1, m2, remove_intercept = FALSE) {
    require(rlang)

    m1 %<>% tidy() %>%
        set_names(paste(colnames(.), "1", sep = "_")) %>%
        mutate_if(is.double, round, 4)

    m2 %<>% tidy() %>%
        set_names(paste(colnames(.), "2", sep = "_")) %>%
        mutate_if(is.double, round, 4)

    if (remove_intercept) {
        m1 %<>%
            slice(2:nrow(.))

        m2 %<>%
            slice(2:nrow(.))
    }
    
    m_tbl <- bind_cols(m1, m2)

    create_eqn <- function(name) {
        evaluator <- function(exp) {
            return(eval_tidy(parse_expr(exp),
                             data = m_tbl))
        }

        name_1 <- paste0(name, "_1")
        name_2 <- paste0(name, "_2")

        return(paste0("|",
                      evaluator(name_1), " - ",
                      evaluator(name_2), "| = ",
                      abs(evaluator(name_1) - evaluator(name_2))))
    }

    out_tbl <- m_tbl %>%
        mutate(estimate = create_eqn("estimate"),
               std.error = create_eqn("std.error"),
               statistic = create_eqn("statistic"),
               p.value = create_eqn("p.value")) %>%
        select(term = term_1, estimate, std.error, statistic, p.value)

    # get sums of digits
    out_tbl %>%
        map(., ~ str_extract(.x, pattern = "[[:digit:]]+\\.[[:digit:]]+$") %>%
            as.double() %>% na.omit()) -> digitz

    sod <- digitz %>% map(., ~.x %>% sum())
    coef_sod <- digitz %>% map(., ~ .x %>% tail(-1) %>% sum())

    out_tbl %<>%
        add_row("term" = "Sum of Differences",
                "estimate" = sod$estimate,
                "std.error" = sod$std.error,
                "statistic" = sod$statistic,
                "p.value" = sod$p.value) %>%
        add_row("term" = "Coefficient Sum of Differences",
                "estimate" = coef_sod$estimate,
                "std.error" = coef_sod$std.error,
                "statistic" = coef_sod$statistic,
                "p.value" = coef_sod$p.value)

    return(out_tbl)
}

lmod_2 <- lm(hipcenter + 10 * rnorm(38) ~ ., seatpos)

compare_summaries(lmod, lmod_2)
compare_summaries(lmod, lmod_2, remove_intercept = TRUE)
```

Although the R^2 and SE are very similar to the previous fit, we see big differences in the estimates (even when accounting for the intercept) and standard error. When we look at our most highly collinear terms, we see very large std errors (Ht and HtShoes). 

We have too many variables that are trying to do the same job of explaining the response. We can reduce the collinearity by carefully removing some of the variables. We can then make a more stable estimation of the coefficients and come to a more secure conclusion regarding the effect of the remaining predictors on the response.

```{r}
cor_table %>% rplot()
```

A lot of these are highly correlated: Ht, HtShoes, Seated, Arm, Thigh, Leg. We can focus on just Ht since it is easy to interpret

```{r}
lmod_3 <- lm(hipcenter ~ Age + Weight + Ht, seatpos)
lmod_3 %>% glance()
```

Comparing this with lmod, we see we have a very similar R^2 but many fewer predictors are used. 

The effect of collinearity on prediction is less serious. The accuracy of prediction depends on where the prediction is to be made. The greater the distance is from the observed data, the more unstable the prediction. Collinear data covers a much smaller fraction of the predictor space than it might appear to, so predictions tend to be greater extrapolations than with data that are closer to the orthogonality.

# Exercises

1. 

```{r}
faithful %<>% as_tibble()
lmod <- lm(eruptions ~ waiting, faithful, x = TRUE)

(simout <- simex(model = lmod, SIMEXvariable = "waiting", measurement.error = 30, B = 100))

lmod %>% tidy() %>% pull(2)
simout$coefficients

tidy(lmod)[2][[1]][1]

faithful %>%
    ggplot(aes(x = waiting, y = eruptions)) +
    geom_point() +
    geom_abline(intercept = simout$coefficients[[1]],
                slope = simout$coefficients[[2]],
                color = "blue",
                lty = 2,
                alpha = 0.5) +
    geom_abline(intercept = tidy(lmod)[2][[1]][1],
                slope = tidy(lmod)[2][[1]][2],
                color = "mediumpurple",
                lty = 2,
                alpha = 0.5) +
    ggtitle("Fitted Model vs. Simex Adjustment",
            subtitle = "30 second measurement error on waiting")

lmod$model
lmod$call %>% as.character() %>% .[2] %>% str_extract("[A-Za-z]+")
lmod$call %>% as.character() %>% .[2] %>% str_extract("~.*") %>% str_sub(start = 3)

get_terms <- function(lmod) {
    formula <- lmod$call %>% as.character() %>% .[2]

    y_term <- formula %>% str_extract("[A-Za-z]+")
    predictors <- formula %>% str_extract("~.*") %>% str_sub(start = 3)

    c(y_term, predictors)
}

lmod %>% get_terms()
tidy(lmod) %>% pull(2) %>% .[[1]]
simout$coefficients[[1]]

compare_simex <- function(simex_model, lmod) {

    get_terms <- function(lmod) {
        formula <- lmod$call %>% as.character() %>% .[2]
        y_term <- formula %>% str_extract("[A-Za-z]+")
        predictors <- formula %>% str_extract("~.*") %>% str_sub(start = 3)
        c(y_term, predictors)
    }

    # get terms and coefficients
    terms_out <- get_terms(lmod)
    lm_coefs <- tidy(lmod) %>% pull(2)
    simex_coefs <- simex_model$coefficients

    lmod$model %>%
        ggplot(aes(y = !!sym(terms_out[[1]]),
                   x = !!sym(terms_out[[2]]))) +
        geom_point() +
        geom_abline(intercept = simex_coefs[[1]],
                    slope = simex_coefs[[2]],
                    color = "blue",
                    lty = 2,
                    alpha = 0.5) +
        geom_abline(intercept = lm_coefs[[1]],
                    slope = lm_coefs[[2]],
                    color = "mediumpurple",
                    lty = 2,
                    alpha = 0.5) +
        ggtitle("Fitted Model vs. Simex Adjustment",
                subtitle = "30 second measurement error on waiting | blue = simex | purple = lmod")
}

compare_simex(simout, lmod)
```

