* Problems with Error
:PROPERTIES:
:header-args: :session R-session :results output value :colnames yes
:END:

#+NAME: round-tbl
#+BEGIN_SRC emacs-lisp :var tbl="" fmt="%.1f"
(mapcar (lambda (row)
          (mapcar (lambda (cell)
                    (if (numberp cell)
                        (format fmt cell)
                      cell))
                  row))
        tbl)
#+end_src

#+RESULTS: round-tbl

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(tidyverse)
library(faraway)
library(broom)
library(magrittr)
#+END_SRC

We have assumed that the error $\epsilon$ is iid, and we have also assumed that the errors are normally distributed in order to carry out normal statistical inference. These assumptions can often be violated and we must consider alternatives.

- When the errors are dependent, we can use *generalized least squares*
- When the errors are independent, but not identically distributed, we can use weighted least squares
- When errors are not normally distributed, we can use *robust regression*

** Generalized Least Squares

We have assumed that $var \epsilon = \sigma^2 I$, but sometimes errors have nonconstant variance or are correlated. Suppose instead that $var \epsilon = \sigma^2 \Sigma$, where $\sigma^2$ is unknown but $\Sigma$ is known. In other words, we know the correlation and relative variance between the errors, but we do not know the absolute scale of the variation. 

We can write $\Sigma = SS^T$ where S is a triangular matrix using the Cholesky Decomposition (a sort of square root for a matrix). Then we can transform the regression model as follows:

$y = x\beta + \epsilon$
$S^{-1}y = S^{-1}X\beta + S^{-1}\epsilon$
$y' = X'\beta + \epsilon'$

Then our sum of squared error is 

#+DOWNLOADED: /tmp/screenshot.png @ 2020-02-15 22:43:34
[[file:Problems with Error/screenshot_2020-02-15_22-43-34.png]]

#+DOWNLOADED: /tmp/screenshot.png @ 2020-02-15 22:44:04
[[file:Problems with Error/screenshot_2020-02-15_22-44-04.png]]

The main problem of GLS in practice is that $\Sigma$ may not be known and we have to estimate it. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(globwarm, package = "faraway")
gwarm <- globwarm %>% as_tibble()

lmod <- lm(nhtemp ~ ., gwarm)

lmod %>% tidy()
#+END_SRC

#+RESULTS:
| term        | estimate | std.error | statistic |              p.value |
|-------------+----------+-----------+-----------+----------------------|
| (Intercept) |    -15.2 |       1.7 |      -8.8 | 4.34443093246517e-15 |
| wusa        |     -0.1 |       0.0 |      -3.2 |                  0.0 |
| jasper      |      0.0 |       0.1 |       0.2 |                  0.9 |
| westgreen   |      0.1 |       0.0 |       2.0 |                  0.0 |
| chesapeake  |      0.0 |       0.0 |       0.2 |                  0.8 |
| tornetrask  |      0.1 |       0.0 |       1.4 |                  0.2 |
| urals       |      0.1 |       0.1 |       1.2 |                  0.2 |
| mongolia    |     -0.2 |       0.0 |      -3.5 |                  0.0 |
| tasman      |      0.0 |       0.0 |       0.1 |                  0.9 |
| year        |      0.0 |       0.0 |       8.7 | 9.59662211186724e-15 |

Then we can check whether our errors are correlated. This data was collected over time, so its a very real possibility. We calculate this by computing the correlation between the vector of residuals with the first and then the last term omitted.

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cor(residuals(lmod)[-1],
    residuals(lmod)[-length(residuals(lmod))])
#+END_SRC

#+RESULTS:
|   x |
|-----|
| 0.4 |

The simplest way to model this is the autoregressive form:

$\epsilon_{i+1} = \phi \epsilon_i + \delta_i$

where $\delta_i \sim N(0, \tau^2)$

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(nlme)

glmod <- gls(nhtemp ~ ., correlation = corAR1(form = ~year), na.omit(gwarm))

glmod %>% summary()
#+END_SRC

There are no -significant- predictors and the standard errors are rather large. However, there is substantial collinearity between the predictors, so this should not be interpreted as "no predictor effect".

We have a reasonably sized Phi estimate, which can expect as we would likely have carryover of temperature from one year to the next. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# check confidence intervals
intervals(glmod, which = "var-cov")
#+END_SRC

We see from the interval on Phi that our coefficient is far from 0 and shows significant positive correlation. 

For this example, we might investigate whether a more sophisticated model should apply to errors, perhaps an ARMA model. This can be implemented using the corARMA function. 

Another situation where correlation between errors might be anticipated is where observations are grouped in some way. For example, consider an experiment to compare eight varieties of oats. The growing area was heterogeneous and was grouped into 5 blocks. Each variety was sown once within each block and the yield in grams per 16 foot row were recorded. 

It is reasonable to expect that the errors are correlated (cor($\epsilon_i, \epsilon_j$)) is $\rho$ if i and j are in the same block and 0 otherwise. This is called the *compound symmetry* assumption and is modeled as follows:

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
glmod <- gls(yield ~ variety,
             data = oatvar,
             correlation = corCompSymm(form = ~1 | block))

intervals(glmod)
#+END_SRC

Our value of $\rho$ shows that there is a correlation around 0.4 between the errors within the blocks. 

** Weighted Least Squares

Sometimes the errors are uncorrelated, but have unequal variance where the form of the inequality is known. Weighted least squares is a special case of Generalized least squares that can be used in this case. 

Cases with low variability get a high weight, while those with high variability get a low weight. 

Some examples: 

#+DOWNLOADED: /tmp/screenshot.png @ 2020-03-03 20:14:46
[[file:Problems with Error/screenshot_2020-03-03_20-14-46.png]]


*** Example 

Elections for the French presidency proceed in two rounds. In 1981 there were 10 candidates in the first round, and the top two candidates went on to the second round. 

The losers in the first round can gain political favor by asking their supporters to vote for one of the two finalists. We wish to infer from the published vote totals how this transfer may have happened. 

 #+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(fpe, package = "faraway")
fpe %<>% as_tibble(rownames = "department")
fpe %>% head()
 #+END_SRC

A:K stand for the first round, and A2, B2 stand for the initial round victors (A and B were the first round victors). EI stands for the registered voters. The number of voters in the second round was higher than the first round, and the difference is denoted by N. 


#+DOWNLOADED: /tmp/screenshot.png @ 2020-03-03 20:24:32
[[file:Problems with Error/screenshot_2020-03-03_20-24-32.png]] 

If we treat the above as a regression equation, there will be some error from department to department. The error will have a variance in proportion to the number of voters because it will be like a variance of a sum rather than the variance of a mean. 

Since the weights should be inversely proportional to the variance, we should set the weights to 1 / EI. Notice this equation also has no intercept. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod <- lm(A2 ~ A + B + C + D + E + F + G + H + J + K + N - 1,
           fpe,
           weights = (1 / EI))

lmod %>% tidy()
#+END_SRC

#+RESULTS:
| term | estimate | std.error | statistic |              p.value |
|------+----------+-----------+-----------+----------------------|
| A    |      1.1 |       0.0 |      29.9 | 2.22469266213566e-13 |
| B    |     -0.1 |       0.0 |      -3.1 |                  0.0 |
| C    |      0.2 |       0.1 |       3.6 |                  0.0 |
| D    |      0.9 |       0.0 |      42.0 | 2.89029872162462e-15 |
| E    |      0.2 |       0.3 |       0.9 |                  0.4 |
| F    |      0.8 |       0.1 |      13.1 | 7.28918857815741e-09 |
| G    |      2.0 |       0.3 |       7.1 | 8.38088163028732e-06 |
| H    |     -0.6 |       0.5 |      -1.1 |                  0.3 |
| J    |      0.6 |       0.6 |       1.1 |                  0.3 |
| K    |      1.2 |       0.5 |       2.4 |                  0.0 |
| N    |      0.5 |       0.1 |       5.6 | 8.38901196792198e-05 |

Note that the weights do matter - look at the difference when they are left out 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod2 <- lm(A2 ~ A + B + C + D + E + F + G + H + J + K + N - 1,
           fpe)

lmod2 %>% tidy()
#+END_SRC

#+RESULTS:
| term | estimate | std.error | statistic |              p.value |
|------+----------+-----------+-----------+----------------------|
| A    |      1.1 |       0.0 |      30.5 | 1.76812290833848e-13 |
| B    |     -0.1 |       0.0 |      -4.8 |                  0.0 |
| C    |      0.3 |       0.1 |       4.5 |                  0.0 |
| D    |      0.9 |       0.0 |      52.0 | 1.80130937575726e-16 |
| E    |      0.7 |       0.3 |       2.3 |                  0.0 |
| F    |      0.8 |       0.1 |      15.4 | 9.74122083137977e-10 |
| G    |      2.2 |       0.2 |       9.8 | 2.31440504183119e-07 |
| H    |     -0.9 |       0.5 |      -1.8 |                  0.1 |
| J    |      0.1 |       0.6 |       0.2 |                  0.8 |
| K    |      0.5 |       0.5 |       1.1 |                  0.3 |
| N    |      0.6 |       0.1 |       6.1 | 3.66135131358541e-05 |

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
tibble("round" = c(letters[1:8], "j", "k", "n"),
       "absolute difference" = abs(coef(lmod) - coef(lmod2)))
#+END_SRC

#+RESULTS:
| round | absolute difference |
|-------+---------------------|
| a     |                 0.0 |
| b     |                 0.0 |
| c     |                 0.0 |
| d     |                 0.0 |
| e     |                 0.4 |
| f     |                 0.0 |
| g     |                 0.2 |
| h     |                 0.3 |
| j     |                 0.5 |
| k     |                 0.7 |
| n     |                 0.0 |

This causes substantial changes for some of the lesser candidates. 

There is one problem left, unrelated to weighting. Proportions are supposed to be between zero and one. We can impose an ad hoc fix by truncating the coefficients that violate this restriction either to 0 or 1 as appropriate. 

The offset function means that no coefficient will be fit which is the same as saying the coefficient will be 1. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lm(A2 ~ offset(A + G + K) + C + D + E + F + N - 1,
   fpe,
   weights = (1 / EI)) %>%
    tidy()
#+END_SRC

#+RESULTS:
| term | estimate | std.error | statistic |              p.value |
|------+----------+-----------+-----------+----------------------|
| C    |      0.2 |       0.1 |       4.1 |                  0.0 |
| D    |      1.0 |       0.0 |      41.6 | 4.04523592592162e-20 |
| E    |      0.4 |       0.2 |       1.7 |                  0.1 |
| F    |      0.7 |       0.1 |       9.2 | 2.08232910399788e-08 |
| N    |      0.6 |       0.1 |       5.1 |  6.9406557537893e-05 |

We see that: 

- Almost all of the voters for D initially voted for A
- Almost all of the voters for C initially voted for B
- The rest are different splits 

This analysis is somewhat crude and there are more sophisticated approaches. 

The pcls() function in the mgcv package of Wood provides a solution to the constrained least squares problem (which in this case requires that $0 \leq \hat{\beta_i} \leq 1$). 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(mgcv)

M <- list(w = (1 / fpe$EI),
          X = model.matrix(lmod),
          y = fpe$A2,
          Ain = rbind(diag(11), - diag(11)),
          C = matrix(0, 0, 0),
          array(0, 0),
          S = list(),
          off = NULL,
          p = rep(0.5, 11),
          bin = c(rep(0, 11), rep(-1, 11)))

A <- pcls(M)

names(A) <- colnames(model.matrix(lmod))

tibble("round" = c(letters[1:8], "j", "k", "n"),
       "coefficients" = round(A, 3))
#+END_SRC

#+RESULTS:
| round | coefficients |
|-------+--------------|
| a     |          1.0 |
| b     |          0.0 |
| c     |          0.2 |
| d     |          1.0 |
| e     |          0.4 |
| f     |          0.7 |
| g     |          1.0 |
| h     |          0.4 |
| j     |          0.0 |
| k     |          1.0 |
| n     |          0.6 |

The results are quite similar for the candidates C, D, E, and N who have substantial numbers of votes, but the coefficients for the small party candidates vary much more. 

In the examples where the form of the variance of epsilon is not completely known, we may model sigma using a smaller number of parameters. For example, 

$sd(\epsilon_i) = \gamma_0 + x_1^{\gamma_1}$

might seem reasonable for in a given situation. 

Consider, for example, the cars data from chapter 7:

#+BEGIN_SRC R :file plot.svg :results graphics file
lmod3 <- lm(dist ~ speed, cars)

plot(residuals(lmod3) ~ speed, cars)

cars %>% 
    ggplot(aes(x = speed, y = residuals(lmod3))) +
    geom_point()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

The plot reveals that the variation in the residuals increases with speed. 

One solution to this problem is to set the weights according to the above form and simultaneously estimate beta and gamma using maximum likelihood methods. 

#+BEGIN_SRC R :results output
wlmod <- gls(dist ~ speed,
             data = cars, 
             weights = varConstPower(1, form = ~ speed))

wlmod %>% summary()
#+END_SRC

#+RESULTS:
#+begin_example

Generalized least squares fit by REML
  Model: dist ~ speed 
  Data: cars 
       AIC      BIC    logLik
  412.8352 422.1912 -201.4176

Variance function:
 Structure: Constant plus power of variance covariate
 Formula: ~speed 
 Parameter estimates:
   const    power 
3.160444 1.022368 

Coefficients:
                 Value Std.Error   t-value p-value
(Intercept) -11.085378  4.052378 -2.735524  0.0087
speed         3.484162  0.320237 10.879947  0.0000

 Correlation: 
      (Intr)
speed -0.9  

Standardized residuals:
       Min         Q1        Med         Q3        Max 
-1.4520579 -0.6898209 -0.1308277  0.6375029  3.0757014 

Residual standard error: 0.7636833 
Degrees of freedom: 50 total; 48 residual
#+end_example

We see that $\gamma_0 = 3.16$ and $\hat{\gamma_1} = 1.022$. 

Since the latter is so close to 1, this variance function takes a simple form.  

** Testing for Lack of Fit 

How can we tell whether a model fits the data? 

If a model is correct, then our variance estimator should be an unbiased estimate of our actual variance. This suggests a testing procedure in which we compare our estimator to our actual variance. This is rather uncommon in practice, so we need some way to get an idea of what the actual variance is. 

We could make a comparison to some model free estimate of $\sigma^2$. We can do this if we have repeated values of the response for one or more fixed values of x. These replicates do need to be truly independent. For example, the cases in the data may be people and the response might be blood pressure. If we had different people but the same predictor values, we could get an idea of the between-subject variability and then we could construct an estimate of $sigma^2$ that does not depend on a particular model.

Let $y_{ij}$ be the ith observation in the group of true replicates j. The "pure error" or model-free estimate of $\sigma^2$ is given by 

**insert math eqns here for sspe and df **

There is a convenient way to compute the estimate. Fit a model that assigns one parameter to each group of observations with fixed x, then the variance estimator from this model will be the pure error. The model itself simply fits a mean to each group of estimates. Comparing this model to the regression model using the standard F-test gives us the lack-of-fit test. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(corrosion, package = "faraway")
corrosion %<>% as_tibble()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
corrosion %>%
    ggplot(aes(x = Fe, y = loss)) +
    geom_point() +
    labs(x = "Iron Content",
         y = "Weight Loss")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

Now we fit a straight line model 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod <- lm(loss ~ Fe, corrosion)

lmod %>% glance()
#+END_SRC

#+RESULTS:
| r.squared | adj.r.squared | sigma | statistic |              p.value |  df | logLik |  AIC |  BIC | deviance | df.residual |
|-----------+---------------+-------+-----------+----------------------+-----+--------+------+------+----------+-------------|
|       1.0 |           1.0 |   3.1 |     352.3 | 1.05535526706818e-09 | 2.0 |  -31.9 | 69.8 | 71.5 |    102.9 |        11.0 |

We have an Rsq value of 97% and what looks like a good fit to the data. 


#+BEGIN_SRC R :file plot.svg :results graphics file
corrosion %>%
    ggplot(aes(x = Fe, y = loss)) +
    geom_point() +
    geom_abline(slope = lmod$coefficients[[2]],
                intercept = lmod$coefficients[[1]],
                lty = 2) +
    labs(x = "Iron Content",
         y = "Weight Loss")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

We now fit a model that reserves a parameter for each group of data with the same value of x. This is accomplished by declaring the predictor to be a factor. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmoda <- lm(loss ~ factor(Fe), corrosion)

lmoda %>% tidy()
#+END_SRC

#+RESULTS:
| term           | estimate | std.error | statistic |              p.value |
|----------------+----------+-----------+-----------+----------------------|
| (Intercept)    |    128.6 |       0.8 |     158.9 | 4.18856557893574e-12 |
| factor(Fe)0.48 |     -5.6 |       1.3 |      -4.4 |                  0.0 |
| factor(Fe)0.71 |    -16.6 |       1.3 |     -13.0 | 1.28177759939807e-05 |
| factor(Fe)0.95 |    -24.7 |       1.6 |     -15.2 | 5.02984555927072e-06 |
| factor(Fe)1.19 |    -27.1 |       1.6 |     -16.7 | 2.91396797676481e-06 |
| factor(Fe)1.44 |    -36.7 |       1.3 |     -28.7 | 1.18433281242119e-07 |
| factor(Fe)1.96 |    -43.6 |       1.3 |     -34.1 | 4.23782401332906e-08 |

The fitted values are the means in each group and we can plot these. 

#+BEGIN_SRC R :file plot.svg :results graphics file
corrosion %>%
    ggplot(aes(x = Fe, y = loss)) +
    geom_point() +
    geom_point(data = augment(lmoda), aes(y = .fitted, x = corrosion$Fe),
               shape = 5, color = "blue") +
    labs(x = "Iron Content",
         y = "Weight Loss")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

We can now compare the two models in the usual way: 

#+BEGIN_SRC R :results output
anova(lmod, lmoda)
#+END_SRC

#+RESULTS:
: Analysis of Variance Table
: 
: Model 1: loss ~ Fe
: Model 2: loss ~ factor(Fe)
:   Res.Df     RSS Df Sum of Sq      F   Pr(>F)   
: 1     11 102.850                                
: 2      6  11.782  5    91.069 9.2756 0.008623 **
: ---
: Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

The low p-value indicates that we must conclude there is a lack of fit. We might investigate models other than a straight line, but no obvious alternatives are suggested by the plot. 

** Robust Regression 

When the errors are normally distributed, least squares regression is the best. When the errors follow some other distribution, other methods of model fitting may be considered. Short-tailed distributions are not as problematic as long-tailed distributions. 

Robust regression works better if you are dealing with more than one or two outliers. 

*** M-Estimation 

M-estimates modify the least squares idea to choose $\beta$ to minimize $\sum_{i = 1}^n \rho (y_i - x_i^T \beta)$

Some possible choices for $\rho$ are:
- $\rho(x) = x^2$ is just least squares
- $\rho(x) = |x|$ is called least absolute deviation regression or $L_1$ regression
- $\rho(x) = \frac{x^2}{2} if |x| \leq c; \frac{c|x| - c^2}{2} \mathrm{ow}$ is called Huber's Method and is a compromise between least squares and LAD regression. $c$ should be a robust estimate of $\rho$. A value proportional to the median of $|\hat{\epsilon}|$ is suitable. 

We demostrate the methods on the Galapagos data

Using least squares first: 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(gala, package = "faraway")
gala %<>% as_tibble(rownames = "location")

gala_form <- as.formula(Species ~ Area + Elevation + Nearest + Scruz + Adjacent)

# fit least squares model
(lsmod <- lm(gala_form, gala))

lsmod %>% tidy()
#+END_SRC

Least squares works well when there are normal errors, but performs poorly for long-tailed errors. The Huber method is the default choice of the rlm() function

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
require(MASS)

(rlmod <- rlm(gala_form, gala))
rlmod %>% tidy()
#+END_SRC

The $R^2$ statistic is not given becuase it doesn't make sense in the context of a robust regression. p-values are not given although we use the asymptotic normality of the estimator to make approximate inferences using the t-values. We see that the same two predictors, elevation and adjacent are significant. The numerical values of the predictors have changed somewhat and the standard errors are generally smaller for these two predictors. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
get_weight_diff <- function(mod1, mod2) {
    ferr <- function(errs) {
        paste(errs, collapse = "\n")
    }
    m1_err <- tidy(mod1)$std.error
    m2_err <- tidy(mod2)$std.error
    diff <- (m1_err - m2_err)
    total_diff <- sum(diff)
    cat("Model 1 Error:\n", ferr(m1_err), "\n",
        "Model 2 Error:\n", ferr(m2_err), "\n",
        "Difference\n", ferr(diff), "\n",
        "Overall Difference", total_diff)
}

get_weight_diff(lsmod, rlmod)
#+END_SRC

It is worth looking at the weights assigned by the final fit. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# extract the weights assigned by the final fit
tibble("weights" = rlmod$w,
       "location" = gala$location) %>%
    arrange(weights)
#+END_SRC

We can see that a few islands are substantially discounted in the calculation of the robust fit. 

We can also do Least Absolute Deviation ($L_1$) regression with the quantreg package

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(quantreg)

l1mod <- rq(gala_form, gala)

l1mod %>% tidy()
#+END_SRC

*** Least Trimmed Squares 

The Huber and $L_1$ methods will still fail if the large errors are sufficiently numerous and extreme in value. Least Trimmed Squares is an example of a resistant regression method. Resistent methods are good for dealing with data where we expect a certain number of bad observations that we want to have no weight in the analysis. 

LTS minimizes the sum of squares for the $q$ smallest residuals $\sum_{i = 1}^q \hat{\epsilon}_{(i)}^2$, where $q$ is some number less than $n$ and $(i)$ indicates sorting. This method has a high breakdown point because it can tolerate a large number of outliers depending on how $q$ is chosen. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
set.seed(123)

(ltsmod <- ltsreg(gala_form, gala))

ltsmod %>% coef()
#+END_SRC

The default choice of $q$ is $\lfloor \frac{n}{2} \rfloor + \lfloor \frac{p + 1}{2} \rfloor$. A genetic algorithm is used by default to compute the coefficients and is non-deterministic. 

An exhaustive search method can be used: 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(ltsmod <- ltsreg(gala_form, gala, nsamp = "exact"))

ltsmod %>% coef() %>% as.double() %>% set_names(names(coef(ltsmod)))
#+END_SRC

We now use a general method of inference that is especially useful when such theory is lacking -- the bootstrap. 

1. Generate $\epsilon^*$ by sampling with replacement from $\hat{\epsilon_1}, ..., \hat{\epsilon_n}$
2. Form $y^* = X \hat{\beta} + \epsilon^*$
3. Compute $\hat{\beta}^*$ from $(X, y^*)$ 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# create bootstrap matrix
bcoef <- matrix(0, 1000, 6)

# generate bootstrap samples
for (i in 1:1000) {
    newy <- predict(ltsmod) + residuals(ltsmod)[sample(30, rep = T)]
    brg <- ltsreg(newy ~ Area + Elevation + Nearest + Scruz + Adjacent, gala, nsamp = "best")
    bcoef[i, ] <- brg$coef
}

# make a 95% CI for this parameter by taking the empirical quantiles
colnames(bcoef) <- names(coef(ltsmod))
apply(bcoef, 2, function(x) quantile(x, c(0.025, 0.975)))

bcoef <- map(1:1000, ~ {
    newy <- predict(ltsmod) + residuals(ltsmod)[sample(30, rep = T)]
    brg <- ltsreg(newy ~ Area + Elevation + Nearest + Scruz + Adjacent, gala, nsamp = "best")
    as.double(brg$coef) %>% set_names(names(coef(ltsmod)))}) %>%
    bind_rows()

bcoef %>%
    map_df(., ~ quantile(.x, c(0.025, 0.975))) %>%
    add_column("names" = names(coef(ltsmod)))

bcoef %<>% t() %>% as_tibble() %>% set_names(names(coef(ltsmod))) %>% slice(1:2)
#+END_SRC

Zero lies outside the interval for Area, Nearest, and Adjacent so we are confident there is an effect for these predictors, although Nearest is marginal. 

#+BEGIN_SRC R :file plot.svg :results graphics file
ggplot(bcoef, aes(x = Area)) +
    geom_density()+
    xlim(1.45, 1.65) +
    geom_vline(xintercept = c(1.49, 1.62), lty = 2)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :file plot.svg :results graphics file
ggplot(bcoef, aes(x = Adjacent)) +
    geom_density()+
    xlim(-0.25, -0.13) +
    geom_vline(xintercept = c(-0.23375, -0.15138), lty = 2)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


The conclusion is that the Area variable is significant, which is in contract to the conclusion from the least squares fit. Which estimates are best? An examination of the Cook distances for the least squares fit shows the island of Isabela to be very influential. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# exclude Isabela
(limod <- lm(gala_form, filter(gala, !(location == "Isabela"))))
summary(limod)
#+END_SRC

This fit is much closer to the Least Trimmed Squares fit in that Area and Adjacent are very significant predictors. 

** Summary 

1. Robust estimators provide protection against long-tailed errors, but they cannot overcome problems with the choice of model and its variance structure
2. Robust estimates supply $\hat{\beta}$ and possibly standard errors without the associated inferential methods. Software and methodology for this inference require extra work. The bootstrap is a general purpose inferential method which is useful in these situations.
3. Robust methods can be used in addition to least squares as a confirmatory method. You have cause to worry if the two estimates are far apart. The source of the difference should be investigated
4. Robust estimates are useful when the data need to be fit automatically without the intervention of a skilled analyst. 

** Exercises 

*** 1. 
**** Pre. Load and look at the data 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(pipeline, package = "faraway")
pipeline %<>% as_tibble()

# skim
pipeline %>% skimr::skim()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
# plawt
pipeline %>%
    GGally::ggpairs()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

**** a. Fit a regression model Lab ~ Field. Check for nonconstant variance 

 #+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(lmod <- lm(Lab ~ Field, pipeline))
 #+END_SRC

 #+BEGIN_SRC R :file plot.svg :results graphics file
# to check for nonconstant variance, we look at our fitted vs. residuals plot
diag_plot <- function(data, x, y) {
    data %>%
        ggplot(aes(x = {{x}}, y = {{y}})) +
        geom_point() +
        geom_hline(yintercept = 0, lty = 2, alpha = 0.5)
}

lmod %>%
    augment() %>%
    diag_plot(.fitted, .resid)
 #+END_SRC

 #+RESULTS:
 [[file:plot.svg]]

 This looks like mild to strong nonconstant variance. We could also look at the fitted values against the sqrt of the absolute value of the residuals 

 #+BEGIN_SRC R :file plot.svg :results graphics file
lmod %>%
    augment() %>%
    diag_plot(.fitted, sqrt(abs(.resid)))
 #+END_SRC

 #+RESULTS:
 [[file:plot.svg]]

 Things still look very off. Let's try a numerical test for nonconstant variance just to be sure

 #+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lm(sqrt(abs(.resid)) ~ .fitted, augment(lmod)) %>% summary()
 #+END_SRC

 We are looking for a linear trend in the variation. We see a mild trend, with a .fitted coefficient of .02. 

**** b.  

We wish to use weights to account for the non-constant variance. Here we split the range of Field into 12 groups of size 9. Within each group, we conpute the variance of Lab as varlab and the mean of Field as meanfield.  

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# Faraway
(i <- order(pipeline$Field))
(npipe <- pipeline[i, ])
(ff <- gl(12, 9)[-108])
(meanfield <- unlist(lapply(split(npipe$Field, ff), mean)))
(varlab <- unlist(lapply(split(npipe$Lab, ff), var)))

# remake
pipeline %>%
    arrange(Field) %>%
    mutate(sample_group = map(1:12, ~ rep(.x, 9)) %>% flatten_dbl() %>% .[-length(.)]) %>%
    group_by(sample_group) %>%
    group_split(~ sample_group) %>%
    map(., ~ .x %>% summarize("meanfield" = mean(Field),
                            "varlab" = var(Lab))) %>%
    bind_rows() -> pline
#+END_SRC

Suppose we guess that the variance in the response is linked to the predictor in the following way: var$(Lab) = a_0 \mathrm{Field}^{a_1}$. Regress `log(varlab)` on `lab(meanfield)` to estimate $a_0$ and $a_1$. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(glmod <- lm(log(varlab) ~ log(meanfield), data = pline))
#+END_SRC

Since our coefficient is ~ 1, we can say that $\log{varlab} \propto \log{meanfield}$, suggesting a weighting of $w_i = x_i^{-1}$

To double check, we can view a plot of $|\hat{\epsilon_i}|$ against $x_i$

#+BEGIN_SRC R :file plot.svg :results graphics file
glmod %>%
    augment() %>%
    diag_plot(abs(.resid), log.meanfield.) +
    geom_smooth(method = "lm", se = FALSE, lty = 2, alpha = 0.3)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

From this plot we can see the positive relationship.

Now we can make the weighted model: 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(glmod <- lm(log(varlab) ~ log(meanfield), data = pline, weights = (1 / log(meanfield))))

# slightly better 
glmod %>%
    summary()
#+END_SRC

**** c. An alternative to weighting is transformation. Find transformations on lab and/or field so that in the transformed scale the relationship is approximately linear with constant variance. You may restrict your choice of transformation to sqrt, log, and inverse 

 #+BEGIN_SRC R :file plot.svg :results graphics file
pipeline %>% 
    diag_plot(sqrt(log(Field)), sqrt(log(Lab))) +
    geom_abline(slope = 1,
                intercept = 0)
 #+END_SRC

 #+RESULTS:
 [[file:plot.svg]]

*** 2.
**** Pre
#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(divusa, package = "faraway")
divusa %<>% as_tibble()

divusa %>% skimr::skim()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
divusa %>%
    GGally::ggpairs()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(lmod <- lm(divorce ~ unemployed + femlab + marriage + birth + military, data = divusa))
lmod %>% tidy()
#+END_SRC

#+RESULTS:
| term        | estimate | std.error | statistic |              p.value |
|-------------+----------+-----------+-----------+----------------------|
| (Intercept) |      2.5 |       3.4 |       0.7 |                  0.5 |
| unemployed  |     -0.1 |       0.1 |      -2.0 |                  0.1 |
| femlab      |      0.4 |       0.0 |      12.5 | 1.10645019087862e-19 |
| marriage    |      0.1 |       0.0 |       4.9 | 6.77180909547692e-06 |
| birth       |     -0.1 |       0.0 |      -8.3 | 4.02709555606474e-12 |
| military    |     -0.0 |       0.0 |      -1.9 |                  0.1 |

**** a. Make two graphical checks for correlated errors. What do you conclude? 

The most sensible one is the time index against residuals. 

#+BEGIN_SRC R :file plot.svg :results graphics file
lmod %>%
    augment() %>%
    diag_plot(divusa$year, .resid)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

In this plot, we clearly see that the residuals are all over the place and seem to follow a pattern 

#+BEGIN_SRC R :file plot.svg :results graphics file
p1 <- lmod %>%
    augment() %>%
    diag_plot(divusa$year, .resid) +
    geom_line(alpha = 0.3) +
    xlab("Year") + ylab("Residuals") +
    ggtitle("DivUSA Time ~ Residuals")

p2 <- diag_plot(divusa, year, rnorm(length(divusa$year))) +
    ylab("Normally Distributed Error") +
    xlab("Year") +
    ggtitle("What we would see without correlated error")

library(patchwork)
p1 / p2
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

The next visualization is the lead vs lag residuals

#+BEGIN_SRC R :file plot.svg :results graphics file
make_successive_res_plot <- function(data) {
    # get data length
    n <- data %>% nrow()

    # add lead lag
    data %<>%
    mutate(lead = .resid[2:nrow(.)] %>% append(0),
           lag = .resid[1:(nrow(.) - 1)] %>% append(0))

    # fit slope
    lm_in <- lm(lead ~ lag, data)

    data %>%
        ggplot(aes(x = lead, y = lag)) +
        geom_point() +
        geom_hline(yintercept = 0, lty = 2, alpha = 0.3) +
        geom_vline(xintercept = 0, lty = 2, alpha = 0.3) +
        geom_abline(slope = lm_in$coefficients[[2]],
                    intercept = lm_in$coefficients[[1]],
                    color = "blue",
                    alpha = 0.3)
}

lmod %>%
    augment() %>%
    make_successive_res_plot()
#+END_SRC

Finally we can also do a Durbin Watson test 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmtest::dwtest(divorce ~ unemployed + femlab + marriage + birth + military, data = divusa) %>%
    tidy()
#+END_SRC

#+RESULTS:
| statistic |              p.value | method             | alternative                            |
|-----------+----------------------+--------------------+----------------------------------------|
|       0.3 | 4.15101640488328e-26 | Durbin-Watson test | true autocorrelation is greater than 0 |

In the Durbin Watson test our alternative hypothesis is that true autocorrelation is greater than 0. Given our very very low p-value, we can be certain that there is autocorrelation. 


**** b. Allow for serial correlation with an AR(1) model for the errors. 

 #+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(glmod <- gls(divorce ~ unemployed + femlab + marriage + birth + military,
             correlation = corAR1(form = ~ year),
             method = "ML",
             divusa))

# femlab marriage birth and unemployed are significant
glmod %>% summary()

# femlab marriage and birth are significant. unemployed is on the fence
lmod %>% summary()
 #+END_SRC

 What is the estimated correlation and is it significant?

 The estimated correlation coefficient is 0.97. We can roughly interpret this as the output getting a larger contribution from the previous term relative to the noise. 

 Does the GLS model change which variables are found to be significant? 

 Yes. Unemployment was on the fence in our initial least squares model. In our generalized least squares model, our p value dropped to ~ 0.05 -> 0.02 

**** c. Speculate as to why there might be correlation in the errors 

 In this case, I think the most interesting plots are the following: 

 #+BEGIN_SRC R :file plot.svg :results graphics file
p2 <- ggplot(divusa, aes(x = year, y = femlab)) +
    geom_point() +
    geom_line(alpha = 0.2) + 
    xlab("Year") + ylab("Female Participation in Labor") +
    ggtitle("Time ~ Femlab")

p3 <- ggplot(divusa, aes(x = year, y = marriage)) +
    geom_point() +
    geom_line(alpha = 0.2) + 
    xlab("Year") + ylab("Marriages / 1k Unmarried Women") +
    ggtitle("Time ~ Marriage")

p4 <- ggplot(divusa, aes(x = year, y = divorce)) +
    geom_point() +
    geom_line(alpha = 0.2) + 
    xlab("Year") + ylab("Divorce Rate") +
    ggtitle("Time ~ Divorce Rate")

p1 / p4 / p2 / p3 
 #+END_SRC

 #+RESULTS:
 [[file:plot.svg]]

 The late 40s were a post-war boom time. Around this same period of time, we see a large bump in the female participation in the labor force. This is likely as a result of less men after WWII. I'm not certain about the drop in divorce in the late 60s. Perhaps financial independence in the 80s for women lead to higher divorce rates as well

*** 3.

For the salmonella dataset, fit a linear model with colonies as the response and log(dose + 1) as the predictor. Check for lack of fit.

**** Pre 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(salmonella, package = "faraway")
salmonella %<>% as_tibble()

salmonella %>%
    skimr::skim()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
salmonella %>%
    GGally::ggpairs()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(lmod <- lm(colonies ~ log(dose + 1), salmonella))
lmod %>% summary()
#+END_SRC

#+RESULTS:
| term          | estimate | std.error | statistic | p.value |
|---------------+----------+-----------+-----------+---------|
| (Intercept)   |     19.8 |       5.1 |       3.9 |     0.0 |
| log(dose + 1) |      2.4 |       1.1 |       2.1 |     0.0 |

#+BEGIN_SRC R :file plot.svg :results graphics file
ggplot(salmonella, aes(x = log(dose + 1), y = colonies)) +
    geom_point() +
    geom_abline(intercept = 19.823,
                slope = 2.396,
                lty = 2)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

This clearly doesn't fit well. Lets try again by grouping the dose into a factor 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(lmod2 <- lm(colonies ~ factor(log(dose + 1)), salmonella))

# we went from an adj rsq of .1713 -> adj rsq of .359 
lmod2 %>%
    summary()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
salmonella %>%
    ggplot(aes(x = log(dose + 1), y = colonies)) +
    geom_point() +
    geom_point(data = augment(lmod2), aes(y = .fitted, x = log(salmonella$dose + 1)),
               shape = 5, color = "mediumpurple") +
    geom_smooth(data = augment(lmod2), aes(y = .fitted, x = log(salmonella$dose + 1)),
                color = "mediumpurple") +
    labs(x = expression("\\log(dose + 1)"),
         y = "Colonies")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


Can we do better? 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(lmod3 <- lm(sqrt(colonies) ~ factor(log(dose + 1)), salmonella))

# from .35 -> .38 
lmod3 %>% summary()

# for laffs
(lmod4 <- lm(sqrt(sqrt(sqrt(sqrt(colonies)))) ~ factor(log(dose + 1)), salmonella))
# .3812 -> .3882 
lmod4 %>% summary()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
salmonella %>%
    ggplot(aes(x = log(dose + 1), y = sqrt(colonies))) +
    geom_point() +
    geom_point(data = augment(lmod3), aes(y = .fitted, x = log(salmonella$dose + 1)),
               shape = 5, color = "mediumpurple") +
    geom_smooth(data = augment(lmod3), aes(y = .fitted, x = log(salmonella$dose + 1)),
                color = "mediumpurple") +
    labs(x = "log(dose + 1)",
         y = "Colonies")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]




*** 4. 

For the cars dataset, fit a linear model with distance as the response and speed as the predictor. Check for lack of fit 

**** Pre

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(cars, package = "faraway")
cars %<>% as_tibble()

cars %>% skimr::skim()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
cars %>%
    GGally::ggpairs()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
# fit me a model 
lmod <- lm(dist ~ speed, cars)
lmod %>% summary()
#+END_SRC

Let's try to make it better than adj R^2 of .6438

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod2 <- lm(sqrt(dist) ~ speed, cars)
lmod2 %>% summary()
#+END_SRC

We got up to .7034

#+BEGIN_SRC R :file plot.svg :results graphics file
p1 <- ggplot(cars, aes(x = speed, y = dist)) +
    geom_point() +
    geom_abline(intercept = lmod$coefficients[[1]],
                slope = lmod$coefficients[[2]]) +
    ggtitle("Speed ~ Dist | No Transformation")

p2 <- ggplot(cars, aes(x = speed, y = sqrt(dist))) +
    geom_point() +
    geom_abline(intercept = lmod2$coefficients[[1]],
                slope = lmod2$coefficients[[2]]) +
    ylim(0, 120) + 
    ggtitle("sqrt(dist)")

p3 <- ggplot(cars, aes(x = speed, y = sqrt(dist))) +
    geom_point() +
    geom_abline(intercept = lmod2$coefficients[[1]],
                slope = lmod2$coefficients[[2]]) +
    ggtitle("sqrt(dist) zoom-in")

p1 / p2 / p3
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


#+BEGIN_SRC R :file plot.svg :results graphics file
library(ggforce)

ggplot(cars, aes(x = speed, y = dist)) +
    geom_point() +
    geom_autodensity(alpha = 0.3) +
    geom_smooth() +
    facet_matrix(vars(everything()), layer.diag = 2)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** 5. 

Using the stackloss data, fit a model with stack.loss as the response and the other three vars as predictors using the following methods: 

- Least Squares 
- Least Absolute Deviations ($L_1$)
- Huber method
- Least Trimmed Squares

Compare the results. 

Now use diagnostic methods to detect any outliers or influential points. 

Remove these points and then use least squares. Compare the results 

