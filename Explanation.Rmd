# Explanation

```{r}
library(tidyverse)
library(magrittr)
library(broom)
library(faraway)
```

Sometimes explanation means causation and sometimes it is just a description of the relationships between variables. Causal conclusions require stronger assumptions than those used for predictive models. This chapter looks at the conditions necessary to conclude a causal relationship and what can be said when we lack these conditions.

# Simple Meaning

```{r}
data(gala, package = "faraway")

gala %<>% as_tibble()

lmod <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala)

lmod %>% summary()
```

When looking at our beta_hat coefficients, it is alluring to describe them as something real, like a physical constant. Since our model has no strong theoretical underpinning, we are simply making an empirical model that we hope is a good approximation of reality. 

Suppose we look at $\hat{\beta_{\mathrm{Elevation}}} = 0.32$. We can make a simple interpretation: a unit increase in x1 (elevation, 1 meter) will produce a change of beta_hat in the response y. This is saying essentially this: Suppose we compare two islands, where the second island is exactly like the first, but with an elevation 1 meter higher. Then we predict that the second island would have 0.32 Species more than the first. If the second island had 100 more meters of elevation, we would estimate an incrase of 32 species for the second island. 


What if we consider a simpler model?

```{r}
lmod_2 <- lm(Species ~ Elevation, gala)
```

We now see that the coefficient for Elevation has changed from 0.32 to 0.2. This means that we cannot interpret a regression coefficient for a given predictor without reference to the other predictors in the model. 

We show the two fits for elevation below. To show the relationship between elevation and the response for the five predictor model, we fix the other predictors at a typical value (the means). We then compute the predicted response for the observed values of elevation. This is called an effect plot

```{r}
# book implementation
plot(Species ~ Elevation, gala)
abline(11.3, 0.201)
colMeans(gala)
p <- predict(lmod, data.frame(Area = 261.7, Elevation = gala$Elevation,
                              Nearest = 10.06, Scruz = 56.98, Adjacent = 261.1))
i <- order(gala$Elevation)
lines(gala$Elevation[i], p[i], lty = 2)

# ggplot implementation

col_means <- gala %>% map_dfc(mean)

preds_df <- tibble("Species" = rep(NA, 30),
                   "Elevation" = gala$Elevation,
                   "Area" = rep(col_means %>% pluck(3), 30),
                   "Nearest" = rep(col_means %>% pluck(5), 30),
                   "Scruz" = rep(col_means %>% pluck(6), 30),
                   "Adjacent" = rep(col_means %>% pluck(7), 30)) %>%
    predict(lmod, .)

gala %>%
    ggplot(aes(y = Species, x = Elevation)) +
    geom_point() + 
    geom_abline(intercept = lmod_2$coefficients[[1]],
                slope = lmod_2$coefficients[[2]]) +
    geom_line(aes(sort(gala$Elevation), preds_df[preds_df %>% order()]), lty = 2)
```

We need to be more specific in our interpretation now. We could say a unit increase in x1 with the other named predictors held constant will produce a change of beta_1 in the response y. 

For a simple one predictor model, a change in elevation will also be associated with a change in the other predictors.

The idea of holding variables constant makes no sense for observational data, since these observables are not under our control. There are also hidden variables that we can't account for in our data which have an effect on species diversity, which are also impossible to hold constant. 

Furthermore, there is no notion of causality. We can predict that taller islands have greater species diversity, but we can not say that altitude causes this. 


# Causality 

The view of causality here is that the causal effect of an action is the difference between the outcomes where the action was taken or was not taken. 

For example, let $T = 0$ for the control and $T = 1$ for the treatment. Let $y_i^T$ denote the response for a patient $i$ when $T$ applies. The causal effect for patient $i$ is then defined as 

$\delta_i = y_i^{1}, y_i^{0}$

The practical problem is that we cannot usually apply both treatment and control at the same time. We only see one of the two outcomes, and they are usually mutually exclusive. The outcome we do not see is called the **counterfactual**. 

# Designed Experiments 

In a designed experiment, we have control over T, or our test and control groups. We control as many variables as possible over the conditions within an experiment in order to make stronger conclusions about the outcomes of our analysis. In experiment design, randomization is key to success.

Consider the simplest case in which we only vary T, and assume that the groups T0 and T1 are mutually exclusive. Then we should randomly allocate each of our test subjects to the two groups in equal-ish numbers. 

There are two compelling reasons to use randomization: 

1. We can minimize the differences in individual variation. This reduces the chance that the control or treatment are favored. 

2. A permutation test can be used to test the null hypothesis that there is no difference between the groups. The justification of the permutation test relies on the observed allocation being chosen randomly from all possible allocations to the groups. Since permutation tests often agree with their normal counterparts, like t-tests, we can often also use these alternatives as well rather than setting up the permutation test. 


Suppose we are aware that our experimental units differ in identifiable ways. We may wish to incorporate this into the design by restricting the randomization. Then we can condition on our differences (suppose we use sex), and choose our random allocation from these subgroups. Then our treatment and control groups will also be balanced by the sex as well. In this example, sex is called a blocking variable. 

In other cases, there may be variables which are not properties of the experimental units (like sex), but can be assigned - like time of exercise. In both situations, we want to arrange the design so that it is orthogonal. This is not essential for causal conclsions, but it does greatly simplify them.

## Recap 

### Permutation Tests

Most of the tests initially covered have the assumption that errors are normally distributed. Permutation tests offer an alternative that need no assumption of normality. 

An example would be to measure the F-statistic for the original data, and then to sample n permutations of the original data, get the F-statistics for them, and see what proportion of the permuted F-statistics exceed the original value. 

### Orthogonal Design 

Orthogonality allows us to easily interpret the effect of one predictor without regard to another. We can essentially interpret the effect of some random variable X1 without the effect of X2. 

# Observational Data 

Sometimes we cannot control the assignment of T, and we can only obtain observational data. In some cases, we can control which cases we observe from those potentially available. A sample survey can be used to collect the data, which, while allowing stronger and wider conclusions, is still observational data. 

```{r}
data(newhamp, package = "faraway")

newhamp %<>% as_tibble()

# machine ballots | D for Digital
newhamp %>%
    filter(votesys == "D") %>%
    select(2:3) %>%
    map_dfc(sum)

# paper ballots | H for Hand
newhamp %>%
    filter(votesys == "H") %>%
    select(2:3) %>%
    map_dfc(sum)
```

We use the proportion voting for Obama in each ward as the response. Strictly speaking this is a binomial response and should be modeled as such. There is, however, a normal approximation to the binomial which holds when the sample is large enough and the probabilities are not close to 0 or 1. That holds for this sample. 

A binomial variance is $np(1-p)$ for proportion p and sample size n. Both of these vary in this example, so the assumption of equal variance is violated here. We see how to fix this in ch 8.2, but to stay on track here we ignore it.

```{r}
# fit a linear model
newhamp %>% glimpse()

lmod <- lm(pObama ~ votesys, newhamp)

lmod %>% summary()
```

In the output above, a 1 represents hand voting, and a 0 represents digital voting. This model takes the form $y_i = \beta_0 + \beta_1 T_i + \epsilon_i$. 

We see that when digital voting is used, our votesysH predictor is 0 and we have a $\hat{\beta_0}$ value of ~ 35%. When hand voting is used, we get a votesysH predictor of 1 and we get a 4% higher probability output than digitally. We see that the P value for the indicator variable is very low, indicating it is significant. This means that Obama received a significantly higher proportion of the vote in the hand voting wards. 

Did the voting method have some causal effect on the outcome? 

Suppose that the "correct" model involved some third variable $Z$ and took the form:

$y_i = \beta_0^* + \beta_1^* T_i + \beta_2^*Z_i + \epsilon_i$ and suppose this $Z$ was linked to $T$ by $Z_i = \gamma_0 + \gamma_1 T_i + \epsilon_i^`$. 

$Z$ is sometimes called a **confounding variable**. If we substitute this into the former model, we find that

$y_i = \beta_0^* + \beta_1^* T_i + \beta_2^*Z_i + \epsilon_i$

$y_i = \beta_0^* + \beta_1^* T_i + \beta_2^*\gamma_0 + \gamma_1 T_i + \epsilon_i^` + \epsilon_i$

$y_i = \beta_0^* + (\beta_1^* + \beta_2^*\gamma_0 + \gamma_1) T_i + \epsilon_i^` + \epsilon_i$

Does this third variable Z exist for the new hampshire voting example? 

Consider the proportion of votes for Howard Dean, a Democratic candidate in the previous campaign. We can add this term to the model:

```{r}
lmod2 <- lm(pObama ~ votesys + Dean, newhamp)

lmod2 %>% summary()
```

We see that the effect of the voting system is no longer statistically significant, and the proportion voting for Dean shows a positive relationship to the proportion voting for Obama. This third variable is related to our "treatment" variable, votesys

```{r}
summary(lm(Dean ~ votesys, newhamp))
```

We see that there is an active confounder in this situation. In the next section we show how we can use counterfactual notions to clarify the effect of the voting system on preferences for Obama.

# Matching 

