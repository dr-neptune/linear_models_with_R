# Explanation

```{r}
library(tidyverse)
library(magrittr)
library(broom)
library(faraway)
```

Sometimes explanation means causation and sometimes it is just a description of the relationships between variables. Causal conclusions require stronger assumptions than those used for predictive models. This chapter looks at the conditions necessary to conclude a causal relationship and what can be said when we lack these conditions.

# Simple Meaning

```{r}
data(gala, package = "faraway")

gala %<>% as_tibble()

lmod <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala)

lmod %>% summary()
```

When looking at our beta_hat coefficients, it is alluring to describe them as something real, like a physical constant. Since our model has no strong theoretical underpinning, we are simply making an empirical model that we hope is a good approximation of reality. 

Suppose we look at $\hat{\beta_{\mathrm{Elevation}}} = 0.32$. We can make a simple interpretation: a unit increase in x1 (elevation, 1 meter) will produce a change of beta_hat in the response y. This is saying essentially this: Suppose we compare two islands, where the second island is exactly like the first, but with an elevation 1 meter higher. Then we predict that the second island would have 0.32 Species more than the first. If the second island had 100 more meters of elevation, we would estimate an incrase of 32 species for the second island. 


What if we consider a simpler model?

```{r}
lmod_2 <- lm(Species ~ Elevation, gala)
```

We now see that the coefficient for Elevation has changed from 0.32 to 0.2. This means that we cannot interpret a regression coefficient for a given predictor without reference to the other predictors in the model. 

We show the two fits for elevation below. To show the relationship between elevation and the response for the five predictor model, we fix the other predictors at a typical value (the means). We then compute the predicted response for the observed values of elevation. This is called an effect plot

```{r}
# book implementation
plot(Species ~ Elevation, gala)
abline(11.3, 0.201)
colMeans(gala)
p <- predict(lmod, data.frame(Area = 261.7, Elevation = gala$Elevation,
                              Nearest = 10.06, Scruz = 56.98, Adjacent = 261.1))
i <- order(gala$Elevation)
lines(gala$Elevation[i], p[i], lty = 2)

# ggplot implementation

col_means <- gala %>% map_dfc(mean)

preds_df <- tibble("Species" = rep(NA, 30),
                   "Elevation" = gala$Elevation,
                   "Area" = rep(col_means %>% pluck(3), 30),
                   "Nearest" = rep(col_means %>% pluck(5), 30),
                   "Scruz" = rep(col_means %>% pluck(6), 30),
                   "Adjacent" = rep(col_means %>% pluck(7), 30)) %>%
    predict(lmod, .)

gala %>%
    ggplot(aes(y = Species, x = Elevation)) +
    geom_point() + 
    geom_abline(intercept = lmod_2$coefficients[[1]],
                slope = lmod_2$coefficients[[2]]) +
    geom_line(aes(sort(gala$Elevation), preds_df[preds_df %>% order()]), lty = 2)
```

We need to be more specific in our interpretation now. We could say a unit increase in x1 with the other named predictors held constant will produce a change of beta_1 in the response y. 

For a simple one predictor model, a change in elevation will also be associated with a change in the other predictors.

The idea of holding variables constant makes no sense for observational data, since these observables are not under our control. There are also hidden variables that we can't account for in our data which have an effect on species diversity, which are also impossible to hold constant. 

Furthermore, there is no notion of causality. We can predict that taller islands have greater species diversity, but we can not say that altitude causes this. 


# Causality 

The view of causality here is that the causal effect of an action is the difference between the outcomes where the action was taken or was not taken. 

For example, let $T = 0$ for the control and $T = 1$ for the treatment. Let $y_i^T$ denote the response for a patient $i$ when $T$ applies. The causal effect for patient $i$ is then defined as 

$\delta_i = y_i^{1}, y_i^{0}$

The practical problem is that we cannot usually apply both treatment and control at the same time. We only see one of the two outcomes, and they are usually mutually exclusive. The outcome we do not see is called the **counterfactual**. 

# Designed Experiments 

In a designed experiment, we have control over T, or our test and control groups. We control as many variables as possible over the conditions within an experiment in order to make stronger conclusions about the outcomes of our analysis. In experiment design, randomization is key to success.

Consider the simplest case in which we only vary T, and assume that the groups T0 and T1 are mutually exclusive. Then we should randomly allocate each of our test subjects to the two groups in equal-ish numbers. 

There are two compelling reasons to use randomization: 

1. We can minimize the differences in individual variation. This reduces the chance that the control or treatment are favored. 

2. A permutation test can be used to test the null hypothesis that there is no difference between the groups. The justification of the permutation test relies on the observed allocation being chosen randomly from all possible allocations to the groups. Since permutation tests often agree with their normal counterparts, like t-tests, we can often also use these alternatives as well rather than setting up the permutation test. 


Suppose we are aware that our experimental units differ in identifiable ways. We may wish to incorporate this into the design by restricting the randomization. Then we can condition on our differences (suppose we use sex), and choose our random allocation from these subgroups. Then our treatment and control groups will also be balanced by the sex as well. In this example, sex is called a blocking variable. 

In other cases, there may be variables which are not properties of the experimental units (like sex), but can be assigned - like time of exercise. In both situations, we want to arrange the design so that it is orthogonal. This is not essential for causal conclsions, but it does greatly simplify them.

## Recap 

### Permutation Tests

Most of the tests initially covered have the assumption that errors are normally distributed. Permutation tests offer an alternative that need no assumption of normality. 

An example would be to measure the F-statistic for the original data, and then to sample n permutations of the original data, get the F-statistics for them, and see what proportion of the permuted F-statistics exceed the original value. 

### Orthogonal Design 

Orthogonality allows us to easily interpret the effect of one predictor without regard to another. We can essentially interpret the effect of some random variable X1 without the effect of X2. 

# Observational Data 

Sometimes we cannot control the assignment of T, and we can only obtain observational data. In some cases, we can control which cases we observe from those potentially available. A sample survey can be used to collect the data, which, while allowing stronger and wider conclusions, is still observational data. 

```{r}
data(newhamp, package = "faraway")

newhamp %<>% as_tibble()

# machine ballots | D for Digital
newhamp %>%
    filter(votesys == "D") %>%
    dplyr::select(2:3) %>%
    map_dfc(sum)

# paper ballots | H for Hand
newhamp %>%
    filter(votesys == "H") %>%
    dplyr::select(2:3) %>%
    map_dfc(sum)
```

We use the proportion voting for Obama in each ward as the response. Strictly speaking this is a binomial response and should be modeled as such. There is, however, a normal approximation to the binomial which holds when the sample is large enough and the probabilities are not close to 0 or 1. That holds for this sample. 

A binomial variance is $np(1-p)$ for proportion p and sample size n. Both of these vary in this example, so the assumption of equal variance is violated here. We see how to fix this in ch 8.2, but to stay on track here we ignore it.

```{r}
# fit a linear model
newhamp %>% glimpse()

lmod <- lm(pObama ~ votesys, newhamp)

lmod %>% summary()
```

In the output above, a 1 represents hand voting, and a 0 represents digital voting. This model takes the form $y_i = \beta_0 + \beta_1 T_i + \epsilon_i$. 

We see that when digital voting is used, our votesysH predictor is 0 and we have a $\hat{\beta_0}$ value of ~ 35%. When hand voting is used, we get a votesysH predictor of 1 and we get a 4% higher probability output than digitally. We see that the P value for the indicator variable is very low, indicating it is significant. This means that Obama received a significantly higher proportion of the vote in the hand voting wards. 

Did the voting method have some causal effect on the outcome? 

Suppose that the "correct" model involved some third variable $Z$ and took the form:

$y_i = \beta_0^* + \beta_1^* T_i + \beta_2^*Z_i + \epsilon_i$ and suppose this $Z$ was linked to $T$ by $Z_i = \gamma_0 + \gamma_1 T_i + \epsilon_i^`$. 

$Z$ is sometimes called a **confounding variable**. If we substitute this into the former model, we find that

$y_i = \beta_0^* + \beta_1^* T_i + \beta_2^*Z_i + \epsilon_i$

$y_i = \beta_0^* + \beta_1^* T_i + \beta_2^*\gamma_0 + \gamma_1 T_i + \epsilon_i^` + \epsilon_i$

$y_i = \beta_0^* + (\beta_1^* + \beta_2^*\gamma_0 + \gamma_1) T_i + \epsilon_i^` + \epsilon_i$

Does this third variable Z exist for the new hampshire voting example? 

Consider the proportion of votes for Howard Dean, a Democratic candidate in the previous campaign. We can add this term to the model:

```{r}
lmod2 <- lm(pObama ~ votesys + Dean, newhamp)

lmod2 %>% summary()
```

We see that the effect of the voting system is no longer statistically significant, and the proportion voting for Dean shows a positive relationship to the proportion voting for Obama. This third variable is related to our "treatment" variable, votesys

```{r}
summary(lm(Dean ~ votesys, newhamp))
```

We see that there is an active confounder in this situation. In the next section we show how we can use counterfactual notions to clarify the effect of the voting system on preferences for Obama.

# Matching 

Suppose we randomly allocate two pools of experimental subjects to treatment and control. We try to balance them, but it may still be imbalanced with respect to the confounders, i.e. one group may be imbalanced in the treatment or control throwing off the others. We could try to fix this by forming matched pairs where the two members of the pair are as alike as possible with respect to the confounders. Then we would randomly assign the treatment and control groups. 

We can then determine the effect of the treatment by looking at the difference in response for each pair. Then we have the knowledge that we have adjusted for the effect of confounding by balancing the differences between treatment and control. 

On the New Hampshire data above, we can perform the matching technique. We wish to form matched pairs based on the similarity of proportion formerly supporting Dean. Within each pair, we would make a random assignment. 

IRL, we can not make the random assignment but we can find pairs of wards with similar values of Dean proportion where one uses hand voting and the other uses digital. 

```{r}
library(Matching)

set.seed(8888)

newhamp %<>%
    mutate(votesys = ifelse(votesys == "H", TRUE, FALSE))

mm <- GenMatch(newhamp$votesys, newhamp$Dean, ties = FALSE, caliper = 0.05, pop.size = 1000)

mmatch <- mm$matches[, 1:2]

newhamp[c(19, 83), c("Dean", "pObama", "votesys")]
```

Since Dean is a continuous variable, it is difficult to find exact matches. We set caliper = 0.05 to accept matches within 0.05 standard deviations of Dean. We also specified that ties are not allowed so that each treatment ward will be matched to just one control. 

```{r}
plot(pObama ~ Dean, newhamp, pch = votesys + 1)
with(newhamp, segments(Dean[mm$match[,1]], pObama[mm$match[,1]],
                       Dean[mm$match[,2]], pObama[mm$match[,2]]))
```

```{r}
matches <- tibble("dean_1" = newhamp$Dean[mmatch[,1]],
                      "p_obama_1" = newhamp$pObama[mmatch[,1]],
                      "dean_2" = newhamp$Dean[mmatch[,2]],
                      "p_obama_2" = newhamp$pObama[mmatch[,2]])

newhamp %>%
    ggplot(aes(x = Dean, y = pObama)) +
    geom_point(aes(color = votesys, shape = votesys)) +
    geom_segment(data = matches, aes(x = dean_1,
                     y = p_obama_1,
                     xend = dean_2,
                     yend = p_obama_2), alpha = 0.2) +
    scale_color_brewer(palette = "Set2")
```

We can compute the difference between the pairs and perform a one sample t-test.

```{r}
matches %<>%
    mutate(diff_dean = dean_1 - dean_2,
           diff_obama = p_obama_1 - p_obama_2)

t.test(matches$diff_obama) %>% tidy()

matches %>%
    ggplot(aes(x = diff_obama, y = dean_1)) +
    geom_point() + 
    geom_vline(xintercept = 0) +
    geom_violin(alpha = 0.2, fill = "mediumpurple") 
```

From the above, we see that the differences are not significantly different from zero. In the plot, we see that the matched pairs show no real preference for hand or digital voting. 


We were not able to do an experiment, and we were not able to view the counterfactual, but we were able to show matched wards with similar political outlooks by using the prior Dean vote proportion. Based on this comparison, we see no significant difference between the two methods. The observed difference is because voters who are inclined to pick Obama are also more likely to be present in hand voting wards. 

# Covariate Adjustment 

We can display the two earlier linear models for the proportion voting for Obama.

```{r}
plot(pObama ~ Dean, newhamp, pch = votesys+1)
abline(h = c(.353, 0.353 + 0.042), lty = 1:2)
abline(0.221, 0.5229)
abline(0.221-0.005, 0.5229, lty = 2)
```

```{r}
lm1 <- lmod$coefficients
lm2 <- lmod2$coefficients

newhamp %>%
    ggplot(aes(x = Dean, y = pObama)) +
    geom_point(aes(shape = votesys)) +
    geom_hline(yintercept = lm1[[1]], lty = 1) +
    geom_hline(yintercept = lm1[[1]] + lm1[[2]], lty = 2, color = "blue") +
    geom_abline(intercept = lm2[[1]], slope = lm2[[3]], color = "mediumpurple", lty = 1) +
    geom_abline(intercept = lm2[[1]] - lm2[[2]], slope = lm2[[3]], lty = 2,
                color = "mediumpurple") + 
    geom_segment(data = matches, aes(x = dean_1, 
                     y = p_obama_1,
                     xend = dean_2,
                     yend = p_obama_2), alpha = 0.2)
```

If we consider the expected response for a fixed value of Dean, the difference between digital and hand is given by the vertical distance between the two lines. Each pair of connected lines vertically represents a local realizaion of that vertical difference. When we average the pairwise distance, we get an estimate of the vertical difference. Thus matching on a covariate and fitting a linear model where we adjust for the covariate by including it in the model are two ways to estimate the same thing. 

Both approaches to drawing conclusions from observation studies are useful. The covariate adjustment method, sometimes called controlling for a covariate is easier to use and extends well to multiple confounders. It does however require that we specify a functional form of the covariate in the model in an appropriate way. We used linear models, but in higher dimensions it may be harder to verify (at least visually). 

The matching approach is more robust in that it does not requre that specify a functional form. Our matching approach made use of the overlap in the data. The covariate adjustment method can also make full use of the data when there is less overlap, but it is vulnerable to the dangers of extrapolation when making comparisons. 

# Qualitative Support for Causation 

Sir Bradford Hill was a central figure in establishing the causal link between smoking and lung cancer. In Hill (1965) he laid out several general considerations that can reinforce the case for a causal link: 

**Strength** - A large beta_hat value is useful. Known covariates can be adjusted for, while unobserved and unsuspected confounding variables can easily lead to small effects. It is less credible that some variable whose existence was previously unknown could counteract such a large effect.

**Consistency** - A similar effect is found across many different population groups. Replication by independent research groups is particularly important.

**Specificity** - The supposed causal factor is associated with a particular response and not with a wide range of other possible responses. If a particular outcome is prevalent in subjects with a particular factor, and the rest of the cohorts with that factor don't experience any other effects more strongly than others with a different factor then the case for causality is stronger. 

**Temporality** - The supposed causal factor is determined or fixed before the outcome or response is generated. Sometimes it is not clear whether X causes Y or vice versa. It helps if X hapens before Y if we want to establish the direction of the effect. 

**Gradient** - The response increases or decreases monotonically as the supposed causal variable increases. 

**Plausibility** - There is a credible theory suggesting a causal effect. 

**Experiment** - A natural experiment where subjects have apparently been randomly assigned values of the causal variable.

# Exercises 

1. 

```{r}
data(teengamb, package = "faraway")

teengamb %<>% as_tibble()

get_combns <- function(data, holdout) {
    # turn holdout into regex
    remove_pattern <- holdout %>%
        paste(collapse = "|")

    # get number of predictors after accounting for holdout
    npreds <- length(names(data)) - length(holdout)

    # create combinations
    map(1:npreds, ~ names(data) %>%
                        str_remove_all(., pattern = remove_pattern) %>%
                        .[. != ""] %>%
                        append("") %>%
                        gtools::combinations(n = length(.),
                                             r = (length(.) - .x),
                                             repeats.allowed = FALSE) %>%
                        as_tibble() %>% 
                        unite(col = "form", sep = "+") %>%
                        mutate(form = str_remove(form, "^[^A-Za-z]"))) %>%
        bind_rows() %>%
        distinct() %>%
        slice(1:nrow(.) - 1) %>%
        arrange(str_length(form)) %>%
        flatten_chr()
}

# grab the sex beta_hat predictor
grab_beta_hat <- function(lm_out) {
    lm_out %>%
        tidy() %>%
        slice(2) %>%
        pluck(2)
}

# get lms
tg_combns <- teengamb %>%
    get_combns(holdout = c("gamble", "sex")) %>% 
    map(., ~ lm(data = teengamb, as.formula(paste0("gamble ~ sex + ", .x)))) 

tg_combns[[8]] <- lm(gamble ~ sex, teengamb)

# redo Q1 
s_betas <- teengamb %>%
    get_combns(holdout = c("gamble", "sex")) %>%
    map(., ~ lm(data = teengamb, as.formula(paste0("gamble ~ sex +", .x)))) %>%
    map(., ~ grab_beta_hat(.x)) %>%
    flatten_dbl()

s_betas %>% summary()
```

The beta hat values for sex seem to be relatively stable, with a range of about 14, and always consistently negative and large. 

2. 

```{r}
data(odor, package = "faraway")

odor %>% get_combns(holdout = c("odor", "temp")) %>% 
    map(., ~ lm(data = odor, as.formula(paste0("odor ~ temp +", .x)))) %>%
    map(., ~ .x %>% summary() %>% tidy() %>% slice(2)) %>%
    bind_rows()
```

Through each of the models, the coefficient stays the same; the standard error changes slightly; the t-statistics and p-values are fairly consistent. Of these models, the only one that stands out is the third one as its std.error is lower indicating less uncertainty within its beta_hat estimate. 

3. 

```{r}
# a 
teengamb %>%
    ggplot(aes(x = income, y = gamble, shape = factor(sex), color = factor(sex))) +
    geom_point() +
    labs(shape = "sex", color = "sex") -> base_plot

base_plot

# b
(lmod <- lm(gamble ~ income + sex, teengamb))

(lmod0 <- lm(gamble ~ income, teengamb %>% filter(sex == 0)))
(lmod1 <- lm(gamble ~ income, teengamb %>% filter(sex == 1)))

base_plot +
    geom_abline(intercept = lmod0$coefficients[[1]], slope = lmod0$coefficients[[2]],
                color = "red") +
    geom_abline(intercept = lmod1$coefficients[[1]],
                slope = lmod1$coefficients[[2]],
                color = "blue")
    
```

```{r}
# c
library(Matching)
set.seed(8888)

# change data type to use the evolutionary matching algo 
teengamb %<>%
    mutate(sex = ifelse(sex == 0, TRUE, FALSE))

# generate matches 
mm <- GenMatch(Tr = teengamb$sex, X = teengamb$gamble, ties = TRUE, caliper = 0.05, pop.size = 1000)

# show matches in context of the original data
show_match <- function(data, matches) {
    1:nrow(matches) %>%
        map(., ~ matches %>%
                   slice(.x) %>%
                   flatten_dbl()) %>%
        map(., ~ slice(.data = data, .[1], .[2]))
}

# get matches
teen_matches <- teengamb %>% show_match(matches = mmatch)

teen_matches
``` 

With a caliper value of 0.05, there were 36 matches. To see how many were not matched, we could calculate 47 * 47 - 47 - 36 = 2126.

```{r}
# d.
teen_matches %<>%
    bind_rows()

## teen_matches %<>%
##     mutate(s1 = ifelse(sex,
##                        gamble,
##                        NA),
##            s2 = ifelse(!sex,
##                        gamble,
##                        NA))

## teen_matches %>% 
##     ggplot(aes(x = sex, y = gamble, color = sex)) +
##     geom_point() +
##     geom_segment(data = teen_matches,
##                  aes(x = teen_matches$sex[[1]],
##                      y = s1,
##                      xend = teen_matches$sex[[2]],
##                      yend = s2)) + 
##     scale_color_brewer(palette = "Set2")
```


