* Model Selection
:PROPERTIES:
:header-args: :session R-session :results output value :colnames yes
:END:

#+NAME: round-tbl
#+BEGIN_SRC emacs-lisp :var tbl="" fmt="%.1f"
(mapcar (lambda (row)
          (mapcar (lambda (cell)
                    (if (numberp cell)
                        (format fmt cell)
                      cell))
                  row))
        tbl)
#+end_src

#+RESULTS: round-tbl

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(tidyverse)
library(faraway)
library(broom)
library(magrittr)
#+END_SRC


In this chapter, we consider the the problem of selecting the "best" subset of predictors. 

** Hierarchical Models 

Some models have a natural hierarchy. When selecting variables, it is important to respect the hierarchy. 

For example, if we have a polynomial regression, we should not drop the $x$ term and keep the $x^2$ term. 

For models with interactions, we should not consider removing the $x_1 x_2$ interaction term without simultaneously considering the removal of the $x_1^2$ and $x_2^2$ terms. 

** Testing Based Procedures 

Backward elimination is the simplest of all variable selection procedures. We start with all the predictors, and then remove the predictor with the highest p-value greater than $\alpha_{crit}$. Then refit the model, and remove the remaining least significant procedure. Sooner or later, all "nonsignificant" predictors will be removed and the selection process will complete. 

The $\alpha_{crit}$ is sometimes called the p to remove and does not have to be 5%. If prediction performance is the goal, then a 15 to 20% cutoff may work best, although methods designed for optimal prediction should be preferred. 

Forward selection is the same thing, but backwards. We start with no variables, and for all the variables not in the model we check their p-values if we add them to the model. 

Stepwise regression is the combination of both forward and backward elimination. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(state)

state.x77 %<>%
    as_tibble() %>%
    janitor::clean_names()

## example of backward elimination
lmod <- lm(life_exp ~ ., state.x77)

lmod %>% summary()

## first remove area, then illiteracy, then income, the population
lmod %>% step(direction = "backward") -> lmod_b
lmod %>% step(direction = "forward") -> lmod_f
lmod %>% step(direction = "both") -> lmod_fb

lmod_b %>% summary()
lmod_f %>% summary()
lmod_fb %>% summary()
#+END_SRC

Testing-based procedures are relatively cheap computationally and easy to understand, but do have some drawbacks.

1. Because of the one at a time nature of adding / dropping variables, it is possible to miss the "optimal" model.

2. The p-values should not be treated too literally. There is so much multiple testing occurring that the validity is dubious. The removal of less significant predictors tends to increase the significance of the remaining predictors. This leads one to overstate the importance of the remaining predictors.

3. The procedures are not directly linked to final objectives of prediction or explanation, so they may not really help solve the problem of interest. Variable selection tends to amplify the statistical significance of the variables that stay in the model. Variables that are dropped can still be correlated with the response; they could be dropped simply because they provide no additional explanatory effect beyond those variables already included in the model 

4. Stepwise variable selection tends to pick models that are smaller than desirable for prediction purposes. 

Except in simple cases where only a few models are compared or in highly structured hierarchical models, testing-based variable selection should not be used. 

** Criterion-Based Procedures 

We choose the model that minimizes the Akaike Information Criterion: $AIC = -2 L(\hat{\theta}) + 2p$. For linear regression models, the -2 * max log likelihood = $n \log(\frac{RSS}{n}) + c$. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(leaps)

(b <- regsubsets(life_exp ~ ., state.x77))
rs <- summary(b)

nrow(lmod$model)

aic_out <- function(lmod) {
    n <- nrow(lmod$model)
    rss <- sum(lmod$residuals^2)
    npred <- 2:length(lmod$coefficients)

    n * log(rss / n) + max(npred) * 2
}

aic_out(lmod)
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
## plot AIC 
aic_out <- 50 * log(rs$rss / 50) + (2:8) * 2

ggplot(enframe(aic_out), aes(x = name, y = value)) +
    geom_point() +
    xlab("Number of Predictors") +
    ylab("Akaike Information Criterion")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

We see that the AIC is minimized by a choice of 4 predictors -- population, murder, hs_grad, and frost. 

Another commonly used criterion is the adjusted $R^2$, written $R_a^2$. Recall that $R^2 = 1 - \frac{RSS}{TSS}$. Our adjusted criterion is $R_a^2 = 1 - \frac{\frac{RSS}{n - p}}{\frac{TSS}{n - 1}} = 1 - \frac{n - 1}{n - p}(1 - R^2) = 1 - \frac{\hat{\sigma_{model}^2}}{\hat{\sigma_{null}^2}}$

#+BEGIN_SRC R :file plot.svg :results graphics file
enframe(rs$adjr2) %>%
    ggplot(aes(x = name, y = value)) +
    geom_point() +
    xlab("Number of Predictors") +
    ylab("Adjusted R^2")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

Our final criterion is Mallow's C_p statistic. 

A good model should predict well, so the average mean squared error of prediction should be a good criterion. This can be estimated by the C_p statistic:

$C_p = \frac{RSS_p}{\hat{\sigma^2}} + 2p - n$

where $\hat{\sigma^2}$ is the model with all predictors and $RSS_p$ indicates the RSS from a model with p parameters. 

#+BEGIN_SRC R :file plot.svg :results graphics file
enframe(rs$cp) %>%
    ggplot(aes(x = name, y = value)) +
    geom_point() +
    geom_line(color = "green", alpha = 0.2) +
    geom_abline(slope = 1, intercept = 0, color = "blue", lty = 2, alpha = 0.5) +
    xlab("Number of Predictors") +
    ylab(latex2exp::TeX("C_p")) +
    ggtitle(latex2exp::TeX("Mallow's $C_p$"))
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion <- function(data, formula, print = FALSE, leaps_in = "") {
    if (leaps_in == "") {
        (b <- leaps::regsubsets(formula, data))
    } else {
        b <- leaps_in
    }
    rs <- summary(b)
    aic_out <- nrow(data) * log(rs$rss / nrow(data)) + (2:length(b$xnames)) * 2

    list("Akaike Information Criterion" = aic_out,
         "Adjusted R^2" = rs$adjr2,
         "Mallow's $C_p$" = rs$cp) %>%
        imap(., ~ {
            .x %>%
                enframe() %>%
                ggplot(aes(x = name, y = value)) +
                geom_point() +
                geom_line(color = "green", alpha = 0.2) +
                xlab("Number of Predictors") +
                ylab(latex2exp::TeX(str_to_title(.y))) +
                ggtitle(latex2exp::TeX(.y))
        }) -> p_out

    p_out[[3]] %<>% `+`(geom_abline(slope = 1, intercept = 0, color = "blue", alpha = 0.3, lty = 2))
    
    if (print) reduce(p_out, `/`)
    else p_out
}

all_criterion(state.x77, as.formula("life_exp ~ .")) -> p_out

library(patchwork)

reduce(p_out, `/`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


Variable selection methods are sensitive to outliers and influential points. 

Let's check for high leverage points:
 
#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod %>%
    augment() %>%
    pull(.hat) %>% 
    `names<-`(state.abb) %>%
    sort() %>%
    rev()
#+END_SRC

We can see that Alaska has high leverage. Let's try excluding it.

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion(state.x77, as.formula(life_exp ~ .)) -> p1
all_criterion(state.x77[-2, ], as.formula(life_exp ~ .)) -> p2

list(p1, p2) %>% flatten() %>% reduce(`+`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

We can see that removing Alaska made all the responses much smoother across the predictor counts. 

#+BEGIN_SRC R :file plot.svg :results graphics file
## stripchart(data.frame(scale(state.x77)),
##            method = "jitter",
##            las = 2,
##            vertical = TRUE)
library(pipeR)

scale(state.x77) %>%
    as_tibble() %>%
    mutate("state_name" = state.abb) %>%
    pivot_longer(-state_name,
                 names_to = "predictor",
                 values_to = "std_ct") %>>%
    (~ inter_df) %>% 
    ## assign("inter_df", value = ., pos = 1) %>% 
    ggplot(aes(x = predictor, y = std_ct, group = predictor)) +
    geom_jitter(position = position_jitter(0.2), aes(color = predictor)) +
    geom_point(data = subset(inter_df, std_ct > 3), color = "black", size = 10, shape = 1) +
    scale_color_viridis_d()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


In the figure above, we see that population and area are skewed. We can try transforming them: 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
b <- regsubsets(life_exp ~ log(population) + income + illiteracy + murder + hs_grad + frost + log(area), state.x77)
rs <- summary(b)
rs$which[which.max(rs$adjr), ]
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
scale(state.x77) %>%
    as_tibble() %>%
    mutate("state_name" = state.abb,
           population = log(population),
           area = log(area)) %>%
    pivot_longer(-state_name,
                 names_to = "predictor",
                 values_to = "std_ct") %>>%
    (~ inter_df) %>% 
    ## assign("inter_df", value = ., pos = 1) %>% 
    ggplot(aes(x = predictor, y = std_ct, group = predictor)) +
    geom_jitter(position = position_jitter(0.2), aes(color = predictor)) +
    geom_point(data = subset(inter_df, std_ct > 3), color = "black", size = 10, shape = 1) +
    scale_color_viridis_d()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** Summary 

Hypothesis testing based methods use a restricted search through the space of potential models and use a dubious method for choosing between models when repeated many times. Criterion-based methods typically involve a wider search and compare models in a more preferable manner. 

If several models are suggested which fit as well as each other, consider:

1. Do the models have similar qualitative consequences?
2. Do they make similar predictions?
3. What is the cost of measuring the predictors?
4. Which has the best diagnostics? 

** Exercises 

** 1. Use the prostate data with lpsa as the response and the other variables as predictors. Implement the following variable selection methods to determine the best model: 

 #+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
prostate %<>% as_tibble()

prostate %>% skimr::skim()
 #+END_SRC

 #+BEGIN_SRC R :file plot.svg :results graphics file
prostate %>%
    ggpairs()
#+END_SRC

 #+RESULTS:
 [[file:plot.svg]]

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
all_criterion <- function(data, formula, print = FALSE, summary = FALSE) {
    (b <- leaps::regsubsets(formula, data))
    rs <- summary(b)

    if (summary) return(rs)
    
    aic_out <- nrow(data) * log(rs$rss / nrow(data)) + (2:length(b$xnames)) * 2

    list("Akaike Information Criterion" = aic_out,
         "Adjusted R^2" = rs$adjr2,
         "Mallow's $C_p$" = rs$cp) %>%
        imap(., ~ {
            .x %>%
                enframe() %>%
                ggplot(aes(x = name, y = value)) +
                geom_point() +
                geom_line(color = "green", alpha = 0.2) +
                xlab("Number of Predictors") +
                ylab(latex2exp::TeX(str_to_title(.y))) +
                ggtitle(latex2exp::TeX(.y))
        }) -> p_out

    p_out[[3]] %<>% `+`(geom_abline(slope = 1, intercept = 0, color = "blue", alpha = 0.3, lty = 2))
    
    if (print) reduce(p_out, `/`)
    else p_out
}
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
all_stepwise <- function(data, formula, print = TRUE) {
    lmod <- lm(formula, data)
    c("backward", "forward", "both") %>>% (~ names_out) %>%  map(., ~ lmod %>% step(direction = .x)) %>% set_names(names_out) -> mods
    mods %>%
        map(glance) %>%
        bind_rows(.id = "model") %>%
        pivot_longer(-model, names_to = "metric") %>>%
        (~ metrics_out) %>% 
        ggplot() +
        geom_point(aes(x = model, y = value, color = model),
                   alpha = 0.75) +
        facet_wrap(~ metric, scales = "free") +
        theme(axis.text.x = element_text(angle = 90)) -> plot

    if (print) plot
    else metrics_out
}
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** a. Backward Elimination 

#+BEGIN_SRC R :file plot.svg :results graphics file
all_stepwise(prostate, "lpsa ~ .", print = TRUE)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** AIC, Adjusted R^2, Mallow's Cp 

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion(prostate, as.formula("lpsa ~ .")) -> p_out
p_out %>% reduce(`/`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** 2. Use the teengamb dataset with gamble as the response and the other variables as predictors, repeat the work of the first question 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
teengamb %<>% as_tibble()
teengamb %>% skimr::skim()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
teengamb %>%
    ggpairs()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
all_stepwise(teengamb, "gamble ~ .")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion(teengamb, as.formula("gamble ~ .")) -> p_out
p_out %>% reduce(`/`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** 3. Using the divusa dataset with divorce as the response and the other variables as predictors, repeat the work of the first question 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
divusa %<>% as_tibble()

divusa %>% skimr::skim()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
divusa %>%
    ggpairs()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


#+BEGIN_SRC R :file plot.svg :results graphics file
all_stepwise(divusa, "divorce ~ .")
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion(divusa, as.formula(divorce ~ .)) -> p_out

p_out %>% reduce(`/`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** 4. Using the trees data, fit a model with log(Volume) as the response and a second order polynomial (including the interaction term) in Girth and Height. Determine whether the model may be reasonably simplified. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
trees %<>% as_tibble()

(lmod <- lm(log(Volume) ~ Girth + Height + (Girth * Height) + I(Girth^2) + I(Height^2), trees))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion(trees, as.formula(log(Volume) ~ Girth + Height + (Girth * Height) + I(Girth^2) + I(Height^2))) %>% reduce(`/`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
leaps::regsubsets(log(Volume) ~ Girth + Height + (Girth * Height) + I(Girth^2) + I(Height^2), trees) %>% summary() -> rs
rs$which
#+END_SRC

#+RESULTS:
| (Intercept) | Girth | Height | I(Girth^2) | I(Height^2) | Girth:Height |
|-------------+-------+--------+------------+-------------+--------------|
| TRUE        | FALSE | FALSE  | FALSE      | FALSE       | TRUE         |
| TRUE        | TRUE  | TRUE   | FALSE      | FALSE       | FALSE        |
| TRUE        | TRUE  | TRUE   | TRUE       | FALSE       | FALSE        |
| TRUE        | TRUE  | TRUE   | TRUE       | TRUE        | FALSE        |
| TRUE        | TRUE  | TRUE   | TRUE       | TRUE        | TRUE         |

Our best model for all our criterions is the 3 predictor model. This corresponds to the model with the formula log(Volume) ~ Girth + Height + I(Girth^2).

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod %>% glance()
lmod %>% tidy()
lm(log(Volume) ~ Girth + Height + I(Girth^2), trees) %>% glance()
#+END_SRC

In order to reasonably simplify the model we must make sure that the model keeps all the lower level terms for our higher order terms. In this case, we should be fine since we have both Girth and Girth^2. 

** 5. Fit a linear model to the stackloss data with stack.loss as the response and the other variables as predictors. Simplify the model if possible. 

#+BEGIN_SRC R :file plot.svg :results graphics file
stackloss %<>% as_tibble()
stackloss %>% skimr::skim()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
stackloss %>% ggpairs()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
## fit the model
(lmod <- lm(stackloss$stack.loss ~ ., stackloss))

## simplify if possible
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
## lmod %>% all_criterion(stackloss, stackloss$stack.loss ~ .)
regsubsets(stackloss$stack.loss ~ ., stackloss) %>>% (~ sl_leaps) %>% all_criterion(stackloss, stackloss$stack.loss, leaps_in = ., print = TRUE)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

Our best model is the one with two predictors. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
summary(sl_leaps)
#+END_SRC

This is the model with air flow and water temp, but not acid concentration. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(lmod2 <- lm(stackloss$stack.loss ~ Air.Flow + Water.Temp, stackloss))
lmod2 %>% summary()

lmod %>% summary()
#+END_SRC

Check the model for outliers and influential points. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
## outliers
get_adj_outliers <- function(lmod,
                      base_p_value = 0.05,
                      correction_strength = NA,
                      top_vals = 3) {
    # get studentized resids
    st_res <- rstudent(lmod)

    # set correction strength
    if (is.na(correction_strength)) {
        correction_strength <- lmod %>%
            augment() %>%
            nrow()
    } else {
        correction_strength
    }

    # compute bonferroni
    dof <- lmod %>%
        glance() %>%
        pull(df.residual)

    base_p_value <- base_p_value

    # compute conferroni
    bf_val <- qt(p = base_p_value / correction_strength,
                 df = dof)

    # get top_n absolute value studentized residuals
    top_n_vals <- st_res %>%
        enframe() %>%
        top_n(abs(value), n = top_vals) %>%
        add_row(name = "Bonferroni Correction Threshold",
                value = bf_val) %>%
        rowwise() %>%
        mutate(over_thresh = ifelse(abs(value) > abs(bf_val),
                                    TRUE,
                                    FALSE))

    # get outlier values
    top_n_vals %>%
        pluck(1) %>%
        head(-1) %>%
        map_df(., ~ lmod %>%
            augment() %>%
            slice(as.integer(.x)) %>%
            mutate(name = .x)) -> outlier_values

    outlier_values %>%
        left_join(top_n_vals, by = "name") %>%
        select("Index" = name, over_thresh, value, everything())
}

## influential point plot
inf_pt_plot <- function(lmod) {
    lmod %>%
        augment() %>%
        pull(.cooksd) %>%
        gghalfnorm::gghalfnorm()
}
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
(lmod2 %>%
 get_adj_outliers() -> ol_sl)

## see what values are outliers
stackloss %>%
    slice(c(3, 4, 21))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
lmod2 %>%
    inf_pt_plot()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

We have 3 outliers. 

Now return to the full model, determine whether there are any outliers or influential points, eliminate them, and then repeat the variable selection procedures.

#+BEGIN_SRC R :file plot.svg :results graphics file
lmod %>%
    get_adj_outliers()

lmod %>% inf_pt_plot()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

We get the same outliers as the reduced model.

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
stackloss %>% slice(-c(3, 4, 21)) -> ss_sl
ss_sl %<>% rename("sl" = stack.loss)
(lmod3 <- lm(sl ~ ., ss_sl))
#+END_SRC

Now we must redo the model selection process.

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion(ss_sl, sl ~ .) %>% reduce(`/`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
all_criterion(ss_sl, sl ~ ., print = FALSE, summary = TRUE)
#+END_SRC

Our new best model is the 2 parameter model with Air flow and water temp. 

** 6. Use the seatpos data with hipcenter as the response 

#+BEGIN_SRC R :file plot.svg :results graphics file
seatpos %<>% as_tibble()

seatpos %>% skimr::skim()

seatpos %>% ggpairs()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** a. Fit a model with all eight predictors. Comment on the effect of leg length on the response

#+BEGIN_SRC R :file plot.svg :results graphics file
(lmod <- lm(hipcenter ~ ., seatpos))

lmod %>% summary()
#+END_SRC

Leg length has a large effect on the hipcenter for this data. That said, all of these values seem highly negatively correlated with each hip center. 

#+BEGIN_SRC R :file plot.svg :results graphics file
seatpos %>%
    correlate() %>%
    rplot()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** b. Compute a 95% confidence interval for the mean value of the predictors 

#+BEGIN_SRC R :file plot.svg :results graphics file
lmod %>%
    tidy(conf.int = .95) %>%
    slice(2:nrow(.)) %>% 
    ggplot(aes(x = term, y = estimate)) +
    geom_point(color = "forestgreen") +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high), alpha = 0.5, color = "mediumpurple")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** c. Use AIC to select a model.  

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion(seatpos, hipcenter ~ .) %>% reduce(`/`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

The AIC chooses the model with only 3 predictors. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
all_criterion(seatpos, hipcenter ~ ., print = FALSE, summary = TRUE) 
#+END_SRC

The new model only has Ht and Leg as predictors. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(lmod2 <- lm(hipcenter ~ Ht + Leg, seatpos))
lmod2 %>% summary()
#+END_SRC

Now interpret the effect of leg length and compute the prediction interval.

In this case, Leg is still a large driver downwards of hipcenter. It is also the less significant variable of the two and has a lot more uncertainty around its point estimate. 

#+BEGIN_SRC R :file plot.svg :results graphics file
lmod2 %>%
    tidy(conf.int = .95) %>%
    slice(2:nrow(.)) %>% 
    ggplot(aes(x = term, y = estimate)) +
    geom_point(color = "forestgreen") +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high), alpha = 0.5, color = "mediumpurple")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

Compare the conclusions from the two models. 

#+BEGIN_SRC R :file plot.svg :results graphics file
lmod %>% summary()
lmod2 %>% summary()
#+END_SRC

Our second model performs a lot better on the adjusted R^2 since it has much fewer parameters, but is a worse fit overall than the first model. 

** 7. Generate some simulated data 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
c_val <- .995
gend <- tibble(x = rgamma(500, 3) * cos(pi/4),
               y = x * c_val + (rgamma(500, 3)) * sqrt(1 - c_val^2))

cor(gend$x, gend$y)
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
gend %>% ggpairs()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** a. Fit regression splines with 12 evenly spaces knots using y ~ bs(x, 12). Display the fit on top of the data 


#+BEGIN_SRC R :file plot.svg :results graphics file
library(splines)

(lmodc <- lm(y ~ bs(x, 12), gend))

gend %>%
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    geom_line(aes(x = x, y = lmodc$fitted.values), color = "blue")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** b. Compute the AIC for the model 

#+BEGIN_SRC R :file plot.svg :results graphics file
AIC(lmodc)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** c. Compute the AIC for all models with a number of knots between 3 and 20 inclusive. Plot the AIC as a function of the number of parameters. Which model is best?

#+BEGIN_SRC R :file plot.svg :results graphics file
map(3:20, ~ lm(y ~ bs(x, .x), gend) %>% AIC()) %>%
    set_names(paste0("basis ", 3:20)) %>%
    enframe(name = "basis", value = "aic") %>%
    unnest() %>%
    ungroup() %>%
    ggplot(aes(x = reorder(basis, aic), y = aic, group = 1)) +
    geom_point() +
    geom_line(color = "orange", alpha = 0.5) +
    theme(axis.text.x = element_text(angle = 90)) +
    xlab("Number of Knots") + ylab("Akaike Information Criterion") +
    ggtitle("AIC by Knot Count")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** d. plot the fit for your selected model on top of the data 

#+BEGIN_SRC R :file plot.svg :results graphics file
(lmodb <- lm(y ~ bs(x, 3), gend))

gend %>%
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    geom_line(aes(x = x, y = lmodb$fitted.values), color = "blue")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** 8. Use the odor data with odor as a response 


