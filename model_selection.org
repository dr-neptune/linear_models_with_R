* Model Selection
:PROPERTIES:
:header-args: :session R-session :results output value :colnames yes
:END:

#+NAME: round-tbl
#+BEGIN_SRC emacs-lisp :var tbl="" fmt="%.1f"
(mapcar (lambda (row)
          (mapcar (lambda (cell)
                    (if (numberp cell)
                        (format fmt cell)
                      cell))
                  row))
        tbl)
#+end_src

#+RESULTS: round-tbl

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(tidyverse)
library(faraway)
library(broom)
library(magrittr)
#+END_SRC


In this chapter, we consider the the problem of selecting the "best" subset of predictors. 

** Hierarchical Models 

Some models have a natural hierarchy. When selecting variables, it is important to respect the hierarchy. 

For example, if we have a polynomial regression, we should not drop the $x$ term and keep the $x^2$ term. 

For models with interactions, we should not consider removing the $x_1 x_2$ interaction term without simultaneously considering the removal of the $x_1^2$ and $x_2^2$ terms. 

** Testing Based Procedures 

Backward elimination is the simplest of all variable selection procedures. We start with all the predictors, and then remove the predictor with the highest p-value greater than $\alpha_{crit}$. Then refit the model, and remove the remaining least significant procedure. Sooner or later, all "nonsignificant" predictors will be removed and the selection process will complete. 

The $\alpha_{crit}$ is sometimes called the p to remove and does not have to be 5%. If prediction performance is the goal, then a 15 to 20% cutoff may work best, although methods designed for optimal prediction should be preferred. 

Forward selection is the same thing, but backwards. We start with no variables, and for all the variables not in the model we check their p-values if we add them to the model. 

Stepwise regression is the combination of both forward and backward elimination. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
data(state)

state.x77 %<>%
    as_tibble() %>%
    janitor::clean_names()

## example of backward elimination
lmod <- lm(life_exp ~ ., state.x77)

lmod %>% summary()

## first remove area, then illiteracy, then income, the population
lmod %>% step(direction = "backward") -> lmod_b
lmod %>% step(direction = "forward") -> lmod_f
lmod %>% step(direction = "both") -> lmod_fb

lmod_b %>% summary()
lmod_f %>% summary()
lmod_fb %>% summary()
#+END_SRC

Testing-based procedures are relatively cheap computationally and easy to understand, but do have some drawbacks.

1. Because of the one at a time nature of adding / dropping variables, it is possible to miss the "optimal" model.

2. The p-values should not be treated too literally. There is so much multiple testing occurring that the validity is dubious. The removal of less significant predictors tends to increase the significance of the remaining predictors. This leads one to overstate the importance of the remaining predictors.

3. The procedures are not directly linked to final objectives of prediction or explanation, so they may not really help solve the problem of interest. Variable selection tends to amplify the statistical significance of the variables that stay in the model. Variables that are dropped can still be correlated with the response; they could be dropped simply because they provide no additional explanatory effect beyond those variables already included in the model 

4. Stepwise variable selection tends to pick models that are smaller than desirable for prediction purposes. 

Except in simple cases where only a few models are compared or in highly structured hierarchical models, testing-based variable selection should not be used. 

** Criterion-Based Procedures 

We choose the model that minimizes the Akaike Information Criterion: $AIC = -2 L(\hat{\theta}) + 2p$. For linear regression models, the -2 * max log likelihood = $n \log(\frac{RSS}{n}) + c$. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(leaps)

(b <- regsubsets(life_exp ~ ., state.x77))
rs <- summary(b)

nrow(lmod$model)

aic_out <- function(lmod) {
    n <- nrow(lmod$model)
    rss <- sum(lmod$residuals^2)
    npred <- 2:length(lmod$coefficients)

    n * log(rss / n) + max(npred) * 2
}

aic_out(lmod)
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
## plot AIC 
aic_out <- 50 * log(rs$rss / 50) + (2:8) * 2

ggplot(enframe(aic_out), aes(x = name, y = value)) +
    geom_point() +
    xlab("Number of Predictors") +
    ylab("Akaike Information Criterion")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

We see that the AIC is minimized by a choice of 4 predictors -- population, murder, hs_grad, and frost. 

Another commonly used criterion is the adjusted $R^2$, written $R_a^2$. Recall that $R^2 = 1 - \frac{RSS}{TSS}$. Our adjusted criterion is $R_a^2 = 1 - \frac{\frac{RSS}{n - p}}{\frac{TSS}{n - 1}} = 1 - \frac{n - 1}{n - p}(1 - R^2) = 1 - \frac{\hat{\sigma_{model}^2}}{\hat{\sigma_{null}^2}}$

#+BEGIN_SRC R :file plot.svg :results graphics file
enframe(rs$adjr2) %>%
    ggplot(aes(x = name, y = value)) +
    geom_point() +
    xlab("Number of Predictors") +
    ylab("Adjusted R^2")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

Our final criterion is Mallow's C_p statistic. 

A good model should predict well, so the average mean squared error of prediction should be a good criterion. This can be estimated by the C_p statistic:

$C_p = \frac{RSS_p}{\hat{\sigma^2}} + 2p - n$

where $\hat{\sigma^2}$ is the model with all predictors and $RSS_p$ indicates the RSS from a model with p parameters. 

#+BEGIN_SRC R :file plot.svg :results graphics file
enframe(rs$cp) %>%
    ggplot(aes(x = name, y = value)) +
    geom_point() +
    geom_line(color = "green", alpha = 0.2) +
    geom_abline(slope = 1, intercept = 0, color = "blue", lty = 2, alpha = 0.5) +
    xlab("Number of Predictors") +
    ylab(latex2exp::TeX("C_p")) +
    ggtitle(latex2exp::TeX("Mallow's $C_p$"))
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion <- function(data, formula, print = FALSE) {
    (b <- leaps::regsubsets(formula, data))
    rs <- summary(b)
    aic_out <- nrow(data) * log(rs$rss / nrow(data)) + (2:length(b$xnames)) * 2

    list("Akaike Information Criterion" = aic_out,
         "Adjusted R^2" = rs$adjr2,
         "Mallow's $C_p$" = rs$cp) %>%
        imap(., ~ {
            .x %>%
                enframe() %>%
                ggplot(aes(x = name, y = value)) +
                geom_point() +
                geom_line(color = "green", alpha = 0.2) +
                xlab("Number of Predictors") +
                ylab(latex2exp::TeX(str_to_title(.y))) +
                ggtitle(latex2exp::TeX(.y))
        }) -> p_out

    p_out[[3]] %<>% `+`(geom_abline(slope = 1, intercept = 0, color = "blue", alpha = 0.3, lty = 2))
    
    if (print) reduce(p_out, `/`)
    else p_out
}

all_criterion(state.x77, as.formula("life_exp ~ .")) -> p_out

library(patchwork)

reduce(p_out, `/`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


Variable selection methods are sensitive to outliers and influential points. 

Let's check for high leverage points:
 
#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod %>%
    augment() %>%
    pull(.hat) %>% 
    `names<-`(state.abb) %>%
    sort() %>%
    rev()
#+END_SRC

We can see that Alaska has high leverage. Let's try excluding it.

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion(state.x77, as.formula(life_exp ~ .)) -> p1
all_criterion(state.x77[-2, ], as.formula(life_exp ~ .)) -> p2

list(p1, p2) %>% flatten() %>% reduce(`+`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

We can see that removing Alaska made all the responses much smoother across the predictor counts. 

#+BEGIN_SRC R :file plot.svg :results graphics file
## stripchart(data.frame(scale(state.x77)),
##            method = "jitter",
##            las = 2,
##            vertical = TRUE)
library(pipeR)

scale(state.x77) %>%
    as_tibble() %>%
    mutate("state_name" = state.abb) %>%
    pivot_longer(-state_name,
                 names_to = "predictor",
                 values_to = "std_ct") %>>%
    (~ inter_df) %>% 
    ## assign("inter_df", value = ., pos = 1) %>% 
    ggplot(aes(x = predictor, y = std_ct, group = predictor)) +
    geom_jitter(position = position_jitter(0.2), aes(color = predictor)) +
    geom_point(data = subset(inter_df, std_ct > 3), color = "black", size = 10, shape = 1) +
    scale_color_viridis_d()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


In the figure above, we see that population and area are skewed. We can try transforming them: 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
b <- regsubsets(life_exp ~ log(population) + income + illiteracy + murder + hs_grad + frost + log(area), state.x77)
rs <- summary(b)
rs$which[which.max(rs$adjr), ]
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
scale(state.x77) %>%
    as_tibble() %>%
    mutate("state_name" = state.abb,
           population = log(population),
           area = log(area)) %>%
    pivot_longer(-state_name,
                 names_to = "predictor",
                 values_to = "std_ct") %>>%
    (~ inter_df) %>% 
    ## assign("inter_df", value = ., pos = 1) %>% 
    ggplot(aes(x = predictor, y = std_ct, group = predictor)) +
    geom_jitter(position = position_jitter(0.2), aes(color = predictor)) +
    geom_point(data = subset(inter_df, std_ct > 3), color = "black", size = 10, shape = 1) +
    scale_color_viridis_d()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** Summary 

Hypothesis testing based methods use a restricted search through the space of potential models and use a dubious method for choosing between models when repeated many times. Criterion-based methods typically involve a wider search and compare models in a more preferable manner. 

If several models are suggested which fit as well as each other, consider:

1. Do the models have similar qualitative consequences?
2. Do they make similar predictions?
3. What is the cost of measuring the predictors?
4. Which has the best diagnostics? 

** Exercises 

** 1. Use the prostate data with lpsa as the response and the other variables as predictors. Implement the following variable selection methods to determine the best model: 

 #+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
prostate %<>% as_tibble()

prostate %>% skimr::skim()
 #+END_SRC

 #+BEGIN_SRC R :file plot.svg :results graphics file
prostate %>%
    ggpairs()
#+END_SRC

 #+RESULTS:
 [[file:plot.svg]]

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
all_criterion <- function(data, formula, print = FALSE) {
    lmod <- lm(formula, data)
    
    lmod %>% step(direction = "backward") -> lmod_b
    lmod %>% step(direction = "forward") -> lmod_f
    lmod %>% step(direction = "both") -> lmod_s
    
    (b <- leaps::regsubsets(formula, data))
    rs <- summary(b)
    aic_out <- nrow(data) * log(rs$rss / nrow(data)) + (2:length(b$xnames)) * 2

    list("Akaike Information Criterion" = aic_out,
         "Adjusted R^2" = rs$adjr2,
         "Mallow's $C_p$" = rs$cp) %>%
        imap(., ~ {
            .x %>%
                enframe() %>%
                ggplot(aes(x = name, y = value)) +
                geom_point() +
                geom_line(color = "green", alpha = 0.2) +
                xlab("Number of Predictors") +
                ylab(latex2exp::TeX(str_to_title(.y))) +
                ggtitle(latex2exp::TeX(.y))
        }) -> p_out

    p_out[[3]] %<>% `+`(geom_abline(slope = 1, intercept = 0, color = "blue", alpha = 0.3, lty = 2))
    
    if (print) reduce(p_out, `/`)
    else p_out
}
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
all_stepwise <- function(data, formula, print = TRUE) {
    lmod <- lm(formula, data)
    c("backward", "forward", "both") %>>% (~ names_out) %>%  map(., ~ lmod %>% step(direction = .x)) %>% set_names(names_out) -> mods
    mods %>%
        map(glance) %>%
        bind_rows(.id = "model") %>%
        pivot_longer(-model, names_to = "metric") %>>%
        (~ metrics_out) %>% 
        ggplot() +
        geom_point(aes(x = model, y = value, color = model),
                   alpha = 0.75) +
        facet_wrap(~ metric, scales = "free") +
        theme(axis.text.x = element_text(angle = 90)) -> plot

    if (print) plot
    else metrics_out
}
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** a. Backward Elimination 

#+BEGIN_SRC R :file plot.svg :results graphics file
all_stepwise(prostate, "lpsa ~ .", print = TRUE)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** AIC, Adjusted R^2, Mallow's Cp 

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion(prostate, as.formula("lpsa ~ .")) -> p_out
p_out %>% reduce(`/`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** 2. Use the teengamb dataset with gamble as the response and the other variables as predictors, repeat the work of the first question 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
teengamb %<>% as_tibble()
teengamb %>% skimr::skim()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
teengamb %>%
    ggpairs()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
all_stepwise(teengamb, "gamble ~ .")
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion(teengamb, as.formula("gamble ~ .")) -> p_out
p_out %>% reduce(`/`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** 3. Using the divusa dataset with divorce as the response and the other variables as predictors, repeat the work of the first question 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
divusa %<>% as_tibble()

divusa %>% skimr::skim()
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
divusa %>%
    ggpairs()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]


#+BEGIN_SRC R :file plot.svg :results graphics file
all_stepwise(divusa, "divorce ~ .")
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion(divusa, as.formula(divorce ~ .)) -> p_out

p_out %>% reduce(`/`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** 4. Using the trees data, fit a model with log(Volume) as the response and a second order polynomial (including the interaction term) in Girth and Height. Determine whether the model may be reasonably simplified. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
trees %<>% as_tibble()

(lmod <- lm(log(Volume) ~ Girth + Height + (Girth * Height) + I(Girth^2) + I(Height^2), trees))
#+END_SRC

#+BEGIN_SRC R :file plot.svg :results graphics file
all_criterion(trees, as.formula(log(Volume) ~ Girth + Height + (Girth * Height) + I(Girth^2) + I(Height^2))) %>% reduce(`/`)
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
leaps::regsubsets(log(Volume) ~ Girth + Height + (Girth * Height) + I(Girth^2) + I(Height^2), trees) %>% summary() -> rs
rs$which
#+END_SRC

#+RESULTS:
| (Intercept) | Girth | Height | I(Girth^2) | I(Height^2) | Girth:Height |
|-------------+-------+--------+------------+-------------+--------------|
| TRUE        | FALSE | FALSE  | FALSE      | FALSE       | TRUE         |
| TRUE        | TRUE  | TRUE   | FALSE      | FALSE       | FALSE        |
| TRUE        | TRUE  | TRUE   | TRUE       | FALSE       | FALSE        |
| TRUE        | TRUE  | TRUE   | TRUE       | TRUE        | FALSE        |
| TRUE        | TRUE  | TRUE   | TRUE       | TRUE        | TRUE         |

Our best model for all our criterions is the 3 predictor model. This corresponds to the model with the formula log(Volume) ~ Girth + Height + I(Girth^2).

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
lmod %>% glance()
lmod %>% tidy()
lm(log(Volume) ~ Girth + Height + I(Girth^2), trees) %>% glance()
#+END_SRC

In order to reasonably simplify the model we must make sure that the model keeps all the lower level terms for our higher order terms. In this case, we should be fine since we have both Girth and Girth^2. 

** 5. Fit a linear model to the stackloss data with stack.loss as the response and the other variables as predictors. Simplify the model if possible. 

Check the model for outliers and influential points. 

Now return to the full model, determine whether there are any outliers or influential points, eliminate them, and then repeat the variable selection procedures.



